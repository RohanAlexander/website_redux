---
title: "The opportunities provided by open data and reproducibility"
description: |
  Some thoughts on getting started with open data and reproducibility. A talk delivered to the University of Toronto Stellar Stats Workshop, 28 May 2021, organised by Gwen Eadie & Josh Speagle.
author:
  - name: Rohan Alexander
    url:
      rohanalexander.com: {}
date: 2021-05-22
preview: if_the_shoes_fit.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
draft: false
bibliography: references.bib
collections:
  posts:
    citations: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Introduction 

Hi, my name is Rohan Alexander. Thank you very much to Gwen and Josh for the opportunity to speak. I'm a baby professor at the University of Toronto, in the Faculty of Information and the Department of Statistical Sciences. Today I would like to talk about taking baby steps toward open data and reproducibility. And in particular the opportunities provided by open data and reproducibility across applied statistics and astronomy. 

I'd like to talk about three benefits of taking steps toward openness and reproducibility: 

1) improved diversity broadly; 
2) improving your own (future) life; and 
3) building on-ramps for your collaborators. 

And then three baby steps that you could get started with this afternoon, and those are: 

1) buy a bunch of notebooks; 
2) write a datasheet; and
3) make a software package.

## Three benefits

### Background

By way of background, @monicasrostockreprotalk says 'research is reproducible if it can be reproduced exactly, given all the materials used in the study'. She goes on:

> Reproducibility is not just publishing your analysis code. The entire workflow of a research project – from formulating hypotheses to dissemination of your results – has decisions and steps that ideally should be reproducible. This extends far beyond just posting the code for your model.

@opendatainstitute defines open data as 'data that's available to everyone to access, use and share.' Open data enables reproducibility and is our bedrock.

### Improved diversity

The first benefit that I see is improved diversity broadly in our fields. By making our work accessible to more people, we are helping to make education and research resources more equitable. In Canadian STEM fields we find that diversity at an undergraduate level is not great, but we're also not too bad---we reflect broader Canadian society to some extent---but that at each successive level we significantly worsen [@villemurewebb]. In my own Department of Statistical Sciences, University Professor Nancy Reid was the only tenured research-track woman in the department for decades. And in astronomy, speaking more broadly, @prescordweinstein [p. 132] says 'I've never met a Black woman professor in the field of theoretical cosmology because I am the first---not just the first professor, but also the first Black woman cosmology theory PhD'^[Thanks very much to Vianey Leos Barajas for gifting me this book.].

Improving diversity is important because diverse teams are associated with higher performance [@mckinsey]. That's just a correlation, but thinking about my own experiences, I do think that diversity in my own team has improved my productivity. For instance, I've found that people with different experiences push back and criticize me about things that I haven't considered. Improving those aspects makes the work better.

I also think that as Canada's best university, and a public one at that, we have a duty to be more reflective of Canada. There's a quote from one of my favourite books, where an astronomer is talking to a child, trying to convince him that humanities are important, and he says:

> Any science is expensive and astronomy is more expensive than most…. If you need hundreds of millions of pounds sooner or later you are going to have to talk to people who don't understand what you're doing and don't want to understand because they hated science at school.
>
>	@dewitt [p. 396]

It's entirely correct that these people are in charge because that's who we voted for. But it would be nice if they didn't hate astronomy or statistics when they take it at university and possibly part of that is reflecting other experiences than just our own.

### Improving your own (future) life

The second benefit is that future-you will be helped. I think that most of us who write code to analyse data for a living have had the feeling of coming back to a project after six months and a lot of the time it's literally like having to start a new project. The variables make no sense, and the code is unintelligible. The main question for me is always 'why did I do that?' And then I spend the afternoon re-coding and almost always end up back where I was. 

If we're spending eight hours a day writing code to analyse data, then almost anything that adds even 1 per cent to our productivity is worthwhile, let alone something that saves an afternoon. And that's particularly the case with open data and reproducibility, because these benefits tend to compound over time and accrue not just to you but other researchers. Talking about knowledge that only you have, doesn't make you knowledgeable, it makes you a crank, and that includes knowledge that only past-you has.


### Building on-ramps

The third benefit is that adopting open data and reproducibility can allow us to build better on-ramps for our collaborators. I love applied statistics, and I can't quite believe that I get paid to do this job, and I really want more people to be able to work with me on my projects. I get to work with a lot of students, and I think the best way for me to make it easier for them to get up to speed is to adopt open data and reproducibility principles. I don't think of myself as a scary or intimidating person, but I'm regularly told that in anonymous surveys. 

New, especially junior, collaborators may be hesitant to ask questions because they feel awkward, or they feel they should know the answer or that they worry I'll think they're dumb. Some thoughts that may run through their head when they watch me talk through some data analysis: "Why did he remove the 99s?", "Maybe everyone does that?", "I'll just look it up after that talk", "Oh no, now I missed it, what did he just say now?". Open data and reproducibility enable them to not be reliant on me and I hope makes it easier for my collaborators to work with me.

## Three baby steps

Right, so you're convinced! You want these three great benefits! How can you go about getting them? I'm not asking for anything big from you, and I'm not asking you to be brave or change much of anything that you do. (Indeed, if you're faculty then I'm just asking you to spend some research funds and hire some undergrads! Statistical Sciences has something in the order of 4,000 undergrads, which is of course quite a lot, but the great thing about it is that the top 5 per cent are just incredibly strong. So, you hire them, and you can get them for literally $25 an hour, and then just get them to do these three things! Possibly even that hiring would add to the diversity of your team.)


### Buy a bunch of notebooks

The first is to buy a notebook. (If you're faculty, then go and buy a bunch for your team.) And then just write one dot point each time you write some code chunk. Let's say you're cleaning some text data, and you want to change all the instances of 'Rohan' to 'Monica'. Just before you write the code that does that, or perhaps just after, write a simple dot point in that fancy notebook that you bought that explains what you did and why. That's it. At the end of the day, you'll magically have a plain-English list of everything that was done to the dataset. That can easily be added to an appendix or added as comments and documentation alongside the code. Newton and Da Vinci kept notebooks! And if that doesn't convince you (see: selection bias) the US NIH describes notebooks in science as 'legal documents' that support claims to patents, defend against allegations of fraud, and also act as your scientific legacy [@ryan].


### Write a datasheet

The second thing is writing a datasheet for your dataset. I had a quick look at some of the astronomy data repos and there's some amazing things around, NASA came up immediately of course, but also a bunch of other ones. The issue with using these datasets is that unless you collected, cleaned, and prepared the dataset yourself, it can be difficult to trust it. This is where datasheets come in [@gebru2020datasheets]. Datasheets are basically nutrition labels for datasets. And it's really important to understand what you're feeding your model, but plenty of researchers don't have any idea. For instance, recently researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated [@bandy2021addressing]! 

Instead of telling you how unhealthy some food is, a datasheet tells you things like: 

- 'Who created the dataset and on behalf of which entity?'
- 'Who funded the creation of the dataset?'
- 'Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?'
- 'Is any information missing from individual instances?'

I've integrated datasheets into my teaching and an example of one that a student wrote earlier in Winter is @rosenthal. Now I'm not saying that this isn't helpful for the student who made it---I think it was---but it's especially helpful for me when I think about how best to use the dataset that he created. 

### Make a software package.

The third and final thing is to build out internal APIs for your code and then make them external. Now this may sound intimidating, but you can do it! (Or if you're faculty, again it's just something that you can have an undergraduate do.) My PhD is in economic history and basically what that means is that I have a PhD in data gathering and cleaning. To my shame, I have literally written code that 'downloads a PDF from a URL, saves it to my computer, pauses, and goes and gets another PDF from a very similar URL', literally hundreds of times. And of course, no one should write the same code hundreds of times. 

Last summer I found myself asking a student to go and write code to do that same task and I finally decided that enough was enough. Instead, I had them put together an R package. Now this was literally the work of a week for them. So instead of everyone in the lab writing code each time then needed to download a PDF, now they could just call this R package and use that instead: we wrote an internal API. I decided that I wanted to make it external facing and there's now an R package---'heapsofpapers'---that anyone can use [@heapsofpapers].

If you want to do this and you use R then just follow 'Chapter 2 - The Whole Game' from @rpackages. Within a few hours you can have a workable solution and within a month or two you can have a great API that will provide you both with a paper, and more importantly, help your collaborators.

## Concluding remarks

Right, so that's three baby steps, so I'll stop there. I think that open data and reproducibility can be intimidating. I have a two-year-old and I imagine that when he first started to walk he was pretty intimidated also. But he did take his first steps and now he runs around the whole day. And I assure you that it's the same for open data and reproducibility. Just take a few baby steps!

Thanks very much to Gwen and Josh for having me.



## Acknowledgments {.appendix}

Thank you very much to Monica for reading a draft of this. If you want something to watch after reading this then I recommend @monicatdrtalk

