[
  {
    "path": "posts/2020-04-14-tidying-the-2019-kenyan-census/",
    "title": "Tidying the 2019 Kenyan Census",
    "description": "In this note I examine the average age of politicians in the Australian Federal Parliament on a daily basis. Using a publicly available dataset I find that generally the Senate is older than the House of Representatives. The average age increased from Federation in 1901 through to 1949, when an expansion of the parliament's size likely brought many new politicians. I am unable to explain a sustained decline that occurred during the 1970s. From the 1980s onward there has been a gradual aging of both houses.",
    "author": [
      {
        "name": "Rohan Alexandere",
        "url": {}
      }
    ],
    "date": "2020-04-14",
    "categories": [],
    "contents": "\nTable of Contents\nIntroduction\nWorkspace set-up\nGet the data into R\nCleaningValues\nAreas\nAges\n\nChecksCheck gender sum\nCheck rural urban split\nCheck ages sum to age-groups\n\nFinal tidying\nMake Monica’s dataset\nIntroduction\nThe distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here: https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units.\nAnd while it is great that they make it easily available, and it is easy to look-up a particular result, it is not overly useful to do larger-scale data analysis, such as building a Bayesian hierarchical model.\nIn this blog post I convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analysed. I will draw on an introduce a bunch of handy packages including: janitor by (???), pdftools by (???), tidyverse by (???), and stringi by (???).\nIf you just want the cleaned tidied data, then it is here.\nWorkspace set-up\nTo get started I need to load the necessary packages.\n\n\nlibrary(janitor)\nlibrary(pdftools)\nlibrary(tidyverse)\nlibrary(stringi)\n\nAnd then I need to read in the PDF that I want to convert.\n\n\n# Read in the PDF\nall_content <- pdftools::pdf_text(\"inputs/pdfs/2019_Kenya_census.pdf\")\n\nThe pdf_text function from pdftools is useful when you have a PDF and you want to read the content into R. For many recently produced PDFs it’ll work pretty well, but there are alternatives. If the PDF is an image, then it won’t work and you’ll need to turn to OCR.\nYou can see a page of the PDF here:\n\n\nknitr::include_graphics(\"images/2020-04-10-screenshot-of-census.png\") \n\n\nGet the data into R\nThe first challenge is to get the dataset into a format that we can more easily manipulate. The way that I am going to do this is to consider each page of the PDF and extract the relevant parts. To do this, I first write a function that I want to apply to each page.\n\n\n# The function is going to take an input of a page\nget_data <- function(i){\n  # Just look at the page of interest\n  # Based on https://stackoverflow.com/questions/47793326/tabulize-function-in-r\n  just_page_i <- stringi::stri_split_lines(all_content[[i]])[[1]] \n  \n  # Grab the name of the location\n  area <- just_page_i[3] %>% str_squish()\n  area <- str_to_title(area)\n  \n  # Grab the type of table\n  type_of_table <- just_page_i[2] %>% str_squish()\n  \n  # Get rid of the top matter\n  just_page_i_no_header <- just_page_i[5:length(just_page_i)] # Just manually for now, but could create some rules if needed\n  \n  # Get rid of the bottom matter\n  just_page_i_no_header_no_footer <- just_page_i_no_header[1:62] # Just manually for now, but could create some rules if needed\n  \n  # Convert into a tibble\n  demography_data <- tibble(all = just_page_i_no_header_no_footer)\n  \n  # # Split columns\n  demography_data <-\n    demography_data %>%\n    mutate(all = str_squish(all)) %>% # Any space more than two spaces is squished down to one\n    mutate(all = str_replace(all, \"10 -14\", \"10-14\")) %>% \n    mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) %>% # Any space more than two spaces is squished down to one\n    separate(col = all,\n             into = c(\"age\", \"male\", \"female\", \"total\", \"age_2\", \"male_2\", \"female_2\", \"total_2\"),\n             sep = \" \", # Just looking for a space. Seems to work fine because the tables are pretty nicely laid out\n             remove = TRUE,\n             fill = \"right\"\n    )\n  \n  # They are side by side at the moment, need to append to bottom\n  demography_data_long <-\n    rbind(demography_data %>% select(age, male, female, total),\n          demography_data %>%\n            select(age_2, male_2, female_2, total_2) %>%\n            rename(age = age_2, male = male_2, female = female_2, total = total_2)\n    )\n  \n  # There is one row of NAs, so remove it\n  demography_data_long <- \n    demography_data_long %>% \n    janitor::remove_empty(which = c(\"rows\"))\n  \n  # Add the area and the page\n  demography_data_long$area <- area\n  demography_data_long$table <- type_of_table\n  demography_data_long$page <- i\n  \n  rm(just_page_i,\n     i,\n     area,\n     type_of_table,\n     just_page_i_no_header,\n     just_page_i_no_header_no_footer,\n     demography_data)\n  \n  return(demography_data_long)\n}\n\nAt this point, I have a function that does what I need to each page of the PDF. I’m going to use the function map_dfr from the purrr package to apply that function to each page, and then combine all the outputs into one tibble.\n\n\n# Run through each relevant page and get the data\npages <- c(30:513)\nall_tables <- map_dfr(pages, get_data)\nrm(pages, get_data, all_content)\n\nCleaning\nI now need to clean the dataset to make it useful.\nValues\nThe first step is to make the numbers into actual numbers, rather than characters. Before I can convert the type I need to remove anything that is not a number otherwise it’ll be converted into an NA. So I first identify any values that are not numbers so that I can remove them.\n\n\n# Need to convert male, female, and total to integers\n# First find the characters that should not be in there\nall_tables %>% \n  select(male, female, total) %>%\n  mutate_all(~str_remove_all(., \"[:digit:]\")) %>% \n  mutate_all(~str_remove_all(., \",\")) %>%\n  mutate_all(~str_remove_all(., \"_\")) %>%\n  mutate_all(~str_remove_all(., \"-\")) %>% \n  distinct()\n\n# A tibble: 3 x 3\n  male  female total\n  <chr> <chr>  <chr>\n1 \"\"    \"\"     \"\"   \n2 \"Aug\" \"\"     \"\"   \n3 \"Jun\" \"\"     \"\"   \n\n# We clearly need to remove \",\", \"_\", and \"-\". \n# This also highlights a few issues on p. 185 that need to be manually adjusted\n# https://twitter.com/RohanAlexander/status/1244337583016022018\nall_tables$male[all_tables$male == \"23-Jun\"] <- 4923\nall_tables$male[all_tables$male == \"15-Aug\"] <- 4611\n\nWhile you could use the janitor package here, it is worthwhile at least first looking at what is going on because sometimes there is odd stuff that janitor (and other packages) will deal with, but not in a way that you want. In this case, they’ve used Excel or similar and this has converted a couple of their entries into dates. If we just took the numbers from the column then we’d have 23 and 15 here, but by inspecting the column we can use Excel to reverse the process and enter the correct values of 4,923 and 4,611, respectively.\nHaving identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers.\n\n\nall_tables <-\n  all_tables %>%\n  mutate_at(vars(male, female, total), ~str_remove_all(., \",\")) %>% # First get rid of commas\n  mutate_at(vars(male, female, total), ~str_replace(., \"_\", \"0\")) %>%\n  mutate_at(vars(male, female, total), ~str_replace(., \"-\", \"0\")) %>%\n  mutate_at(vars(male, female, total), ~as.integer(.))\n\nAreas\nThe next thing to clean is the areas. We know that there are 47 counties in Kenya, and a whole bunch of sub-counties. They give us a list on pages 19 to 22 of the PDF (document pages 7 to 10). However, this list is not complete, and there are a few minor issues that we’ll deal with later.\nIn any case, I first need to fix a few inconsistencies.\n\n\n# Fix some area names\nall_tables$area[all_tables$area == \"Taita/ Taveta\"] <- \"Taita/Taveta\"\nall_tables$area[all_tables$area == \"Elgeyo/ Marakwet\"] <- \"Elgeyo/Marakwet\"\nall_tables$area[all_tables$area == \"Nairobi City\"] <- \"Nairobi\"\n\nKenya has 47 counties, each of which has sub-counties. The PDF has them arranged as the county data then the sub-counties, without designating which is which. We can use the names, to a certain extent, but in a handful of cases, there is a sub-county that has the same name as a county so we need to first fix that.\nThe PDF is made-up of three tables.\n\n\nall_tables$table %>% table()\n\n.\nTable 2.3: Distribution of Population by Age, Sex*, County and Sub- County \n                                                                     48216 \n      Table 2.4a: Distribution of Rural Population by Age, Sex* and County \n                                                                      5535 \n      Table 2.4b: Distribution of Urban Population by Age, Sex* and County \n                                                                      5781 \n\nSo I can first get the names of the counties based on those first two tables and then reconcile them to get a list of the counties.\n\n\n# Get a list of the counties \nlist_counties <- \n  all_tables %>% \n  filter(table %in% c(\"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\",\n                      \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n         ) %>% \n  select(area) %>% \n  distinct()\n\nAs I hoped, there are 47 of them. But before I can add a flag based on those names, I need to deal with the sub-counties that share their name. We will do this based on the page, then looking it up and deciding which is the county page and which is the sub-county page.\n\n\n# The following have the issue of the name being used for both a county and a sub-county:\nall_tables %>% \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\") %>% \n  filter(area %in% c(\"Busia\",\n                     \"Garissa\",\n                     \"Homa Bay\",\n                     \"Isiolo\",\n                     \"Kiambu\",\n                     \"Machakos\",\n                     \"Makueni\",\n                     \"Samburu\",\n                     \"Siaya\",\n                     \"Tana River\",\n                     \"Vihiga\",\n                     \"West Pokot\")\n         ) %>% \n  select(area, page) %>% \n  distinct()\n\n# A tibble: 24 x 2\n   area        page\n   <chr>      <int>\n 1 Samburu       42\n 2 Tana River    53\n 3 Tana River    56\n 4 Garissa       65\n 5 Garissa       69\n 6 Isiolo        98\n 7 Isiolo       100\n 8 Machakos     149\n 9 Machakos     154\n10 Makueni      159\n# … with 14 more rows\n\nNow we can add the flag for whether the area is a county and adjust for the ones that are troublesome,\n\n\n# Add flag for whether it is a county or a sub-county\nall_tables <- \n  all_tables %>% \n  mutate(area_type = if_else(area %in% list_counties$area, \"county\", \"sub-county\"))\n# Fix the flag for the ones that have their names used twice\nall_tables <- \n  all_tables %>% \n  mutate(area_type = case_when(\n    area == \"Samburu\" & page == 42 ~ \"sub-county\",\n    area == \"Tana River\" & page == 56 ~ \"sub-county\",\n    area == \"Garissa\" & page == 69 ~ \"sub-county\",\n    area == \"Isiolo\" & page == 100 ~ \"sub-county\",\n    area == \"Machakos\" & page == 154 ~ \"sub-county\",\n    area == \"Makueni\" & page == 164 ~ \"sub-county\",\n    area == \"Kiambu\" & page == 213 ~ \"sub-county\",\n    area == \"West Pokot\" & page == 233 ~ \"sub-county\",\n    area == \"Vihiga\" & page == 333 ~ \"sub-county\",\n    area == \"Busia\" & page == 353 ~ \"sub-county\",\n    area == \"Siaya\" & page == 360 ~ \"sub-county\",\n    area == \"Homa Bay\" & page == 375 ~ \"sub-county\",\n    TRUE ~ area_type\n    )\n  )\n\nrm(list_counties)\n\nAges\nNow we can deal with the ages.\nFirst we need to fix some errors.\n\n\n# Clean up ages\ntable(all_tables$age) %>% head()\n\n    0   0-4     1    10 10-14 10-19 \n  484   484   484   484   482     1 \n\nunique(all_tables$age) %>% head()\n\n[1] \"Total\" \"0\"     \"1\"     \"2\"     \"3\"     \"4\"    \n\n# Looks like there should be 484, so need to follow up on some:\nall_tables$age[all_tables$age == \"NotStated\"] <- \"Not Stated\"\nall_tables$age[all_tables$age == \"43594\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"43752\"] <- \"10-14\"\nall_tables$age[all_tables$age == \"9-14\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"10-19\"] <- \"10-14\"\n\nThe census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year-age. As such I’ll add a flag as to the type of age it is: an age group, such as ages 0 to 5, or a single age, such as 1.\n\n\n# Add a flag as to whether it's a summary or not\nall_tables$age_type <- if_else(str_detect(all_tables$age, c(\"-\")), \"age-group\", \"single-year\")\nall_tables$age_type <- if_else(str_detect(all_tables$age, c(\"Total\")), \"age-group\", all_tables$age_type)\n\nAt the moment, age is a character variable. We have a decision to make here, because we don’t want it to be a character variable (because it won’t graph properly), but we don’t want it to be a numeric, because there is total and also 100+ in there. So for now, we’ll just make it into a factor, and at least that will be able to be nicely graphed.\n\n\nall_tables$age <- as_factor(all_tables$age)\n\nChecks\nCheck gender sum\nGiven the format of the data, at this point it is easy to check that total is the sum of male and female.\n\n\n# Check the parts and the sums\nfollow_up <- \n  all_tables %>% \n  mutate(check_sum = male + female,\n         totals_match = if_else(total == check_sum, 1, 0)\n         ) %>% \n  filter(totals_match == 0)\n\nThere is just one that seems wrong.\n\n\n# There is just one that looks wrong\nall_tables$male[all_tables$age == \"10\" & all_tables$page == 187] <- as.integer(1)\n\nrm(follow_up)\n\nCheck rural urban split\nThe census provides different tables for the total of each county and sub-county; and then within each county, for the number in an urban area in that county, and the number in a urban area in that county. Some counties only have an urban count, but we’d like to make sure that the sum of rural and urban counts equals the total count. This requires reshaping the data from a long to wide format.\nFirst, construct different tables for each of the three. I just do it manually, but I could probably do this a nicer way.\n\n\n# Table 2.3\ntable_2_3 <- all_tables %>% \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\")\ntable_2_4a <- all_tables %>% \n  filter(table == \"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\")\ntable_2_4b <- all_tables %>% \n  filter(table == \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n\nHaving constructed the constituent parts, I now join then based on age, area, and whether it is a county.\n\n\nboth_2_4s <- full_join(table_2_4a, table_2_4b, by = c(\"age\", \"area\", \"area_type\"), suffix = c(\"_rural\", \"_urban\"))\n\nall <- full_join(table_2_3, both_2_4s, by = c(\"age\", \"area\", \"area_type\"), suffix = c(\"_all\", \"_\"))\n\nall <- \n  all %>% \n  mutate(page = glue::glue('Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}')) %>% \n  select(-page, -page_rural, -page_urban,\n         -table, -table_rural, -table_urban,\n         -age_type_rural, -age_type_urban\n         )\n\nrm(both_2_4s, table_2_3, table_2_4a, table_2_4b)\n\nWe can now check that the sum of rural and urban is the same as the total.\n\n\n# Check that the urban + rural = total\nfollow_up <- \n  all %>% \n  mutate(total_from_bits = total_rural + total_urban,\n         check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0),\n         total_from_bits - total) %>% \n  filter(check_total_is_rural_plus_urban == 0)\n\nhead(follow_up)\n\n# A tibble: 3 x 16\n  age     male female  total area  area_type age_type\n  <fct>  <int>  <int>  <int> <chr> <chr>     <chr>   \n1 Not …     31     10     41 Naku… county    single-…\n2 Total 434287 441379 875666 Bomet county    age-gro…\n3 Not …      3      2      5 Bomet county    single-…\n# … with 9 more variables: male_rural <int>,\n#   female_rural <int>, total_rural <int>,\n#   male_urban <int>, female_urban <int>,\n#   total_urban <int>, total_from_bits <int>,\n#   check_total_is_rural_plus_urban <dbl>, `total_from_bits\n#   - total` <int>\n\nrm(follow_up)\n\nThere are just a few, but they only have a a difference of 1, so I’ll just move on.\nCheck ages sum to age-groups\nFinally, I want to check that the single age counts sum to the age-groups.\n\n\n# One last thing to check is that the ages sum to their age-groups.\nfollow_up <- \n  all %>% \n  mutate(groups = case_when(age %in% c(\"0\", \"1\", \"2\", \"3\", \"4\", \"0-4\") ~ \"0-4\",\n                            age %in% c(\"5\", \"6\", \"7\", \"8\", \"9\", \"5-9\") ~ \"5-9\",\n                            age %in% c(\"10\", \"11\", \"12\", \"13\", \"14\", \"10-14\") ~ \"10-14\",\n                            age %in% c(\"15\", \"16\", \"17\", \"18\", \"19\", \"15-19\") ~ \"15-19\",\n                            age %in% c(\"20\", \"21\", \"22\", \"23\", \"24\", \"20-24\") ~ \"20-24\",\n                            age %in% c(\"25\", \"26\", \"27\", \"28\", \"29\", \"25-29\") ~ \"25-29\",\n                            age %in% c(\"30\", \"31\", \"32\", \"33\", \"34\", \"30-34\") ~ \"30-34\",\n                            age %in% c(\"35\", \"36\", \"37\", \"38\", \"39\", \"35-39\") ~ \"35-39\",\n                            age %in% c(\"40\", \"41\", \"42\", \"43\", \"44\", \"40-44\") ~ \"40-44\",\n                            age %in% c(\"45\", \"46\", \"47\", \"48\", \"49\", \"45-49\") ~ \"45-49\",\n                            age %in% c(\"50\", \"51\", \"52\", \"53\", \"54\", \"50-54\") ~ \"50-54\",\n                            age %in% c(\"55\", \"56\", \"57\", \"58\", \"59\", \"55-59\") ~ \"55-59\",\n                            age %in% c(\"60\", \"61\", \"62\", \"63\", \"64\", \"60-64\") ~ \"60-64\",\n                            age %in% c(\"65\", \"66\", \"67\", \"68\", \"69\", \"65-69\") ~ \"65-69\",\n                            age %in% c(\"70\", \"71\", \"72\", \"73\", \"74\", \"70-74\") ~ \"70-74\",\n                            age %in% c(\"75\", \"76\", \"77\", \"78\", \"79\", \"75-79\") ~ \"75-79\",\n                            age %in% c(\"80\", \"81\", \"82\", \"83\", \"84\", \"80-84\") ~ \"80-84\",\n                            age %in% c(\"85\", \"86\", \"87\", \"88\", \"89\", \"85-89\") ~ \"85-89\",\n                            age %in% c(\"90\", \"91\", \"92\", \"93\", \"94\", \"90-94\") ~ \"90-94\",\n                            age %in% c(\"95\", \"96\", \"97\", \"98\", \"99\", \"95-99\") ~ \"95-99\",\n                            TRUE ~ \"Other\")\n         ) %>% \n  group_by(area_type, area, groups) %>% \n  mutate(group_sum = sum(total, na.rm = FALSE),\n         group_sum = group_sum / 2,\n         difference = total - group_sum) %>% \n  ungroup() %>% \n  filter(age == groups) %>% \n  filter(total != group_sum) \n\nhead(follow_up)\n\n# A tibble: 6 x 16\n  age    male female total area  area_type age_type\n  <fct> <int>  <int> <int> <chr> <chr>     <chr>   \n1 0-4       1      5     6 Mt. … sub-coun… age-gro…\n2 5-9       1      2     3 Mt. … sub-coun… age-gro…\n3 10-14     6      0     6 Mt. … sub-coun… age-gro…\n4 15-19     9      1    10 Mt. … sub-coun… age-gro…\n5 20-24    21      4    25 Mt. … sub-coun… age-gro…\n6 25-29    59      9    68 Mt. … sub-coun… age-gro…\n# … with 9 more variables: male_rural <int>,\n#   female_rural <int>, total_rural <int>,\n#   male_urban <int>, female_urban <int>,\n#   total_urban <int>, groups <chr>, group_sum <dbl>,\n#   difference <dbl>\n\nrm(follow_up)\n\nMt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly dodgy. I can’t see it in the documentation, but it looks like they have apportioned these between various countries. It’s understandable why they’d do this and it’s probably not a big deal, so I’ll just move on.\nFinal tidying\nNow that we are confident that everything is looking good, we can just convert it to long-format so that it is easy to work with.\n\n\nall <- \n  all %>% \n  rename(male_total = male,\n         female_total = female,\n         total_total = total) %>% \n  pivot_longer(cols = c(male_total, female_total, total_total, male_rural, female_rural, total_rural, male_urban, female_urban, total_urban),\n               names_to = \"type\",\n               values_to = \"number\"\n               ) %>% \n  separate(col = type, into = c(\"gender\", \"part_of_area\"), sep = \"_\") %>% \n  select(area, area_type, part_of_area, age, age_type, gender, number)\n\nwrite_csv(all, path = \"outputs/data/cleaned_kenya_2019_census.csv\")\n\nhead(all)\n\n# A tibble: 6 x 7\n  area  area_type part_of_area age   age_type gender  number\n  <chr> <chr>     <chr>        <fct> <chr>    <chr>    <int>\n1 Momb… county    total        Total age-gro… male    610257\n2 Momb… county    total        Total age-gro… female  598046\n3 Momb… county    total        Total age-gro… total  1208303\n4 Momb… county    rural        Total age-gro… male        NA\n5 Momb… county    rural        Total age-gro… female      NA\n6 Momb… county    rural        Total age-gro… total       NA\n\nMake Monica’s dataset\nThe original purpose of all of this was to make a table for Monica. She needed single-year counts, by gender, for the counties.\n\n\nmonicas_dataset <- \n  all %>% \n  filter(area_type == \"county\") %>% \n  filter(part_of_area == \"total\") %>%\n  filter(age_type == \"single-year\") %>% \n  select(area, age, gender, number)\n\nhead(monicas_dataset)\n\n# A tibble: 6 x 4\n  area    age   gender number\n  <chr>   <fct> <chr>   <int>\n1 Mombasa 0     male    15111\n2 Mombasa 0     female  15009\n3 Mombasa 0     total   30120\n4 Mombasa 1     male    15805\n5 Mombasa 1     female  15308\n6 Mombasa 1     total   31113\n\n\n\nwrite_csv(monicas_dataset, \"outputs/data/monicas_dataset.csv\")\n\nI’ll leave the fancy stats to Monica, but I’ll just make a quick graph of Nairobi.\n\n\nmonicas_dataset %>% \n  filter(area == \"Nairobi\") %>%\n  ggplot() +\n  geom_col(aes(x = age, y = number, fill = gender), position = \"dodge\") + \n  scale_y_continuous(labels = scales::comma) +\n  scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n  theme_classic()+\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(y = \"Number\",\n       x = \"Age\",\n       fill = \"Gender\",\n       title = \"Distribution of age and gender in Nairobi in 2019\",\n       caption = \"Data source: 2019 Kenya Census\")\n\n\n\n\n",
    "preview": "posts/2020-04-14-tidying-the-2019-kenyan-census/images/2020-04-10-screenshot-of-census.png",
    "last_modified": "2020-08-04T08:41:15-04:00",
    "input_file": {},
    "preview_width": 1060,
    "preview_height": 1500
  },
  {
    "path": "posts/2020-02-11-a-review-of-forecasting-elections-with-non-representative-polls/",
    "title": "A review of 'Forecasting elections with non-representative polls'",
    "description": "This brief review of Wang, Rothschild, Goel, and Gelman, (2015) 'Forecasting elections with non-representative polls' is designed to motivate discussion during [Lauren Kennedy](https://jazzystats.com/)'s weekly MRP Discussion Group.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2020-02-11",
    "categories": [],
    "contents": "\nTable of Contents\nIntroduction\nMotivation\nData\nModel\nResults - Voter intention\nResults - Election outcomes\nConclusions\nReferences\nIntroduction\n‘Forecasting elections with non-representative polls’ by Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman, published in 2015 in the International Journal of Forecasting, is a wonderful paper that should be more widely read, studied, and cited.1 It touches on many aspects from across social sciences and applied statistics, allowing readers from a variety of backgrounds a chance to better understand multi-level regression with post-stratification (MRP), and how they might contribute to its development or apply it in their own contexts. The paper clearly illustrates the power of MRP and motivates its use more widely, but is held back by not being reproducible and some unusual choices around datasets.\nIn the paper the authors use a non-representative sample of voting intention in the 2012 US presidential election from the Xbox platform, which enables people to play video games against each other online. They then combine this sample with data from exit polls, as well as a Bayesian hierarchical model, to provide estimates that are very similar to those produced from representative samples.\nThe key contributions of the paper include:\nShowing how a ‘non-traditional’ dataset can still yield meaningful results.\nTranslating daily estimates of opinion into what actually determines the election: electoral college votes.\nIllustrating the potential for MRP in other areas.\nKey weaknesses of the paper include:\na lack of reproducibility, and even a lack of transparency in some aspects;\na difficult-to-justify choice of dataset for post-stratification;\na lack of clarity around the propagation of uncertainty; and\nan inconsistent definition of what a good MRP model produces.\nIn the remainder of this review I will summarise the key aspects of the paper, attempting to link them to subsequent research where appropriate, and discussing the paper’s strengths and weaknesses, as well as raising questions that are unresolved in my mind. Based on Google Scholar, as of 11 February 2020, the paper has ‘only’ been cited around 250 times. It deserves to be much more widely read as it is a straight-forward paper that clearly illustrates the power of MRP, without hiding the potential issues.\nMotivation\n\nThe king (representative sampling) is dead, long live the king (non-representative sampling).\n\n\n\n\nFigure 1: Wang, Rothschild, Goel, and Gelman, (2015), page 1.\n\n\n\nThe paper begins with a detailed description of the role of representative sampling in modern opinion polling. It clearly illustrates why representative sampling become important, following the Literary Digest error during the 1936 US presidential election between Landon and Roosevelt. It does this with reference to two papers, both published in Public Opinion Quarterly albeit with a gap of more than 50 years, that are very interesting in their own right: Gosnell (1937) and Squire (1988) (Figure 2).\n\n\n\nFigure 2: Gosnell, 1937, on the left, and Squire, 1988 on the right.\n\n\n\nThese first few paragraphs may prompt some readers to think about the role of representative sampling in their own areas. The authors clearly establish the paper as aimed at a general audience,2 rather than solely Bayesian cult members, quantitative absolutionists, math wunderkinds3, Stan stans4 those who are already convinced of the power of MRP. The authors then give two reasons why the primacy of representative sampling may not now be absolute:\nNon-response rates are increasing for a variety of reasons including a reluctance to answer phone surveys and increased screening. The effect is possible concern around selection, but even if this selection issue does not arise, lower response rates require more post-sampling adjustment.\nOnline surveys mean large, non-representative, samples are much cheaper to collect.\nOn the basis of this the authors suggest MRP as an alternative and proceed to discuss their sample and their post-stratification dataset.\nData\nAt a minimum MRP requires two datasets. The first is the sample that is of interest, and the second is a dataset by which to adjust for some of that sample’s bias. This paper uses data from a biased, but large, sample of Xbox users about their voting intention in the 2012 US presidential election, and uses a poststratification dataset from exit polls.\nSurvey data\nKey facts about the survey data:\nData from an opt-in poll which was available on the Xbox gaming platform during the 45 days preceding the 2012 US presidential election.\nEach day there were three to five questions, including voter intention: ‘If the election were held today, who would you vote for?’.\nRespondents were allowed to answer at most once per day.\nFirst-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election. (That demographic data was collected before a respondent answered a poll to try to limit the amount of people switching their demographics to be in line with their intention, although it is difficult to adjust for the inverse, and increasingly respondents seem to be responding as pundits.)\nThere were 750,148 responses, 345,858 unique respondents, and over 30,000 unique respondents who completed five or more polls.5 6\nThe dataset is highly skewed (Figure 3):\n18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll.\nMen make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.\n\n\n\nFigure 3: (This is Figure 1 in the paper) ‘A comparison of the demographic, partisan, and 2008 vote distributions in the Xbox dataset and the 2012 electorate (as measured by adjusted exit polls). As one might expect, the sex and age distributions exhibit considerable differences.’\n\n\n\nAge and sex are known to be strongly correlated with voting preferences, and this shows up in the raw data (Figure 4).\n\n\n\nFigure 4: (This is Figure 2 in the paper.) ‘Daily (unadjusted) Xbox estimates of the two-party Obama support during the 45 days leading up to the 2012 presidential election, which suggest a landslide victory for Mitt Romney. The dotted blue line indicates a consensus average of traditional polls (the daily aggregated polling results from Pollster.com), the horizontal dashed line at 52% indicates the actual two-party vote share obtained by Barack Obama, and the vertical dotted lines give the dates of the three presidential debates.’\n\n\n\nThere are considerable changes over time, for instance a decrease in support for Obama after the first debate (Figure 4).\nPost-stratification data\nThe survey is biased, but the question at the heart of MRP is whether it can be salvaged.\nIn order to adjust for known bias in the sample we need a dataset that we would like our dataset to ‘mimic’. This is done by constructing ‘cells’ which are defined by a combination of all of the variables (say economic, demographic, and geographic features). For instance if we were interested in explaining vote share on the basis of age-group and sex, then we would need to know the number of 18-29-year-old males, the numbers of 18-29-year-old females, the number of 30-45-year-old males, and so on. We then apply a model trained on the survey to these ‘corrected’ proportions. Hence each additional variable adds considerably to what is needed from the post-stratification dataset.7\nOnce those cells are constructed, then the model trained on the sample is applied to each cell to generate an estimate for each cell. For instance, we might be interested in the proportion of 18-29-year-old males who would vote for Obama. Those cells can then be aggregated based on their relative-weight in the post-stratification dataset. For instance, if 18-29-year-old males make up 10 per cent of the population then their support for Obama would be weighted to 10 per cent.\nThis section of the paper is especially compelling. It draws on a variety of important issues including sampling and uncertainty, to motivate the use of a hierarchical model. The specific issue in the minds of many readers who may be new to MRP but are used to analysing surveys is that the above post-stratification process could be done with weights. This requires assuming that within each cell the sampling is un-biased. That implies a need for fine cells. But in practice especially fine cells are likely to have small populations, and so small differences will have large effects. Here we can think of the kidney cancer counties example where the clusters of the high and low countries tend to be right next to each other, because a handful of cases in a small area has a big effect. We can also think of Monica Alexander’s shrinking California example (Figure 5).\n\n\n\nFigure 5: Effect of shrinking California on estimates of mortality by age.\n\n\n\nHence, the need for a hierarchical model!8 The authors mention a variety of MRP papers going back to 2004.9\nThe authors use a large number of variables to generate the cells by:\nsex (2 categories);\nrace (4 categories);\nage-group (4 categories);\neducation (4 categories);\nstate (51 categories);\nparty ID (3 categories);\nideology (3 categories); and\n2008 vote (3 categories).\nAs such they have \\(2\\times4\\times4\\times4\\times51\\times3\\times3\\times3 = 176,256\\) cells. O.M.G. The authors provide some justification for the inclusion of each of these variables, but there was scope for more given the importance of this aspect within an MRP analysis.10\nThe Current Population Survey (CPS) is the usual go-to survey in the US in terms of post-stratification data. But the authors turn away from the CPS because ‘the CPS is missing some key poststratification variables, such as party identification.’11 Instead they use exit poll data from the 2008 presidential election.\n\nExit polls are conducted outside voting stations on election day, and record the choices of exiting voters; they are generally used by researchers and news media to analyze the demographic breakdown of the vote (after a post-election adjustment that aligns the weighted responses to the reported state-by-state election results).\n\nThe exit poll that they use is made up of 101,638 respondents.12 The authors describe how this disadvantages their analysis as they have to use data that is four years out of date (it wouldn’t be appropriate to use 2012 exit polls to forecast the 2012 election!).13\nModel\nGiven the nature of the US electorate, they use a nested modelling approach: the first models whether a respondent is likely to vote for one of Obama or Romney given various information such as state, education, sex, etc:[Romney! Wow! Remember the good old days when it was the fact that the Republican candidate was going to go out of his way to appoint lots of women to positions of power was something that was held against him.][The point was raised during discussion that to a certain extent it’s not clear why they bother with this two-level approach. How large could the number of non-major party voters be? In any case, it introduces a lot, and I’m not sure the trade-off is there.] \\[\nPr\\left(Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) =\\\\\n\\mbox{logit}^{-1}\\left(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) \n+ \\\\\n\\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}} + \\alpha_{j[i]}^{\\mbox{age}}+ \\alpha_{j[i]}^{\\mbox{race}}+ \\alpha_{j[i]}^{\\mbox{party ID}}+\\\\ \\alpha_{j[i]}^{\\mbox{ideology}}+ \\alpha_{j[i]}^{\\mbox{last vote}}\n\\right)\n\\]\nThe priors on the coefficients for each variable - “var” - are given by independent distributions: \\(N(0, \\sigma^2_{var})\\)14 and the variance parameters are assigned a hyperprior distribution: \\(\\sigma^2_{var}\\sim \\mbox{inv-}\\chi^2(\\nu,\\sigma^2_0)\\).15\nHere, again, the authors go to some effort to ‘sell’ the MRP approach by making explicit the notion of sparse cells borrowing strength. The idea is that if some cell has very little information then it’s coefficients will be drawn from an average of those cells that are similar.\nAfter establishing whether a person is likely to vote for one of Obama or Romney, they use a very similar model to consider whether, given a person is voting for one of those two, a person is voting for Obama: \\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) =\\\\\n\\mbox{logit}^{-1}\\left(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) \n+ \\\\\n\\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}} + \\alpha_{j[i]}^{\\mbox{age}}+ \\alpha_{j[i]}^{\\mbox{race}}+ \\alpha_{j[i]}^{\\mbox{party ID}}+\\\\ \\alpha_{j[i]}^{\\mbox{ideology}}+ \\alpha_{j[i]}^{\\mbox{last vote}}\n\\right)\n\\]\nAll of this is run in R (R Core Team, 2019) using ‘approximate marginal maximum likelihood estimates’ via glmer() from the lme4 package (Bates, et al, 2015).16 17 18 The estimate for each day is run on the basis of that day’s results as well as the previous four, hence introducing a degree of smoothing.19\nAfter this model is trained it is applied to the post-stratification dataset.20 Estimates are made for each cell and then aggregated up to the population based on the weight of the proportion of the electorate in each cell.\nResults - Voter intention\nOverall results\nFigure 6 compares the MRP estimate (red line) with a pollster average (blue line). The vertical dotted lines are presidential debates, the horizontal dashed line is the actual outcome.\n\n\n\nFigure 6: (This is Figure 3 in the paper.) ‘National MRP-adjusted voter intent of two-party Obama support over the 45-day period, with the associated 95% confidence bands. The horizontal dashed line indicates the actual two-party Obama vote share. The three vertical dotted lines indicate the presidential debates. Compared with the raw responses in Fig. 2, the MRP-adjusted voter intent is much more reasonable, and the voter intent in the last few days is close to the actual outcome. On the other hand, the daily aggregated polling results from Pollster.com, shown by the blue dotted line, are further away from the actual vote share than the estimates generated from the Xbox data in the last few days.’\n\n\n\nWith relation to Figure 6, the authors argue:21\n\nOn the day before the election, our estimate of voter intent is off from the actual outcome (indicated by the dotted horizontal line) by a mere 0.6 percentage points.\n\nState-specific results\nThey then disaggregate the national-level vote share into states (Figure 7). This has important considerations for the issue of importance - electoral college results. This disaggregation illustrates an important feature of MRP - that state-specific estimates can be obtained by using a state-specific post-stratification dataset. The trained model remains the same, it is only the post-stratification that changes.\n\n\n\nFigure 7: (This is Figure 4 in the paper.) ‘MRP-adjusted daily voter intent for the 12 states with the most electoral votes, and the associated 95% confidence bands. The horizontal dashed lines in each panel give the actual two-party Obama vote shares in that state. The mean and median absolute errors of the last day voter intent across the 51 Electoral College races are 2.5 and 1.8 percentage points, respectively. The state-by-state daily aggregated polling results from Pollster.com, given by the dotted blue lines, are broadly consistent with the estimates from the Xbox data.’\n\n\n\nFigure 7 illustrates the twelve states with the most electoral college votes. The authors argue that although there are similar trends between the states, there are state-specific movements, which speak to a blending of national and state-level signals. Again, the blue dotted line is a pollster average, the red dotted line is the MRP estimates, the horizontal dashed line is the eventual outcome, and the vertical dotted line is the debates.\nDemographic-specific results\nSimilarly, demographic-specific estimates can be estimated by re-weighting the post-stratification dataset. Figure 8 illustrates some important demographic features.22\n\n\n\nFigure 8: (This is Figure 5 in the paper.) ‘Comparison of the two-party Obama vote share for various demographic subgroups, as estimated from the 2012 national exit poll and from the Xbox data on the day before the election.’\n\n\n\nFinally, the results can be compared with the actual results, say on the basis of the most-important two-dimensional demographic sub-groups (left-panel of Figure 9) or illustrating their size (right panel of Figure 9).23\n\n\n\nFigure 9: (This is Figure 6 in the paper.) ‘Left panel: Differences between the Xbox MRP-adjusted estimates and the exit poll estimates for the 30 largest two-dimensional demographic subgroups, ordered by the differences. Positive values indicate that the Xbox estimate is larger than the corresponding exit poll estimate. Among these 30 subgroups, the median and mean absolute differences are 1.9 and 2.2 percentage points, respectively. Right panel: Two-party Obama support, as estimated from the 2012 national exit poll and from the Xbox data on the day before the election, for various two-way interaction demographic subgroups (e.g., 65+ year-old women). The sizes of the dots are proportional to the population sizes of the corresponding subgroups.’\n\n\n\nResults - Election outcomes\nThe survey question asked ‘if the election were held today’, which has been found to be biased in various ways. Additionally, not everyone who is able to vote actually votes. As such there is a need to translate estimates of voter intentions into estimates of election outcomes. The authors describe this as ‘calibrating’ voter intent.\nTo calibrate voter intent, they get historical data from three US presidential elections (2000, 2004, and 2008) in terms of overall national and state estimates of voter intent. They then take a moving average of the poll numbers leading up to the election. The point is to try to establish a relationship between the poll estimates and the actual outcome. The moving average of voting intent, as measured by the polls, is used as an explanatory variable for the national election day vote share of the incumbent party candidate in election year \\(e\\):24 \\[y^{US}_e = a_0 + a_1x^{US}_{t,e} + a_2|x^{US}_{t,e}|x^{US}_{t,e} + a_3tx^{US}_{t,e} + \\eta(t, e).\\] Where \\(x^{US}_{t,e}\\) is the national voter intent of the incumbent party candidate \\(t\\) days before the election in year \\(e\\), and \\(\\eta\\sim N(0,\\sigma^2)\\) is the error term.25 Both \\(y^{US}_e\\) and \\(x^{US}_{t,e}\\) are offset by 0.5 so that they run from -0.5 to 0.5.26 The fully calibrated model is run with the gls() function in the R package nlme (Pinheiro, et al 2019).\nThe authors also do the same but for state-specific election outcomes.\nState-by-state outcomes\nFigure 10 shows the estimates of the calibrated model, that is, it projects Obama’s vote share in each of the twelve largest states.\n\n\n\nFigure 10: (This is Figure 7 in the paper.) ‘Projected Obama share of the two-party vote on election day for each of the 12 states with the most electoral votes, with the associated 95% confidence bands. Compared to the MRP-adjusted voter intent in Fig. 4, the projected two-party Obama support is more stable, and the North Carolina race switches direction after applying the calibration model. In addition, the confidence bands become much wider and give more reasonable state-by-state probabilities of Obama victories.’\n\n\n\nThe issue is that it is difficult to know whether this result is reasonable - what is the appropriate comparison?27 For this reason, the authors convert the vote share estimates into probabilistic forecasts and then compare them to prediction market estimates (Figure 11).\n\n\n\nFigure 11: (This is Figure 8 in the paper.) ‘Comparison between the probabilities of Obama winning the 12 largest Electoral College races based on Xbox and prediction market data. The prediction market data are the average of the raw Betfair and Intrade prices from winner-take-all markets. The three vertical lines represent the dates of the three presidential debates. The shaded halves indicate the direction in which race went.’\n\n\n\nThey find that their probabilistic estimates are consistent with prediction markets.\nImplied electoral college outcomes\nFinally, given state estimates, it is possible to provide an estimate of the aspect of presidential elections that actually matters: electoral college outcomes. The authors begin by examining the median distribution of Electoral College votes (Figure 12).\n\n\n\nFigure 12: (This is Figure 9 in the paper.) ‘Daily projections of Obama electoral votes over the 45-day period leading up to the 2012 election, with the associated 95% confidence bands. The solid line represents the median of the daily distribution. The horizontal dashed line represents the actual electoral votes, 332, that Obama captured in 2012 election. The three vertical dotted lines indicate the dates of the three presidential debates.’\n\n\n\nTheir central estimate is quite similar to the eventual outcome. But Figure 12 also makes the MRP trade-off clear, as the distribution of the estimates is very wide and covers almost all possible outcomes.28\nThey also, more clearly illustrate the distribution of potential outcomes in the electoral college (Figure 13).\n\n\n\nFigure 13: (This is Figure 10 in the paper.) ‘The projected distribution of electoral votes for Obama one day before the election. The green vertical dotted line represents 269, the minimum number of electoral votes that Obama needed for a tie. The blue vertical dashed line indicates 332, the actual number of electoral votes captured by Obama. The estimated likelihood of Obama winning the electoral vote is 88%.’\n\n\n\nThey find that ‘[w]hile Obama actually captured 332 votes, we estimate a median of 312 votes, with the most likely outcome being 303.’29\nConclusions\nThe paper closes with a discussion of various properties that one would like elections to have: ‘not only accurate, but also relevant, timely, and cost-effective’. The authors then point out how their approach satisfies these properties, especially through the use of non-representative samples.30\nThey point to the possible use of non-representative polling in local elections, and close where they began - with a recommendation that non-representative polling be considered in a new light, despite its failure 75 years ago.\nReferences\nBates, Douglas, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. doi:10.18637/jss.v067.i01.\nGosnell, H. F. (1937). How accurate were the polls? Public Opinion Quarterly, 1, 97–105.\nHanretty, C. (2019). An introduction to multilevel regression and post-stratification for estimating constituency opinion. Political Studies Review, pages 1–16.\nKastellec, J. P., Lax, J. R., and Phillips, J. (2016). Estimating State Public Opinion With Multi-Level Regression and Poststratification using R. Working Paper.\nPinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team (2019). nlme: Linear and Nonlinear Mixed Effects Models_. R package version 3.1-143, <URL: https://CRAN.R-project.org/package=nlme>.\nR Core Team (2019). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nSquire, P. (1988). Why the 1936 Literary Digest poll failed. Public Opinion Quarterly, 52, 125–133.\nWang, W., Rothschild, D., Goel, S., & Gelman, A. (2015). Forecasting elections with non-representative polls. International Journal of Forecasting, 31(3), 980-991.\nThis paper literally changed the course of my professional life. I read it in late 2015 when I was very bored by my economics PhD. I couldn’t put it down. It seems silly now, but until then I didn’t realise that statisticians wrote papers like this, and that not only could you bring together the best aspects of politics, surveys, quantitative analysis, etc but that you could also write academic papers about it! It was the combination of everything that I was interested in. Starting in early 2016 Monica Alexander and I followed this paper and Monica implemented a version of the model in JAGS for Australia, which become: Petit Poll. We gathered our own polling data by emailing friends and family and used all of this to forecast the 2016 Australian election. (We forecast the Coalition would 80-85 seats, and they eventually won 76 seats - the electoral divisions of Bass, Braddon, Lyons, and Mayo haunt my dreams.) We went on to use MRP for various other projects, improving our understanding of it and related methods. Monica was already there, but I dove further into learning R, Bayesian statistics, and machine learning, focused on applications in political science, eventually also branching out into related techniques such as text analysis. Now almost all of my work is very much statistics applied to political science, but it all started with this paper.↩\nOne of the interesting aspects around this introduction and motivation section is that there is little discussion of the usual aspects that those who are already convinced of the MRP approach get excited about. Features such as the importance of probability distributions, and the appropriate role of uncertainty, which those of us who use MRP (and related approaches) day-to-day see as motivation, are of less concern to a broader audience. By carefully constructing the introduction and motivation section, the authors likely greatly increased the number of readers who continue through the rest of the paper.↩\nThe first use of mathematical notation does not occur until the end of the third page. This also likely helps boost the readership of this paper!↩\nI stole this name from Jack Bailey. FWIW Stan existed at this point, but had not taken off to the extent it would in the following few years.↩\nIt doesn’t really matter, but why was this number rounded, while the others are exact?↩\nThe point was raised during discussion that it would have been interesting to know the number of those who changed their mind. My experience with Australian data (as well as talking to people who are much more experienced in this area than I am) suggests there would have been very few.↩\nMonica and I have submitted an application to present a paper about this topic at a super fantastic MRP-fest being organised by Lauren Kennedy and held at Columbia in April 2020. Hopefully our proposal is accepted!↩\nThe author’s describe multi-level modelling as regularised regression. I’d be keen to explore this in a little more detail.↩\nWe’ve seen in earlier papers in this reading group that we can see echoes (that’s the wrong word; what’s the antecedent of an echo?) in much earlier work including Fay and Herriot (1979) and Little (1993) and probably a bunch of other papers. The definitive MRP history paper is probably not yet written. Lauren - are you taking submissions for MRP-fest 2021 or maybe 2022?↩\nGiven how correlated and overlapping some of these variables seem (e.g. party ID, ideology, and 2008 vote), this discussion is not as extensive as I would like. As a reviewer of the paper, I would be interested in examining the performance of a ‘cut down’ version of the model. This would have also helped with some of the dataset issues raised later.↩\n1. This seems like something that could have a big effect or at least needs a lot of justification. I would have liked to have seen much more justification of this. If I were a reviewer then I would have requested they run the model using CPS and throw away whatever variables that it doesn’t have from their model. They only mention one variable, so I just can’t see why it is worth taking the hit in being so unconventional. 2. This decision also limits the capacity of others to reproduce the results. Anyone can get CPS data. But how do I get the exit poll data that they used? What exit polls did they even use? Did I miss this? The post-stratification dataset is kind of everything.↩\nI couldn’t work out which exit poll they used? I probably just missed a description of it, but even so, I also couldn’t see that they provide the data, which would have really helped improve the reproducibility of the paper.↩\nHowever this would be another benefit of using the CPS dataset, as it is run monthly.↩\nWould the authors would still use these priors or has the state-of-the-art changed?↩\nAgain, is this what we’d still do today?↩\nMaybe I missed it but they don’t provide their code. This was very annoying in 2016 when Monica and I were trying to make our own version of the model. But these days it really limits the paper. Other ‘beginner-friendly’ MRP papers, such as Kastellec et al., 2016, or Hanretty, 2019, include all of their code which make them easier to assign when teaching.↩\nWhat effect does using glmer() have on the results? I’m assuming these days this would be fit in Stan, but how easy would it be? What do we gain?↩\nIt wasn’t clear to me whether all of this can be run at once or do they need to run each separately? If the later, how is uncertainty being propagated between the two? Does it matter? I’m sure there are good answers here that I’m just not seeing, but I can’t see it.↩\nBest I can tell each of these rolling averages for a day is run independently. Why not bring it all together?↩\nSomehow you need to bring both models to the table here, but I don’t understand whether each model is applied separately in which case what is happening to the uncertainty, or if it is possible to somehow combine it all using lme4?↩\nThey show how close their results are to the actual outcome, but at the end acknowledge that this probably isn’t the best way to evaluate MRP models and discuss other aspects. What does a ‘meaningful result’ mean in the context of MRP?↩\nthe point was raised during discussion that there should probably be some measure of uncertainty around these estimates.↩\nWe’ve talked before about these types of graphs, and how they may be better with difference between ‘actual’ and ‘estimated’ on the y-axis, while ‘actual’ remains on the x-axis.↩\nI don’t understand what has happened to uncertainty. Those poll numbers would have had an uncertainty surrounding them, but I didn’t see a mention of this. Maybe they just used the central estimate, in which case this relationship may be overly precisely measured.↩\nI don’t understand what is happening in that \\(a_2\\) coefficient, or really much of any of it. Is there a subtle difference that I’m missing? It’s the same in the states model. I understand what they are trying to do, but I don’t understand how this achieves it.↩\nI don’t understand why this is done.↩\nI’m not sure that their comparison - either here or above - is appropriate at all. By what do we evaluate an MRP model? I have thoughts on this, but am keen to develop them further.↩\nAt no point is this result really all that meaningful. Is there something better that could be done here?↩\nAgain, is this really how we should be analysing the success of our models? In any case, the point was raised during discussion that claiming that the mode is informative here is a little disengenious. The ‘second mode’ (is that a thing?), anyway, there’s not much different between the probability of any of the top five more likely outcomes, but they are enormously spread out.↩\nI would have liked more discussion about these aspects by which to evaluate an MRP model.↩\n",
    "preview": "posts/2020-02-11-a-review-of-forecasting-elections-with-non-representative-polls/page_1.png",
    "last_modified": "2020-04-26T17:18:38-04:00",
    "input_file": {},
    "preview_width": 1168,
    "preview_height": 1594
  },
  {
    "path": "posts/2019-12-04-getting_started_with_mrp/",
    "title": "Getting started with MRP",
    "description": "Multi-level regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. I recently ran a hands-on workshop at the ANU, aimed at interested, but not experienced, social scientists to help de-mystify MRP. The workshop aimed to give participants the ability and confidence to: 1) critically read papers that use it; and 2) apply it in their own work. Examples of how to implement MRP were illustrated in R using the brms package. The following post gives the outline of the workshop and the material and coding exercises covered.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2019-12-03",
    "categories": [],
    "contents": "\nTable of Contents\nOverview\nSchedule\nHelp with computer set-up.Computing\nGetting help\n\nIntroduction, motivation, and example\nLive-coding introductory example\nParticipants pair-code introductory example\nLive coding extended example\nParticipants pair-code extended example\nLive codingAdding layers\nGathering data\nCommunication\n\nConcluding remarks\nNext steps\nOverview\nMulti-level regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous.\nThe purpose of this hands-on workshop is to de-mystify MRP and give participants the ability and confidence to: 1) critically read papers that use it; and 2) apply it in their own work. Examples of how to implement MRP will be illustrated in R using the brms package. No experience with R is required but workshop participants should bring a laptop that is: a) connected to the internet; b) has R and R Studio installed, along with the tidyverse and brms packages (if you have a hassle doing this then come early to the workshop and I can help you).\nThe GitHub repo that you should download is: https://github.com/RohanAlexander/mrp_workshop.\nSchedule\n8:45 - 9:00: (Optional) Help with computer set-up.\n9:00 - 9:15: Introduction, motivation, and example.\n9:15 - 9:25: Live-coding introductory example.\n9:25 - 9:45: Participants pair-code introductory example.\n9:45 - 9:55: Live coding extended example.\n10:00 - 10:30: Participants pair-code extended example.\n10:30 - 10:50: Live example improving the workflow: gathering data from the ABS, improving the model, and communicating results.\n10:50 - 11:00: Concluding remarks about strengths, weaknesses, and potential areas of application.\nHelp with computer set-up.\nThe primary programming language used for MRP tends to be R, but any similar language would be fine. That said, if you are already comfortable with another open source language, such as Python, then it wouldn’t hurt to learn R as well. You are welcome to use whatever language you are most comfortable with, but it will be easiest for you to be able to draw on other examples if you use R. All of the examples in this workshop are in R.\nComputing\nR can be downloaded for free from: http://cran.utstat.utoronto.ca/.\nRStudio is an interface that makes using R easier and it can be downloaded for free from: https://rstudio.com/products/rstudio/download/.\nWe will use brms later in the tutorial. In order to use this your Mac needs to have Xcode and a bunch of other things installed. To do this go to: https://github.com/rmacoslib/r-macos-rtools#how-do-i-use-the-installer and within the ‘assets’ bit of the project’s release page, download ‘macos-rtools-3.1.0.pkg’ and then install that. It’ll take a few minutes because it is downloading and setting up a bunch of things.\nGetting help\nAt some point your code won’t run or will throw an error. This is normal, and it happens to everyone. It happens to me on a daily, sometimes hourly, basis. Getting frustrated is understandable. There are a few steps that are worthwhile taking when this happens:\nIf you are having issues with a particular function then the Help file for that function can be accessed by adding a ? to the front. e.g. ‘?lm’.\nIf you’re getting an error then try googling it, (I find it can help to include the term ‘R’ and ‘MRP’ or ‘tidyverse’ or the relevant package name).\nIf your code just isn’t running, then try searching for what you are trying to do, e.g. ‘save PDF of graph in R made using ggplot’. Almost always there are relevant blog posts or Stack Overflow answers that will help.\nTry to restart R and R Studio and load everything again.\nTry to restart your computer.\nThere are a few small mistakes that I often make and may be worth checking in case you make them too:\ncheck the class e.g. class(my_dataset$its_column) to make sure that is what it should be;\nwhen you’re using ggplot make sure you use ‘+’ not ‘%>%’;\ncheck whether you are using ‘.’ when you shouldn’t be, or vice versa.\nIt’s almost always helpful to take a break and come back the next day.\nIntroduction, motivation, and example\nMulti-level regression with post-stratification (MRP) is a handy approach when dealing with survey data. Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages:\nIt can allow us to ‘re-weight’ in a way that includes uncertainty front-of-mind and isn’t hamstrung by small samples.\nIt can allow us to use broad surveys to speak to subsets.\nFrom a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. As Lauren Kennedy points out, ‘MRP has traditionally been used in probability surveys and had potential for non-probability surveys, but we’re not sure of the limitations at the moment.’\nOne famous example is Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman, 2014, ‘Forecasting elections with non-representative polls’, International Journal of Forecasting. They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election.\nKey facts about the set-up:\nData from an opt-in poll which was available on the Xbox gaming platform during the 45 days preceding the 2012 US presidential election.\nEach day there were three to five questions, including voter intention: “If the election were held today, who would you vote for?”\nRespondents were allowed to answer at most once per day.\nFirst-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.\nIn total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls\nYoung men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.\nGiven the US electorate, they use a two-stage modelling approach. The details don’t really matter too much, and essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc: \\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) \n+ \\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}}...\n)\n\\] They run this in R using glmer() from lme4.\nHaving a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these “cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).”\nThis means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not an option for Australia in general).\nThey make state-specific estimates by post-stratifying to the features of each state. \nSimilarly, they can examine demographic-differences. \nFinally, they convert their estimates into electoral college estimates. \nLive-coding introductory example\nThe workflow that we are going to use is:\nread in the poll;\nmodel the poll;\nread in the post-stratification data; and\napply the model to the post-stratification data.\nFirst load the packages.\n\n\n# Uncomment these (by deleting the #) if you need to install the packages\n# install.packages(\"broom\")\n# install.packages(\"here\")\n# install.packages(\"skimr\")\n# install.packages(\"tidyverse\")\n\nlibrary(broom) # Helps make the regression results tidier\nlibrary(here) # Helps make file referencing easier.\nlibrary(skimr) # Helps summarise the data\nlibrary(tidyverse) # Helps make programming with R easier\n\nThen load some sample polling data to analyse. I have generated this fictitious data so that we have some idea of what to expect from the model. The dependent variable is supports_ALP, which is a binary variable - either 0 or 1. We’ll just use two independent variables here: gender, which is either Female or Male (as that is what is available from the ABS); and age_group, which is one of four groups: ages 18 to 29, ages 30 to 44, ages 45 to 59, ages 60 plus.\n\n\nexample_poll <- read_csv(\"outputs/data/example_poll.csv\") # Here we read in a \n# CSV file and assign it to a dataset called 'example_poll'\n\nhead(example_poll) # Displays the first 10 rows\n\n# A tibble: 6 x 4\n  gender age_group  supports_ALP state\n  <chr>  <chr>             <dbl> <chr>\n1 Male   ages30to44            0 NSW  \n2 Female ages45to59            0 NSW  \n3 Female ages60plus            1 VIC  \n4 Male   ages30to44            1 QLD  \n5 Female ages30to44            1 QLD  \n6 Female ages18to29            1 VIC  \n\n# Look at some summary statistics to make sure the data seem reasonable\nsummary(example_poll) \n\n    gender           age_group          supports_ALP   \n Length:5000        Length:5000        Min.   :0.0000  \n Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Median :1.0000  \n                                       Mean   :0.5514  \n                                       3rd Qu.:1.0000  \n                                       Max.   :1.0000  \n    state          \n Length:5000       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nskimr::skim(example_poll)\n(#tab:initial_model_simulate_data)Data summary\nName\nexample_poll\nNumber of rows\n5000\nNumber of columns\n4\n_______________________\n\nColumn type frequency:\n\ncharacter\n3\nnumeric\n1\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ngender\n0\n1\n4\n6\n0\n2\n0\nage_group\n0\n1\n10\n10\n0\n4\n0\nstate\n0\n1\n2\n3\n0\n8\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nsupports_ALP\n0\n1\n0.55\n0.5\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\nI generated this polling data to make both made males and older people less likely to vote for the Australian Labor Party; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset.\n\n\n# The '%>%' is called a 'pipe' and it takes whatever the output is of the \n# command before it, and pipes it to the command after it.\nexample_poll %>% # So we are taking our example_poll dataset and using it as an \n  # input to 'summarise'.\n   # summarise reduces the dimensions, so here we will get one number from a column.\n  summarise(raw_ALP_prop = sum(supports_ALP) / nrow(example_poll))\n\n# A tibble: 1 x 1\n  raw_ALP_prop\n         <dbl>\n1        0.551\n\nNow we’d like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is: \\[\n\\mbox{ALP support}_j = \\mbox{gender}_j + \\mbox{age_group}_j + \\epsilon_j.\n\\]\nThis model says that the probability that some person, \\(j\\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party.\n\n\n# Here we are running an OLS regression with supports_ALP as the dependent variable \n# and gender and age_group as the independent variables. The dataset that we are \n# using is example_poll. We are then saving that OLS regression to a variable called 'model'.\nmodel <- lm(supports_ALP ~ gender + age_group, \n            data = example_poll\n            )\n\n# broom::tidy just displays the outputs of the regression in a nice table.\nbroom::tidy(model) \n\n# A tibble: 5 x 5\n  term                estimate std.error statistic   p.value\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)            0.900    0.0131      68.8 0.       \n2 genderMale            -0.205    0.0142     -14.4 2.69e- 46\n3 age_groupages30to44   -0.186    0.0176     -10.6 6.50e- 26\n4 age_groupages45to59   -0.402    0.0177     -22.7 8.29e-109\n5 age_groupages60plus   -0.585    0.0175     -33.4 5.20e-221\n\nEssentially we’ve got our inputs back. We just used regular OLS even though our dependent variable is a binary. (It’s usually fine to start with an OLS model and then iterate toward an approach that may be more appropriate such as logistic regression or whatever, but where the results are a little more difficult to interpret.) If you wanted to do that then the place to start would be glmer() from the R package lme4, and we’ll see that in the next section.\nMonica is horrified by the use of OLS here, and wants it on the record that she regrets not making not doing this part of our marriage vows.\nNow we’d like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features.\nFirst read in some real demographic data, on a seat basis, from the ABS (we’ll go into the process of getting this later).\n\n\ncensus_data <- read_csv(\"outputs/data/census_data.csv\")\nhead(census_data)\n\n# A tibble: 6 x 5\n  state gender age_group  number cell_prop_of_division_total\n  <chr> <chr>  <chr>       <dbl>                       <dbl>\n1 ACT   Female ages18to29  34683                       0.125\n2 ACT   Female ages30to44  42980                       0.155\n3 ACT   Female ages45to59  33769                       0.122\n4 ACT   Female ages60plus  30322                       0.109\n5 ACT   Male   ages18to29  34163                       0.123\n6 ACT   Male   ages30to44  41288                       0.149\n\nWe’re just going to do some rough forecasts. For each gender and age_group we want the relevant coefficient in the example_data and we can construct the estimates.\n\n\n# Here we are making predictions using our model with some new data from the \n# census, and we saving the results of those predictions by adding a new column \n# to the census_data dataset called 'estimate'.\ncensus_data$estimate <- \n  model %>% \n  predict(newdata = census_data)\n\ncensus_data %>% \n  mutate(alp_predict_prop = estimate*cell_prop_of_division_total) %>% \n  group_by(state) %>% \n  summarise(alp_predict = sum(alp_predict_prop))\n\n# A tibble: 8 x 2\n  state alp_predict\n  <chr>       <dbl>\n1 ACT         0.525\n2 NSW         0.495\n3 NT          0.541\n4 QLD         0.496\n5 SA          0.479\n6 TAS         0.464\n7 VIC         0.503\n8 WA          0.503\n\nWe now have post-stratified estimates for each division. Our model has a fair few weaknesses. For instance small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it.\nParticipants pair-code introductory example\nPlease break into pairs and with one person ‘driving’ (typing) and the other person ‘navigating’, and attempt to pair-code the introductory example.\nIf you run into issues then I am happy to help point you in the right direction. The full code of the example will be made available after the workshop, so it doesn’t matter if you’re not able to complete the example now.\nAs a reminder, our workflow is:\nread in the poll;\nmodel the poll;\nread in the post-stratification data;\napply your model to the post-stratification data.\nGet started by opening the Rproj file from the workshop repo and opening a new R script.\nLive coding extended example\nWe’d like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are crticial, and it’s not appropriate to only report central estimates. To do this we’ll use the same broad approach as before, but just improving bits of our workflow.\nFirst load the packages.\n\n\n# Uncomment these if you need to install the packages\n# install.packages(\"broom\")\n# install.packages(\"brms\")\n# install.packages(\"here\") \n# install.packages(\"tidybayes\")\n# install.packages(\"tidyverse\") \n\nlibrary(broom)\nlibrary(brms) # Used for the modelling\nlibrary(here)\nlibrary(tidybayes) # Used to help understand the modelling estimates\nlibrary(tidyverse) \n\nAs before, read in the polling dataset.\n\n\nexample_poll <- read_csv(\"outputs/data/example_poll.csv\")\n\nhead(example_poll)\n\n# A tibble: 6 x 4\n  gender age_group  supports_ALP state\n  <chr>  <chr>             <dbl> <chr>\n1 Male   ages30to44            0 NSW  \n2 Female ages45to59            0 NSW  \n3 Female ages60plus            1 VIC  \n4 Male   ages30to44            1 QLD  \n5 Female ages30to44            1 QLD  \n6 Female ages18to29            1 VIC  \n\nNow, using the same basic model as before, but we move it to a setting that acknowledges the dependent variable as being binary, and in a Bayesian setting.\n\n\nmodel <- brm(supports_ALP ~ gender + age_group, \n             data = example_poll, \n             family = bernoulli(),\n             file = \"outputs/model/brms_model\"\n             )\n\nmodel <- read_rds(\"outputs/model/brms_model.rds\")\n\nsummary(model)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: supports_ALP ~ gender + age_group \n   Data: example_poll (Number of observations: 5000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept               2.07      0.09     1.91     2.23 1.00\ngenderMale             -1.06      0.07    -1.20    -0.91 1.00\nage_groupages30to44    -1.10      0.10    -1.29    -0.91 1.00\nage_groupages45to59    -2.04      0.10    -2.23    -1.85 1.00\nage_groupages60plus    -2.88      0.10    -3.09    -2.68 1.00\n                    Bulk_ESS Tail_ESS\nIntercept               2240     2194\ngenderMale              3403     2595\nage_groupages30to44     2483     2805\nage_groupages45to59     2521     3061\nage_groupages60plus     2517     2858\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe’ve moved to the Bernoulli distribution, so we have to do a bit more work to understand our results, but we are broadly getting back what we’d expect.\nAs before, we’d like an estimate for each state based on their demographic features and start by reading in the data.\n\n\ncensus_data <- read_csv(\"outputs/data/census_data.csv\")\nhead(census_data)\n\n# A tibble: 6 x 5\n  state gender age_group  number cell_prop_of_division_total\n  <chr> <chr>  <chr>       <dbl>                       <dbl>\n1 ACT   Female ages18to29  34683                       0.125\n2 ACT   Female ages30to44  42980                       0.155\n3 ACT   Female ages45to59  33769                       0.122\n4 ACT   Female ages60plus  30322                       0.109\n5 ACT   Male   ages18to29  34163                       0.123\n6 ACT   Male   ages30to44  41288                       0.149\n\nWe’re just going to do some rough forecasts. For each gender and age_group we want the relevant coefficient in the example_data and we can construct the estimates (this code is from Monica Alexander, https://www.monicaalexander.com/posts/2019-08-07-mrp/).\n\n\npost_stratified_estimates <- \n  model %>% \n  tidybayes::add_predicted_draws(newdata = census_data) %>% \n  rename(alp_predict = .prediction) %>% \n  mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) %>% \n  group_by(state, .draw) %>% \n  summarise(alp_predict = sum(alp_predict_prop)) %>% \n  group_by(state) %>% \n  summarise(mean = mean(alp_predict), \n            lower = quantile(alp_predict, 0.025), \n            upper = quantile(alp_predict, 0.975))\n\npost_stratified_estimates\n\n# A tibble: 8 x 4\n  state  mean lower upper\n  <chr> <dbl> <dbl> <dbl>\n1 ACT   0.526 0.243 0.791\n2 NSW   0.491 0.214 0.750\n3 NT    0.538 0.236 0.852\n4 QLD   0.493 0.215 0.762\n5 SA    0.478 0.201 0.761\n6 TAS   0.460 0.183 0.757\n7 VIC   0.501 0.224 0.766\n8 WA    0.506 0.219 0.769\n\nWe now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we’d need to get new cell counts), or adding some layers.\nParticipants pair-code extended example\nPlease break into the same pairs as before, but swap who is typing, and attempt to pair-code the extended example.\nIf you run into issues then I am happy to help point you in the right direction. The full code of the example will be made available after the workshop, so it doesn’t matter if you’re not able to complete the example now.\nAs a reminder, our workflow is:\nread in the poll;\nmodel the poll;\nread in the post-stratification data;\napply your model to the post-stratification data.\nLive coding\nI will now briefly demonstrate some other aspects that may be useful to improve three aspects of our MRP workflow:\n(Workflow step 2) adding some more complexity to our model; and\n(Workflow step 3) gathering and preparing some data from the ABS that we could use to post-stratify on.\nWe will also add a fifth stage to our workflow: Communicating our results.\nAdding layers\nWe may like to try to add some layers to our model. For instance, we may like a different intercept for each state.\n\n\nmodel_states <- brm(supports_ALP ~ gender + age_group + (1|state), \n                    data = example_poll, \n                    family = bernoulli(),\n                    file = \"outputs/model/brms_model_states\",\n                    control = list(adapt_delta = 0.90)\n                    )\nsummary(model_states)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: supports_ALP ~ gender + age_group + (1 | state) \n   Data: example_poll (Number of observations: 5000) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~state (Number of levels: 8) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)     0.06      0.05     0.00     0.20 1.00     1553\n              Tail_ESS\nsd(Intercept)     2072\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept               2.07      0.09     1.90     2.26 1.00\ngenderMale             -1.06      0.08    -1.21    -0.91 1.00\nage_groupages30to44    -1.10      0.10    -1.30    -0.90 1.00\nage_groupages45to59    -2.04      0.10    -2.24    -1.84 1.00\nage_groupages60plus    -2.89      0.10    -3.10    -2.69 1.00\n                    Bulk_ESS Tail_ESS\nIntercept               1660     2273\ngenderMale              4106     2833\nage_groupages30to44     2110     2566\nage_groupages45to59     2058     2347\nage_groupages60plus     2201     2581\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nbroom::tidy(model_states, par_type = \"varying\")\n\n       term group level      estimate  std.error       lower\n1 Intercept state   ACT -0.0168699419 0.07074301 -0.14969628\n2 Intercept state   NSW  0.0017210052 0.04612268 -0.07223933\n3 Intercept state    NT  0.0153301257 0.07586938 -0.08509056\n4 Intercept state   QLD  0.0249702604 0.05321612 -0.04216737\n5 Intercept state    SA  0.0150266070 0.05864962 -0.06881398\n6 Intercept state   TAS -0.0146658602 0.07317706 -0.14542943\n7 Intercept state   VIC -0.0263030593 0.05188695 -0.12296610\n8 Intercept state    WA -0.0006364996 0.05548879 -0.09364575\n       upper\n1 0.07652361\n2 0.07734117\n3 0.14549805\n4 0.12297713\n5 0.12224758\n6 0.08379179\n7 0.04212995\n8 0.08755071\n\nbroom::tidy(model_states, par_type = \"non-varying\", robust = TRUE)\n\n                 term  estimate  std.error     lower      upper\n1           Intercept  2.069011 0.08988424  1.919983  2.2237171\n2          genderMale -1.058954 0.07408394 -1.186584 -0.9351159\n3 age_groupages30to44 -1.097002 0.10370072 -1.264609 -0.9257060\n4 age_groupages45to59 -2.038541 0.10100761 -2.205630 -1.8753367\n5 age_groupages60plus -2.884174 0.10356840 -3.063525 -2.7166738\n\nOne interesting aspect is that our multi-level approach will allow us to deal with small cell counts by borrowing information from other cells.\n\n\nexample_poll %>% \n  count(state)\n\n# A tibble: 8 x 2\n  state     n\n  <chr> <int>\n1 ACT     107\n2 NSW    1622\n3 NT       50\n4 QLD     982\n5 SA      359\n6 TAS     105\n7 VIC    1285\n8 WA      490\n\nAt the moment we have 50 respondents in the Northern Territory, 105 in Tasmania, and 107 in the ACT. Even if we were to remove most of the, say, 18 to 29 year old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents.\nGathering data\nGetting data tends to be the most troublesome aspect. I’ve found that the census is fairly useful although it can require some trade-offs (e.g. if you are doing political work then it’s not exactly the same as the electoral roll even if you restrict it to Australian citizens aged at least 18). Nonetheless, I’ve found the best way to get the sub-cell counts is to use ABS TableBuilder. There are two versions - ‘basic’ which is free, and ‘pro’, which is normally $2,510 per year, but which you can get access to if you’re associated with an Australian university.\nTableBuilder front page.Once you create an account then you can access census data for 2006, 2011, and 2016. (The ABS have relatively recently done some linking between censuses so there is actually some linked data, which is exciting).\nTableBuilder after logging in.The website is a bit cumbersome, but considering what they provide it is worth sticking with it. I usually use ‘Counting Persons, Place of Usual Residence’, but sometimes ‘Counting Persons, Place of Enumeration’ is also handy.\nTableBuilder selecting which census.We want to create a new table, and we do this by specifying the columns and rows.\nTableBuilder selecting columns.Once you have the set-up that you want then you can retrieve the data.\nTableBuilder selecting rows.You can download the data in various Excel, CSV, and other formats. If your dataset is large then you may need to submit for it to be built, which can take a day or two. Finally, if your sub-cell counts are especially small, then they will be blown around by the randomness that the ABS adds.\nTableBuilder downloading data and creating custom groups.Helpfully you can create custom groupings for geography, say to load specific electorates, and other aspects, such as age-groups. To get started with this, select ‘Custom data’.\nCommunication\nThere are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate.\n\n\npost_stratified_estimates %>% \n  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  ylab(\"Proportion ALP support\") + \n  xlab(\"State\") + \n  geom_point(data = example_poll %>% \n               group_by(state, supports_ALP) %>%\n               summarise(n = n()) %>% \n               group_by(state) %>% \n               mutate(prop = n/sum(n)) %>% \n               filter(supports_ALP==1), \n             aes(state, prop, color = \"Raw data\")) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank())\n\n\nSimilarly, we may like to plot the distribution of the coefficients.\n\nYou can work out which coefficients to be pass to gather_draws by using tidybayes::get_variables(model). (In this example I passed ‘b_.’, but the ones of interest to you may be different.)\n\n\nmodel %>%\n  gather_draws(`b_.*`, regex=TRUE) %>%\n  ungroup() %>%\n  mutate(coefficient = stringr::str_replace_all(.variable, c(\"b_\" = \"\"))) %>%\n  mutate(coefficient = forcats::fct_recode(coefficient,\n                                           Intercept = \"Intercept\",\n                                           `Is male` = \"genderMale\",\n                                           `Age 30-44` = \"age_groupages30to44\",\n                                           `Age 45-59` = \"age_groupages45to59\",\n                                           `Age 60+` = \"age_groupages60plus\"\n                                           )) %>% \n\n# both %>% \n  ggplot(aes(y=fct_rev(coefficient), x = .value)) + \n  ggridges::geom_density_ridges2(aes(height = ..density..),\n                                 rel_min_height = 0.01, \n                                 stat = \"density\",\n                                 scale=1.5) +\n  xlab(\"Distribution of estimate\") +\n  ylab(\"Coefficient\") +\n  scale_fill_brewer(name = \"Dataset: \", palette = \"Set1\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  theme(legend.position = \"bottom\")\n\n\nConcluding remarks\nIn general, MRP is a good way to accomplish specific aims, but it’s not without trade-offs. If you have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if you are concerned about uncertainty then it is a good way to think about that. If you have a biased survey then it’s a great place to start, but it’s not a panacea.\nThere’s not a lot of work that’s been done using Australian data, so there’s plenty of scope for exciting work. I look forward to seeing what you do with it!\nNext steps\nThere are a lot of resources out there that would make great next steps. I recommend having a look at the following resources to see which speaks best to your interests and background.\nAlexander, Monica, 2019, ‘Analyzing name changes after marriage using a non-representative survey’, available at: https://www.monicaalexander.com/posts/2019-08-07-mrp/.\nKennedy, Lauren, and Jonah Gabry, 2019, ‘MRP with rstanarm’, available at: https://mc-stan.org/rstanarm/articles/mrp.html.\nKennedy, Lauren, and Andrew Gelman, 2019, ‘Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample’, available at: https://arxiv.org/abs/1906.11323.\nKastellec, Jonathan, Jeffrey Lax, and Justin Phillips, 2016, ‘Estimating State Public Opinion With Multi-Level Regression and Poststratification using R’, available at: https://scholar.princeton.edu/sites/default/files/jkastellec/files/mrp_primer.pdf.\nHanretty, Chris, 2019, ‘An introduction to multilevel regression and post-stratification for estimating constituency opinion’, available at: https://journals.sagepub.com/doi/abs/10.1177/1478929919864773.\nDownes, Marnie, Lyle Gurrin, Dallas English, Jane Pirkis, Dianne Currier, Matthew Spittal, and John Carlin, 2018, ‘Multilevel Regression and Poststratification: A Modeling Approach to Estimating Population Quantities From Highly Selected Survey Samples’, available at: https://www.ncbi.nlm.nih.gov/pubmed/29635276.\nJackman, Simon, Shaun Ratcliff, and Luke Mansillo, 2019, ‘Small area estimates of public opinion: Model-assisted post-stratification of data from voter advice applications’, available at: https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf\n(Self-promotion alert) Alexander, Rohan, Patrick Dumont, and Patrick Leslie, 2019, ‘Forecasting Multi-District Election’, available at: https://github.com/RohanAlexander/ForecastingMultiDistrictElections.\nIf you don’t have survey data, then there is some individual-level data available on the Australian Data Archive: https://ada.edu.au. You will need to request access to the datasets, but they are very keen for people to use their data and will help you through the process if needed.\n\n\n",
    "preview": "posts/2019-12-04-getting_started_with_mrp/getting-started-with-mrp_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-04-26T17:18:17-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-01-03-gathering-and-analysing-text-data/",
    "title": "Gathering and analysing text data",
    "description": "Text modelling is an exciting area of research. But many guides assume that you already have a nice dataset. Similarly, web scraping is an exciting way to get information, but not many explanations go on to explain what you could do with it. This post attempts to go from scraping text from a website through to modelling the topics. It's not meant to be an exhaustive post, but should hopefully provide enough that you can get started with your own project and know where to go for more information.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2019-01-03",
    "categories": [],
    "contents": "\nTable of Contents\nIntroduction\nGathering data\nAnalysing data\nIntroduction\nText modelling is an exciting area of research. But many guides assume that you already have a nice dataset. Similarly, web scraping is an exciting way to get information, but not many explanations go on to explain what you could do with it. This post attempts to go from scraping text from a website through to modelling the topics. It’s not meant to be an exhaustive post, but should hopefully provide enough that you can get started with your own project and know where to go for more information.\nThe example that I’m going to use is getting data from the minutes of the RBA board meeting.\nGathering data\nThe first step is to get some data. I’m going to use the rvest package to do the web scraping. When you are scraping data you should try to be polite - slow down your requests as much as possible, avoid times you know they’ll have a lot of traffic, and check if the website has an API or a robots.txt file (usually access that at domain.com/robots.txt) that provides guidance.\n\n\n# install.packages(\"rvest\")\nlibrary(rvest)\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# Read in the list of the website addresses\ndata_to_scrape <- read_csv(\"inputs/addresses.csv\") # Just a list of the URLs \n# and dates for each minutes.\naddress_to_visit <- data_to_scrape$address\nsave_name <- data_to_scrape$save_name\n\n# Create the function that will visit address_to_visit and save to save_name files\nvisit_address_and_save_content <-\n  function(name_of_address_to_visit,\n           name_of_file_to_save_as) {\n    # The function takes two inputs\n    read_html(name_of_address_to_visit) %>% # Go to the website and read the html\n      html_node(\"#content\") %>% # Find the content part\n      html_text() %>% # Extract the text of the content part\n      write_lines(name_of_file_to_save_as) # Save as a text file\n    print(paste(\"Done with\", name_of_address_to_visit, \"at\", Sys.time()))  \n    # Helpful so that you know progress when running it on all the records\n    Sys.sleep(sample(30:60, 1)) # Space out each request by somewhere between \n    # 30 and 60 seconds each so that we don't overwhelm their server\n  }\n\n# If there is an error then ignore it and move to the next one\nvisit_address_and_save_content <-\n  safely(visit_address_and_save_content)\n\n# Walk through the addresses and apply the function to each\nwalk2(address_to_visit,\n      save_name,\n      ~ visit_address_and_save_content(.x, .y)) \n\nThe CSV with the addresses and save names that we use looks something like this:\n\naddress\nsave_name\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-11-06.html\ninputs/minutes/2018-11-06.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-10-02.html\ninputs/minutes/2018-10-02.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-09-04.html\ninputs/minutes/2018-09-04.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-08-07.html\ninputs/minutes/2018-08-07.txt\nhttps://www.rba.gov.au/monetary-policy/rba-board-minutes/2018/2018-07-03.html\ninputs/minutes/2018-07-03.txt\n\nAnalysing data\nIn this example we’ll use a whole bunch of packages so that you can see what’s available. In general probably stringr, quanteda and stm are the workhorse packages with others used as needed.\n\n\n#### Workspace set-up ####\n# install.packages(\"broom\")\nlibrary(broom) # Used to clean up results\n# install.packages(\"devtools\")\nlibrary(devtools)\n# devtools::install_github(\"DavisVaughan/furrr\")\nlibrary(furrr) # Used to do parallel processing with the topic models\nplan(multiprocess)\n# install.packages(\"quanteda\")\nlibrary(quanteda) # Used for data cleaning\n# install.packages(\"readtext\")\nlibrary(readtext) # Used to read in the txt files that were scraped\n# install.packages(\"stm\")\nlibrary(stm) # Used for more interesting topic models\n# install.packages(\"tictoc\")\nlibrary(tictoc) # Used for timing\n# install.packages(\"tidytext\")\nlibrary(tidytext)\n# install.packages(\"tidyverse\")\nlibrary(tidyverse) # Used for everything\n# install.packages('topicmodels')\nlibrary(topicmodels) # Used to make basic topic models\n\n# Read in the text that we scraped earlier\ntext <- readtext::readtext(\"inputs/minutes/*.txt\") # readtext makes this easy, \n# but could also use the usual base approach of listing files that end in txt etc.\n\nIn general you’ll often need to do a lot of cleaning before you can do the stats bit and get results. Here, I’ll just show two example steps. I’ve found that cleaning the dataset seems to take about 80 per cent of the time.\n\n\n#### Clean data ####\n# Do some basic cleaning - remove puncuation and change everything to lower case\ntext$text <- str_to_lower(text$text)\ntext$text <- str_replace_all(text$text, \"[:punct:]\", \" \")\n\nNow that we have a plausibly clean dataset (of course you’d want to come back and clean it more if you were actually interested in analysing the RBA minutes), we can try a topic model. Topic models are essentially just summaries. Instead of a document becoming a collection of words, they become a collection of topics with some probability associated with each topic.\n\n\n#### First topic modelling ####\n# Convert the corpus to a form that the topic model can work with\nrba_minutes <- quanteda::corpus(text) %>% # Minimum viable conversion\n  quanteda::dfm(remove_punct = TRUE, remove = stopwords('en')) %>% # Get rid of\n  # punctuation (in case you didn't already do that) and stop words - check \n  # those stop words assumptions\n  quanteda::dfm_trim(min_termfreq = 2, # Remove any word that doesn't occur at \n                     # least twice\n           min_docfreq = 2) # Get rid of any word that isn't in at least two documents\n\n# Run the topic model with 10 topics\ndtm <- quanteda::convert(rba_minutes, to = \"topicmodels\") # Getting the dfm \n# into a form that topicmodels can deal with\nlda_topics <- topicmodels::LDA(dtm, k = 10) # The k is the number of topics - \n# this decision has a big impact\n\n# Have a look at the terms\nterms(lda_topics, 10) # Top 10 words for each topic. Topics are just \n# probability distributions over words so you should look at different numbers of words\n\nLooking at the words in the topics, it seems as though “per” and “cent” are being treated as separate words. The RBA is proud that it separates “per” and “cent”, and if you’re a grad there that’ll stick with you for a while (see earlier paragraphs), but for our purposes they are one word and we need to combine them.\n\n\n#### Clean data ####\n# Let's deal with the first issue first.\ntext$text <- stringr::str_replace_all(text$text, \"per cent\", \"per_cent\")\ntext$text <- stringr::str_replace_all(text$text, \"per cent\", \"per_cent\")\n\n# You could run the topic model again if you wanted.\n\nRight, that issue of per cent has been fixed, but what if there are combinations of words like this that don’t show up very high in the topics? To identify these we need to construct n-grams. Earlier with ‘per’ ‘cent’, we generated a 2-gram. Quanteda and the tidyverse makes it easy to identify popular n-grams (if your dataset is large then I’d work with a sample of it because these can get a little unwieldy, and we only really care about the popular ones anyway). Our text is in sentences, paragraphs, etc, and we first need to break it down into tokens (essentially separate words). There’s a wonderful set of tutorials put together by the quanteda team here: https://tutorials.quanteda.io and the code for this section is from: https://tutorials.quanteda.io/basic-operations/tokens/tokens_ngrams/.\n\n\n#### Adjusting for common co-location ####\ntoks <- tokens(text$text)\n# First generate 2-grams\nngrams <- tokens_ngrams(toks, n = 2:4)\n# Somewhat annoyingly for our purposes (although understandably given the broader \n# picture) quanteda puts tokens into its own class, so we need ot convert in \n# order to use the usual tidyverse tools that we may be more familiar with.\n# As a side note, I often find it worthwhile to checking class in R when there's \n# an issue because often that's part of the issue, in this case: class(ngrams).\n# The tokens class seems to just be a list, so we can unlist it and then put it \n# into a more-friendly tibble.\nngram_counts <- tibble(ngrams = unlist(ngrams)) %>% \n  count(ngrams, sort = TRUE)\n\n# We can identify a bunch of obvious replacements. If we start getting a long \n# list then we can create a file that holds the replacement.\ntext$text <- stringr::str_replace_all(text$text, \"assistant governor\", \"assistant_governor\")\ntext$text <- stringr::str_replace_all(text$text, \"reserve bank board\", \"reserve_bank_board\")\ntext$text <- stringr::str_replace_all(text$text, \"unemployment rate\", \"unemployment_rate\")\ntext$text <- stringr::str_replace_all(text$text, \"national accounts\", \"national_accounts\")\ntext$text <- stringr::str_replace_all(text$text, \"australian dollar\", \"australian_dollar\")\ntext$text <- stringr::str_replace_all(text$text, \"monetary policy\", \"monetary_policy\")\ntext$text <- stringr::str_replace_all(text$text, \"united states\", \"united_states\")\ntext$text <- stringr::str_replace_all(text$text, \"exchange rate\", \"exchange_rate\")\ntext$text <- stringr::str_replace_all(text$text, \"glenn stevens\", \"glenn_stevens\")\ntext$text <- stringr::str_replace_all(text$text, \"reserve bank\", \"reserve_bank\")\ntext$text <- stringr::str_replace_all(text$text, \"cash rate\", \"cash_rate\")\ntext$text <- stringr::str_replace_all(text$text, \"us dollar\", \"us_dollar\")\ntext$text <- stringr::str_replace_all(text$text, \"iron ore\", \"iron_ore\")\n\nrm(toks, ngrams, ngram_counts)\n\nTake a look at the topics again. Notice that ‘growth’ is in essentially every topic. So is ‘members’ and a couple of others. It’s not that growth isn’t important (insert standard economist joke here), but the fact that ‘members’ shows up suggests that these may just be due to the way that language is used at the RBA, rather than communicating topics. If you read these minutes, you’ll know that the RBA starts a LOT of sentences with ‘Members noted…’. What does this mean for our purposes? Essentially, if you look at each topic by itself they seem ‘coherent’, but taken as a group it seems as though the topics are too similar. Another way to say that is that the words lack ‘exclusivity’. This is a common tradeoff, and our results suggest that it may be worthwhile for us to reduce some of the coherence in order to increase the exclusivity. At this point, we’ll use a different package for creating topic models - the STM package - because it has a bunch of nice features that you might like to take advantage of in future work.\n\n\n#### Introducing STM and quanteda ####\nrba_minutes <- quanteda::corpus(text) %>% # Minimum viable conversion\n  quanteda::dfm(remove_punct = TRUE, \n                 remove_numbers = TRUE,\n                 remove = stopwords('en')) %>% # Get rid of punctuation (in \n  # case you didn't already do that) and stop words - check those stop words assumptions\n  quanteda::dfm_trim(min_termfreq = 2, # Remove any word that doesn't occur at least twice\n                     min_docfreq = 0.05, # Get rid of any word that isn't in at \n                     # least 5 per cent of documents\n                     max_docfreq = 0.90, # Get rid of any word that is in at \n                     # least 90 per cent of documents\n                     docfreq_type = \"prop\" # Above we specified percentages - you \n                     # could specify counts or ranks\n                     ) \n\n# We can run the topic model using STM\ntopics_stm <- stm(rba_minutes, K = 10)\n# Looking at the results you can see that the results are fairly similar to \n# those that we got from the topicmodels package, which is what we want. \nlabelTopics(topics_stm)\nrm(topics_stm)\n# If we were interested in the results then we might like to pre-process the text \n# a little more, for instance removing the names of months.\n\nOther than pre-processing decisions, the other major determininat of the outputs of topic models is the number of topics specified. There are a bunch of diagnostic tests that have been developed to help with this decision and we can use some nice code from Julia Silge (https://juliasilge.com/blog/evaluating-stm/) to try a bunch of different values for the number of topics.\n\n\n#### Deciding on the number of topics ####\ntic(\"With parallel\") # This allows us to time the code\nmany_models <- data_frame(K = seq(5, 20, by = 5)) %>% # Here we're running four \n  # topic models: 5 topics, 10 topics, 15 topics and 20 topics\n  mutate(topic_model = future_map(K, ~stm(rba_minutes, \n                                          K = .,\n                                          verbose = FALSE)))\ntoc()\n\n# You can also try setting K to zero within STM and seeing the number of topics \n# that it recommends: e,g, choose_topic_num_for_me <- stm(rba_minutes, K = 0, verbose = FALSE)\n\n# We want to compare those models with different numbers of topics using various diagnostics.\nheldout <- make.heldout(rba_minutes) # First create a test/training set\n\nk_result <- many_models %>%\n  mutate(exclusivity = map(topic_model, exclusivity), # How unique are words to the topics\n         semantic_coherence = map(topic_model, semanticCoherence, rba_minutes), # How \n         # much the topics tend to be coherent if we look at them (usually a \n         # tradeoff with exclusivity)\n         eval_heldout = map(topic_model, eval.heldout, heldout$missing),\n         residual = map(topic_model, checkResiduals, rba_minutes),\n         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),\n         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),\n         lbound = bound + lfact,\n         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))\n\nPut these diagnostics into a nice summary graph (again code is Julia’s originally).\n\n\nk_result %>%\n  transmute(K,\n            `Lower bound` = lbound,\n            Residuals = map_dbl(residual, \"dispersion\"),\n            `Semantic coherence` = map_dbl(semantic_coherence, mean),\n            `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %>%\n  gather(Metric, Value, -K) %>%\n  ggplot(aes(K, Value, color = Metric)) +\n  geom_line(show.legend = FALSE) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(x = \"K (number of topics)\",\n       y = NULL,\n       title = \"Model diagnostics by number of topics\") +\n  theme_minimal()\n\n\nIn general we are looking for the max/min of parabolas, so our results suggest we may be best with some more topics (go to Julia’s post for to see another example: https://juliasilge.com/blog/evaluating-stm/.\n\n\n# Have a look at that exclusivity to coherence tradeoff\nk_result %>%\n  select(K, exclusivity, semantic_coherence) %>%\n  unnest() %>%\n  mutate(K = as.factor(K)) %>%\n  ggplot(aes(semantic_coherence, exclusivity)) +\n  geom_point() +\n  facet_wrap(vars(K)) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\") +\n  theme_minimal()\n\n\nAlthough you’d probably want more, let’s just choose 10 topics for now. What we’re most interested in is getting the betas and gammas so that we can do our usual analysis.\n\n\ntopic_model <- k_result %>% \n  filter(K == 10) %>% \n  pull(topic_model) %>% \n  .[[1]]\n\n# Grab the betas - these are the probability of each term in each topic\ntd_beta <- broom::tidy(topic_model, \n                       matrix = \"beta\")\n\n# Grab the gammas - these are the probability of each word in each topic\ntd_gamma <- tidy(topic_model, \n                 matrix = \"gamma\",\n                 document_names = rownames(rba_minutes))\n\nFrom here you could look at how the gammas and betas evolve or change using a statistical model. Or even sometimes just looking at them is interesting. Julia Silge has a bunch of code that makes very nice graphs and tables. One of the advantages of the STM package is that it makes it easier to include specific types of additional information. For instance, we know that over our time period there have been two governors: GRS and Phil Lowe. We could associate each date with who the governor is and then allow that to affect the prevalence of certain topics.\nYou can grab the files and folder set up from GitHub if you’d like.\n\n\n",
    "preview": "posts/2019-01-03-gathering-and-analysing-text-data/gathering-and-analysing-text-data_files/figure-html5/diagnosticmeasuregraphs-1.png",
    "last_modified": "2020-04-26T17:17:00-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2018-08-13-cleaning-hansard/",
    "title": "Cleaning Hansard: The pay's not great but the work is hard",
    "description": "Cleaning the Australian Hansard is mind-numbing, annoying and time-consuming, but someone has to do it.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2018-08-13",
    "categories": [],
    "contents": "\nThanks to Monica for the title.\nAfter getting an email along the lines of ‘hey aren’t you doing something with Australia’s Hansard?’ I realised that I’ve been a bit remiss about sharing my Australian Hansard progress. This is the first of a series of posts to fix that.\n{{< tweet 926509282874585089 >}}\nI haven’t written about what I’ve been doing with Australian Hansard to this point because: 1) my knowledge of the political science literature is piecemeal and I’m sure someone must have already done all this and I just can’t find it; and 2) my coding knowledge is also piecemeal and there’s no doubt a million ways to better do what I’ve done so far. If anyone has advice on either aspect (or anything really!) I’d be keen to hear it - please email.\nBest I can tell, Australian Hansard is a treasure-trove of data and it’s hard to believe it hasn’t been more analysed. I’m probably missing a whole bunch of literature (insert standard joke about economists here) but so far I can really only find a handful of papers using Australia’s Hansard.1 There’s plenty of work using the parliamentary records of other Commonwealth countries such as Canada2, NZ3 and the UK4. I think that in Australia it’s really only Patrick Leslie who may be using it at the moment (big thank you to Jill Sheppard for pointing this out), but I’ll update this if I find others.\nThe good news is that Australia’s Hansard has been digitised and is available on the parliament’s website, so a figurative pseudo-Manhattan-project isn’t required (cf. what was needed in Canada, see: https://www.lipad.ca/). If you just want short, specific, sections then the situation is fine - for pre-1981 go to Tim Sherratt’s brilliant Historic Hansard website (http://historichansard.net); for 1981 to 2006 just use the parliament’s website; and for 2006 onward just use Open Australia. The bad news is that Hansard isn’t really available as a nice corpus for larger scale analysis. Making this nice corpus has been keeping me busy, and will be the subject of this series of posts.\nHelpfully, various people/organisations have gone to the Hansard website to get the XML files they provide and made them available as an easy download (note that these tend to have been posted as they were provided by the parliament, so they’re full of typos, transcription errors, and a bunch of other mistakes):\n1901-1980 Hansard is available as XML from Tim Sherratt’s Historic Hansard Github - https://github.com/wragge/hansard-xml.\n1998-2014 Hansard is available from Andrew Turpin’s website at University of Melbourne (https://people.eng.unimelb.edu.au/aturpin/qtCorpus/index.html).\n2006-current is available from Open Australia’s website (http://data.openaustralia.org.au/origxml/).\nThese helpful people/organisations were able to get those dates (1901-1980 and 1998-current) because the Hansard provides the XML for those years on their website. The problem is the 1980s and the early/mid 1990s because they don’t have the XML available on the website (and from emails with them - it seems as though they simply don’t have it) - the only choice seems to be either to scrape it manually from the Hansard website or to grab all the PDFs, convert them, and then fix the mistakes. I’ve started on the second option - unsure how wise it is but I don’t know of any alternative.\nFuture posts:\nWho has been elected to parliament?\nWhat divisions are relevant?\nWhat parties are relevant?\nHansard pre-1981\nHansard post-1980\nFor instance: Turpin ‘An Attempt to Measure the Quality of Questions in Question Time of the Australian Federal Parliament’.↩\nFor instance: Beelen, Thijm, Cochrane, Halvemaan, Hirst, Kimmins, Lijbrink, Marx, Naderi, Rheault, Polyanovsky, Whyte ‘Digitization of the Canadian Parliamentary Debates’.↩\nFor instance: Curran, Higham, Ortiz, Vasques Filho ‘Look Who’s Talking: Bipartite Networks as Representations of a Topic Model of New Zealand Parliamentary Speeches’.↩\nFor instance: Abercrombie, Batista-Navarro ‘Aye or No? Speech-level Sentiment Analysis of Hansard UK Parliamentary Debate Transcripts’↩\n",
    "preview": {},
    "last_modified": "2020-04-26T17:16:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-28-the-sql-is-never-as-good-as-the-original/",
    "title": "The SQL Is Never As Good As The Original",
    "description": "SQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL has increased the number of datasets that I can get data from to then analyse with other tools such as R or Python. You can use SQL within RStudio if you want. The following are a few notes to help future-Rohan when he needs to use SQL. A worked example with a sample of the Hansard data will be included in a future post.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2018-07-28",
    "categories": [],
    "contents": "\nThanks to Monica for the title.\nIntroduction\nSQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL has increased the number of datasets that I can get data from to then analyse with other tools such as R or Python. You can use SQL within RStudio if you want. The following are a few notes to help future-Rohan when he needs to use SQL. A worked example with a sample of the Hansard data will be included in a future post.\n\nSQL is fairly straightforward if you’ve used mutate, filter and join in the R tidyverse as the concepts (and sometimes even the verb) are the same. In that case, half the battle is getting used to the terminology, and the other half is getting on top of the order of operations because SQL can be a tad pedantic.\nSQL (“see-quell” or “S.Q.L.” - both camps seem fairly insistent on their way…) is used with relational databases. A relational database is just a collection of at least one table, and a table is just some data organized into rows and columns. If there’s more than one table in the database, then there should be some column that links them. Using it feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that line spaces mean nothing: include them or don’t, but always end a SQL command in a semicolon;\nCreating a table\nCreate an empty table of three columns of type: int, text, int:\n\n\nCREATE TABLE table_name (\n  column1 INTEGER,\n  column2 TEXT,\n  column3 INTEGER\n);\n\nAdd a row of data:\n\n\nINSERT INTO table_name (column1, column2, column3)\n  VALUES (1234, 'Gough Menzies', 32);\n\nAdd a column:\n\n\nALTER TABLE table_name\n  ADD COLUMN column4 TEXT;\n\nViewing the data\nSee one column (similar to R’s select):\n\n\nSELECT column2\n  FROM table_name;\n\nSee two columns:\n\n\nSELECT column1, column2\n  FROM table_name;\n\nSee all columns:\n\n\nSELECT *\n  FROM table_name;\n\nSee unique rows in a column (similar to R’s distinct):\n\n\nSELECT DISTINCT column2\n  FROM table_name;\n\nSee the rows that match a criteria (similar idea to R’s which or filter):\n\n\nSELECT *\n  FROM table_name\n    WHERE column3 > 30;\n\nAll the usual operators are fine with WHERE: =, !=, >, <, >=, <=. Just make sure the condition evaluates to true/false.\nSee the rows that are pretty close to a criteria:\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '_ough Menzies';\n\nThe _ above is a wildcard that matches to any character e.g. ‘Cough Menzies’ would be matched here, as would ‘Gough Menzies’. LIKE is not case-sensitive: ‘Gough Menzies’ and ‘gough menzies’ would both match here.\nUse % as an anchor to matches pieces:\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '%Menzies';\n\nThat matches anything ending with ‘Menzies’, so ‘Cough Menzies’, ‘Gough Menzies’, ‘Sir Menzies’ etc, would all be matched here. Use surrounding percentages to match within, e.g. %Menzies% would also match ‘Sir Menzies Jr’ whereas %Menzies would not.\nThis is wild: NULL values (!) (True/False/NULL) are possible, not just True/False, but they need to be explicitly matched for:\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 IS NOT NULL;\n\nThis too is wild: There’s an underlying ordering build into number, date and text fields that allows you to use BETWEEN on all those, not just numeric! The following looks for text that starts with a letter between A and M (not including M) so would match ‘Gough Menzies’, but not ‘Sir Gough Menzies’!\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M';\n\nIf you look for a numeric (as opposed to text) then BETWEEN is inclusive.\nCombine conditions with AND (both must be true to be returned) or OR (at least one must be true):\n\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M'\n    AND column3 = 32;\n\nYou can order the result:\n\n\nSELECT *\n  FROM table_name\n    ORDER BY column3;\n\nAscending is the default, add DESC for alternative:\n\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC;\n\nRestrict the return to a certain number of values by adding LIMIT at the end:\n\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC\n    LIMIT 1;\n\n(This doesn’t work all the time - only certain SQL databases.)\nModifying data and using logic\nEdit a value:\n\n\nUPDATE table_name\n  SET column3 = 33\n    WHERE column1 = 1234;\n\nImplement if/else logic:\n\n\nSELECT *,\n  CASE\n    WHEN column2 = 'Gough Whitlam' THEN 'Labor'\n    WHEN column2 = 'Robert Menzies' THEN 'Liberal'\n    ELSE 'Who knows'\n  END AS 'Party'\n  FROM table_name;\n\nThis returns a column called ‘Party’ that looks at the name of the person to return a party.\nDelete some rows:\n\n\nDELETE FROM table_name\n  WHERE column3 IS NULL;\n\nAdd an alias to a column name (this just shows in the output):\n\n\nSELECT column2 AS 'Names'\n  FROM table_name;\n\nSummarising data\nWe can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of summarise in R. COUNT counts the number of rows that are not empty for some column by passing the column name, or for all using *:\n\n\nSELECT COUNT(*)\n  FROM table_name;\n\nSimilarly, pass a column to SUM, MAX, MIN, and AVG:\n\n\nSELECT SUM(column1)\n  FROM table_name;\n\nROUND takes a column and an integer to specify how many decimal places:\n\n\nSELECT ROUND(column1, 0)\n  FROM table_name;\n\nSELECT and GROUP BY is similar to group_by in R:\n\n\nSELECT column3, COUNT(*)\n  FROM table_name\n    GROUP BY column3;\n\nYou can GROUP BY column number instead of name e.g. 1 instead of column3 in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.\nHAVING for aggregates, is similar to filter in R or the WHERE for rows from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.\nCombining data\nCombine two tables using JOIN or LEFT JOIN:\n\n\nSELECT *\n  FROM table1_name\n  JOIN table2_name\n    ON table1_name.colum1 = table2_name.column1;\n\nBe careful to specify the matching columns using dot notation. Primary key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3) only one column per table. A primary key can be primary in one table and foreign in another. Unique columns have a different value for every row and there can be many in one table.\nUNION is the equivalent of cbind if the tables are already fairly similar.\n\n\n",
    "preview": {},
    "last_modified": "2020-04-26T17:16:22-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-29-getting_started_with_topic_modelling-theory/",
    "title": "Topic Modelling - Theory",
    "description": "Each statement in Hansard needs to be classified by its topic. Sometimes Hansard includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement in Hansard is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, Jordan, @Blei2003latent, as implemented by the R package 'topicmodels' by Grun @Grun2011.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2018-06-29",
    "categories": [],
    "contents": "\nThese are notes that I put together in the process of trying to understand how topic modelling works with a view to applying it to Australia’s Hansard. There are undoubtedly mistakes and aspects that are unclear. Please get in touch if you have any suggestions.\nOverview\nEach statement in Hansard needs to be classified by its topic. Sometimes Hansard includes titles that make the topic clear. But not every statement has a title and the titles do not always define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement in Hansard is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as implemented by the R package ‘topicmodels’ by Grün and Hornik (2011).\nThe key assumption behind the LDA method is that each statement, ‘a document’, in Hansard is made by a speaker who decides the topics they would like to talk about in that document, and then chooses words, ‘terms’, that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified ex ante; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in Hansard group themselves to define topics.\nDocument generation process\nAs applied to Hansard, the LDA method considers each statement to be a result of a process where a politician first chooses the topics they want to speak about. After choosing the topics, the speaker then chooses appropriate words to use for each of those topics.\nMore generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure 1).\n\n\n\nFigure 1: Probability distributions over topics\n\n\n\nSimilarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure 2).\n\n\n\nFigure 2: Probability distributions over terms\n\n\n\nFollowing Blei and Lafferty (2009), Blei (2012) and Griffiths and Steyvers (2004), the process by which a document is generated is more formally considered to be:\nThere are \\(1, 2, \\dots, k, \\dots, K\\) topics and the vocabulary consists of \\(1, 2, \\dots, V\\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \\(k\\)th topic is \\(\\beta_k\\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \\(0<\\eta<1\\) is used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).1 Strictly, \\(\\eta\\) is actually a vector of hyperparameters, one for each \\(K\\), but in practice they all tend to be the same value.\nDecide the topics that each document will cover by randomly drawing distributions over the \\(K\\) topics for each of the \\(1, 2, \\dots, d, \\dots, D\\) documents. The topic distributions for the \\(d\\)th document are \\(\\theta_d\\), and \\(\\theta_{d,k}\\) is the topic distribution for topic \\(k\\) in document \\(d\\). Again, the Dirichlet distribution with the hyperparameter \\(0<\\alpha<1\\) is used here because usually a document would only cover a handful of topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). Again, strictly \\(\\alpha\\) is vector of length \\(K\\) of hyperparameters, but in practice each is usually the same value.\nIf there are \\(1, 2, \\dots, n, \\dots, N\\) terms in the \\(d\\)th document, then to choose the \\(n\\)th term, \\(w_{d, n}\\):\nRandomly choose a topic for that term \\(n\\), in that document \\(d\\), \\(z_{d,n}\\), from the multinomial distribution over topics in that document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).\nRandomly choose a term from the relevant multinomial distribution over the terms for that topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).\n\nGiven this set-up, the joint distribution for the variables is (Blei (2012), p.6): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{i=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\]\nBased on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \\(\\beta_{1:K}\\) and \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). This is intractable directly, but can be approximated (Griffiths and Steyvers (2004) and Blei (2012)).\nAnalysis process\nAfter the documents are created, they are all that we have to analyse. The term usage in each document, \\(w_{1:D, 1:N}\\), is observed, but the topics are hidden, or ‘latent’. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures 1 or 2. In a sense we are trying to reverse the document generation process – we have the terms and we would like to discover the topics.\nIf the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (Steyvers and Griffiths (2006)). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (Blei (2012), p.7): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\]\nThe initial practical step when implementing LDA given a corpus of documents is to remove ‘stop words’. These are words that are common, but that don’t typically help to define topics. There is a general list of stop words such as: “a”; “a’s”; “able”; “about”; “above”… An additional list of words that are commonly found in Hansard, but likely don’t help define topics is added to the general list. These additions include words such as: “act”; “amendment”; “amount”; “australia”; “australian”; “bill”… A full list will be available in a follow up post going through an example with R code. We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.\nAfter the dataset is ready, the R package ‘topicmodels’ by Grün and Hornik (2011) can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following Steyvers and Griffiths (2006) and Darling (2011), the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \\(\\alpha = \\frac{50}{K}\\) and \\(\\eta = 0.1\\) (Steyvers and Griffiths (2006) recommends \\(\\eta = 0.01\\)), where \\(\\alpha\\) refers to the distribution over topics and \\(\\eta\\) refers to the distribution over terms (Grün and Hornik (2011), p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (Grün and Hornik (2011), p.6): \\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \\propto \\frac{\\lambda'_{n\\rightarrow k}+\\eta}{\\lambda'_{.\\rightarrow k}+V\\eta} \\frac{\\lambda'^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda'^{(d)}_{-i}+K\\alpha} \\] where \\(z'_{d, n}\\) refers to all other topic assignments; \\(\\lambda'_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\); \\(\\lambda'_{.\\rightarrow k}\\) is a count of how many other times that any term has been assigned to topic \\(k\\); \\(\\lambda'^{(d)}_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\) in that particular document; and \\(\\lambda'^{(d)}_{-i}\\) is a count of how many other times that term has been assigned in that document. Once \\(z_{d,n}\\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.\nThis conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (Steyvers and Griffiths (2006)). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.\nWarnings and extensions\nThe choice of the number of topics, k, affects the results, and must be specified a priori. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for k and then picking an appropriate value that performs well.\nOne weakness of the LDA method is that it considers a ‘bag of words’ where the order of those words does not matter (Blei (2012)). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking.\nReferences\n\n\nBlei, David M. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84.\n\n\nBlei, David M, and John D Lafferty. 2009. “Topic Models.” In Text Mining, 101–24. Chapman; Hall/CRC.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n\n\nDarling, William M. 2011. “A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 642–47.\n\n\nGriffiths, Thomas, and Mark Steyvers. 2004. “Finding Scientific Topics.” PNAS 101: 5228–35.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic Models.” In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch.\n\n\nThe Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \\(\\eta=1\\), it is equivalent to a uniform distribution. If \\(\\eta<1\\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \\(\\eta\\) decreases. A hyperparameter is a parameter of a prior distribution.↩\n",
    "preview": "posts/2018-06-29-getting_started_with_topic_modelling-theory/topic_modelling-theory_files/figure-html5/topicsoverdocuments-1.png",
    "last_modified": "2020-04-26T17:15:40-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2017-09-14-getting-started-with-latex/",
    "title": "Getting started with LaTeX",
    "description": "LaTeX makes it easier to produce papers that look great, but it can be overwhelming at the start. These notes help you get up-and-running with LaTeX.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "contents": "\nThank you to Janet Bradly and Maria Racionero for their support of this workshop. These notes are based on ones prepared by Zac Cranko and I for a presentation in 2015 and those that I put together for a 2016 presentation. Zac’s work is used with permission.\nIntroduction\nLaTeX makes it easier to produce papers that look great, but it can be overwhelming at the start. These notes help you get up-and-running with LaTeX.\nBy the end you will have created an example paper and slides that include a title, author, affiliation, abstract, sections, tables, figures and references that looks like this:\n\n\n\nIt is best to type everything out yourself, but you can download the tex file from here: https://github.com/RohanAlexander/blogdown_website/blob/master/static/img/paper.tex\nWriting papers\nHello world!\nYou can treat LaTeX as a markup language. You mark your text with commands depending on how you want it to look, and then compile it to produce an output such as a PDF. Get started by downloading LaTeX, for free, from: https://www.latex-project.org/get/.\nUse the default settings. You will likely end up installing a bunch of programs. (There is now a smaller version of MacTeX available, but for now it is probably best to stick with the original version for now.)\nLet’s make a document. Open ‘TeXShop’ if you’re using a Mac, and ‘TeXStudio’ if you’re using a PC.\nIn TeXShop/TeXStudio type:\n\n\n\nThen click ‘Typeset’. A bunch of miscellaneous files are produced in the process of compiling. Don’t worry about these – all they do is make your directory ugly. The two important files are the tex file, which contains your markup, and the pdf file, which contains your output.\nSquare brackets are optional in LaTeX, but it is worth specifying the paper size and the font for the documentclass command. If you wanted to specify A4 paper and 12pt text by default then our ‘Hello world!’ becomes:\n\n\n\nThe type of document that you want goes in the braces. We used ‘article’, which is good for producing papers. There are many classes of document, including ‘book’, ‘letter’, and ‘beamer’ which is for presentations, but ‘article’ is probably the one that you will most commonly use.\nPackages\nPackages expand the basics of LaTeX. There are a few that you should load every time because they’re often used when writing economics papers:\namsmath;\namsthm;\namssymb; and\ngraphicx.\nLoad these packages by adding the following after documentclass[a4paper, 12pt]{article}, but before begin document:\n\n\n\nThen click ‘Typeset’. Nothing should change in the output, we are just typesetting it to make sure that we have not introduced an error. The first three packages help when writing maths. The fourth helps with including graphs.\nEverything before begin document is called the preamble, and everything after it is called the content.\nTitle, author, and affiliation\nYou can add a title, author and affiliation to your paper by adding the following to the content:\n\n\n\nClick ‘Typeset’ and a title should be added to your paper.\nAdd an affiliation by changing the author markup to:\n\n\n\nThe textit command makes the text italic. You don’t need to make the affiliation italic, but many people seem to. You could make your text bold using textbf.\nFinally, add an acknowledgements section by changing the author markup to:\n\n\n\nIf you don’t include date{A specific date} then LaTeX will add today’s date. If you don’t want the date then add date{} instead.\nAbstract\nIf you want an abstract, then LaTeX will do the formatting for you if you use the abstract tags. Add this markup just below maketitle:\n\n\n\nTo summarise what your tex file should look like to this point:\n\n\n\nSections\nThe main section commands are: section, subsection, and subsubsection. These produce headings of decreasing importance and are numbered automatically. That can be turned off using an asterisk, for instance: section*.\nFor now let’s add numbered introduction, model, and data sections to our document by adding the following markup after the abstract:\n\n\n\nMaths\nNow let’s add some maths into the model section of our paper. Type the following under the model section:\n\n\n\nWhen you compile your tex file you should get this equation:\n\\[ \\hat{\\alpha} = \\frac{\\sum^J_{j=1}\\beta_j}{\\int^{\\infty}_{0}f(k)} \\]\nWe will now go through the pieces of this.\nGreek, limits, infinity, and integrals\nWe invoked ‘maths mode’ by using double dollar signs. That put the maths that you write on its own line. If you wanted to have your maths content without breaking the line, such as \\(x = 5\\), then just use single dollar signs.\nWithin maths mode you can get many greek letters by backslash followed by their name. The examples above were alpha and beta.\nLimits, infinity and integrals are built into LaTeX math mode, and can be access with the command lim, infty, and int. You can use accents and underbars if you need to denote what the limit refers to or upper and lower bounds. For instance, add the following markup underneath the formula:\n\n\n\nWhen you compile this it should looks like:\nAnd when \\(\\lim_{k\\rightarrow0}f(k)/k = \\infty\\) it is the case that \\(\\int^{\\infty}_{0}f(k) = 100\\).\nProbability, expectations, real numbers, integrals\nThe \\(\\mathbb{P}\\), \\(\\mathbb{E}\\), and \\(\\mathbb{R}\\) that you may be used to seeing to denote probability, expectation, and the real numbers are made by a call to mathbb within maths mode. For instance add the following to your paper:\n\n\n\nWhen you compile it should look like this:\nOften we care about probability, \\(\\mathbb{P}\\), because of expectations, \\(\\mathbb{E}\\), over real numbers \\(\\mathbb{R}\\).\nYou can also make a call to mathcal, for instance in naming sets \\(\\mathcal{A}\\), \\(\\mathcal{B}\\), \\(\\mathcal{C}\\) or for a nice Lagrangian \\(\\mathcal{L}\\).\nFractions\nFractions are built into math mode using frac{}{} and you can nest them if you need to. For instance add the following to your paper:\n\n\n\nThis should compile to:\nIt can be surprising when you first learn that \\(\\frac{\\frac{x}{y}}{y} = \\frac{x}{y^2}\\).\nBe careful when using brackets and fractions because sometimes the sizes need to be aligned. You can do it manually, but alternative specify left and right, for instance, compare with and without:\n\n\n\nwhich compiles to:\n\\[\\left(\\frac{\\frac{x}{y}}{y}\\right) = (\\frac{\\frac{x}{y}}{y})\\].\nTheorems, definitions and proofs\nTheorems and proofs draw on the amsthm package that was loaded earlier. You need to declare the name that you’ll use to refer to it in the preamble. After that you can call a theorem, proposition, description, whatever it was you defined, throughout the document.\nFirst, add this to the preamble:\n\n\n\nThen add this to the content:\n\n\n\nIn this case, I defined a theorem and LaTeX will print Theorem when compiled You could add another for propositions, etc.\nProofs are similar, but don’t need to be defined in the preamble:\n\n\n\nYour tex file should now look like this:\n\n\n\nText\nParagraphs\nLaTeX was designed for maths, but it does text well too. To start a new paragraph, just leave a blank line in your editor, LaTeX will take care of spacing. For instance add the following to your introduction:\n\n\n\nThere are a few aspects to be aware of:\nTo get ‘a quote’, you need to use the key next to the ‘1’ on your keyboard for the opening mark and then the normal quotation mark for the end mark.\nYou can makes words italic using textit{some italic words}, or make them bold using textbf{some bold words}.\nBecause the dollar sign is used to invoke maths mode, if you want to refer to prices, say $4, then you need to use a slash before the dollar sign. This is the same for the per cent symbol, %.\nSome people prefer different formatting on the paragraphs. Although it can cause some issues, you can change this by adding to the preamble:\n\n\n\nLists\nThere are two main types of lists: itemize and enumerate.\nAdd the following to your data section:\n\n\n\nTables\nTables are often annoying in LaTeX. Fortunately, many programs will automatically format their table outputs with LaTeX markup for your to copy-and-paste into your tex file, and there are websites that can help.\nSimple tables are not a problem. For instance, add the following to your data section:\n\n\n\nBut it gets complicated. If you commonly use tables then it is easier to get your statistics program to output tables that have been formatted for LaTeX, or use an online table generator, such as http://www.tablesgenerator.com/.\nIn R, there is a package ‘Huxtable’.\nGraphs and pictures\nTo include graphs or pictures in your document, add the file to the same folder that your tex file is in. From there you can add it. Many adjustments are possible in terms of size and layout. Fortunately, all the labelling is done for us. For instance, download the following image into the folder where your tex file is saved: https://github.com/RohanAlexander/blogdown_website/blob/master/static/img/me.png\nThen add this markup into your data section:\n\n\n\nYour tex file should look like this:\n\n\n\nReferences\nLaTeX uses Bibtex for references. To use this open a new file in TeXShop/TeXStudio and add the following:\n\n\n\nSave it as first_bibliography.bib in the same folder as your tex file. Then add the following at the end of the paragraphs in your Introduction:\n\n\n\nAnd add the following at the end of your document, on the line above end{document}.\n\n\n\nThen typeset as BibTeX, (you’ll only need to do this once each time you update your bib file), and then typeset as normal with LaTeX.\nYour tex file should look like this:\n\n\n\nSlides\nMaking slides is similar to writing a paper in that the markup can be the same. But you need to specify when the content of a slide should start and stop and also what the title should be.\nTo get started, open a new file in TeXShop/TeXStudio and add the following:\n\n\n\nYou’ll notice that the only difference is that the document class has been changed to beamer.\nCopy the title, author and date, etc markup from your paper, and then paste it between begin{frame} and end{frame}. So your file should look like this:\n\n\n\nWhen you save and compile this you should get slides.\nYou can add a content slide by adding the following after that first slide:\n\n\n\nI’ve chosen to include a list, but you could include paragraphs, or images or tables using the same markup that you have in your paper.\nMisc\nLearning more and getting help\nCheck that there is an end for every begin, and similarly that all braces that are opened are also closed.\nCompile frequently so that you have a better idea of where the error is.\nStack Overflow is helpful if you want specific changes or features.\nThe wikibooks guide - https://en.wikibooks.org/wiki/LaTeX - is an excellent resource to improve your knowledge.\nShareLaTeX\nThese days I more commonly use LaTeX in the cloud instead of on my local computer, via: https://www.sharelatex.com/. The advantage is that it brings google-docs-style collaboration tools.\n\n\n",
    "preview": "posts/2017-09-14-getting-started-with-latex/images/LaTeX_first_example.png",
    "last_modified": "2020-04-26T17:15:24-04:00",
    "input_file": {},
    "preview_width": 1014,
    "preview_height": 1448
  },
  {
    "path": "posts/2017-08-11-reproducing-a-grattan-map/",
    "title": "Reproducing a Grattan Institute map",
    "description": "Blogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-08-11",
    "categories": [],
    "contents": "\nThank you to Monica for her helpful edits.\nGeoffrey Liu found an error in how I deal with the postcodes data that I haven’t fixed yet.\nIntroduction\nThe Grattan Institute is an Australian think tank that produces reports about public policy. Last week they released ‘Regional patterns of Australia’s economy and population’. That report looks into the differences between geographic areas across various economic and demographic variables. It includes interactive maps made using Carto. The Grattan Institute also released the datasets that underpin the report’s maps and graphs.\nI was interested to see if I could reproduce one of their interactive maps - Annual income growth per person (FY04 - FY15) - in an hour using R and the Leaflet package. A day later I found the underlying dataset did not correspond with the map that the Grattan Institute published. But following the methodology in the report I was able to create a dataset that seems pretty close to the values illustrated on their map.\nThe final map that I produced is below (it’ll take about 5 seconds to load) and this note records what I did to produce it in R using the Leaflet package.\n\n\n\n\nWorkspace\nThe first step was to set-up the workspace. Mostly this just meant loading packages. The tidyverse and magrittr packages help with general data manipulation tasks; zoo helps with the rolling average needed later; and leaflet, rgdal, and rmapshaper are specific to mapping.\n\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(zoo)\nlibrary(leaflet)\nlibrary(rgdal)\nlibrary(rmapshaper)\n\nData\nI decided to reproduce Figure 2.4 of the report. This shows the average annual growth in real taxable income per tax filer between the financial years 2003/04 and 2014/15 for the 2011 SA3 areas. To do this I needed the incomes data that the Grattan Institute mapped and the geographic data that defines the areas.\nIncomes\nThe Grattan Institute released the datasets that they said had been mapped. It was straightforward to download this file and export the relevant sheet as a csv file. The Excel file is available at https://grattan.edu.au/report/regional-patterns-of-australias-economy-and-population/ (in the left panel).\n\n\n# Import the income data that has been taken from the Grattan data download, sheet 'Figure 2.4' (which was saved as a csv).\nincomes <- read_csv(\"data/890-Regional-patterns-chart-data.csv\")\n# Remove some debris columns\nincomes <- incomes %>%\n  select(\"SA3\", \"Growth_mean_income_FY04_FY15\")\n\nGeographies\nThe other dataset that I needed was the geographies that the Grattan Institute had used. I initially wasted a lot of time using the 2016 SA3 release. Eventually I realised they were using the SA3 release from 2011. This is available from the ABS website at: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202011?OpenDocument and I used the zipped file: ‘Statistical Area Level 3 (SA3) ASGS Ed 2011 Digital Boundaries in ESRI Shapefile Format’.\n\n\n# 2011 SA3 boundaries\nold_boundaries <- readOGR(dsn = \"data/1270055001_sa3_2011_aust_shape\", layer = \"SA3_2011_AUST\")\n# Add the income data into the boundaries dataset\nold_boundaries <- merge(old_boundaries, incomes, by.x = \"SA3_NAME11\", by.y = \"SA3\")\n\nComparing the incomes data with the 2011 geographies data indicates the incomes data is missing two SA3 areas: ‘Illawarra Catchment Reserve’ and ‘Blue Mountains - South’. I also found these areas had been left as NA in the Grattan Institute’s interactive map. This made me confident that the Grattan Institute was using the 2011 boundaries (I may have missed it but I don’t think this was documented).\nMap\nMaking the map was not complicated once the pieces were in place. I called Leaflet and specified a black and white base map. After that I adjusted the default view and then added the patchwork quilt that shows the incomes dataset by SA3 area.\nI didn’t spend too much time on the colours because the inferno palette got fairly close. I just copied their published bins to reproduce the breaks that the Grattan Institute used. Box 1 in Appendix A of the Grattan Institute’s report specifies how they came up with these bins. Give my purposes I didn’t worry too much about this.\n\n\n# Set the color scheme \npal <- colorBin(\n  palette = \"inferno\",\n  domain = old_boundaries$Growth_mean_income_FY04_FY15,\n  bins = c(0, 0.005, 0.012, 0.015, 0.017, 0.019, 0.022, 0.029, 0.07),\n  reverse = TRUE\n  )\n\n# Make the map\nAustralia_incomes_map <- \n  leaflet() %>%\n  # Base map\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %>% \n  setView(lng = 133.7751, lat = -25.2744, zoom = 4) %>% # Specify where the map is initially focused\n  addPolygons(data = old_boundaries, \n              color = pal(old_boundaries$Growth_mean_income_FY04_FY15), \n              weight = 1, \n              stroke = FALSE,\n              smoothFactor = 0.5,\n              fillOpacity = 0.8, \n              label = paste(\"Area name (SA3):\", as.character(old_boundaries$SA3_NAME11), \n                            \"Mean annual growth:\", as.character(old_boundaries$Growth_mean_income_FY04_FY15)),\n              highlightOptions = highlightOptions(color = \"#666\", weight = 2, bringToFront = FALSE)) %>% \n  addLegend(\"bottomright\", pal = pal, values = old_boundaries$Growth_mean_income_FY04_FY15,\n            title = \"Mean annual income growth (FY04 - FY15)\",\n            opacity = 0.4\n  )\n\n# Call the map\nAustralia_incomes_map\n\nThere were many small changes that could be made to better reproduce the Grattan Institute’s map, but at this stage I realised there was something going on with the data. I decided to spend more time with that than tweaking the remaining aspects.\nComparison\nIssues\nComparing screenshots of the maps shows some of the issues:\n\n\n\nFigure 1: Grattan’s map\n\n\n\n\n\n\nFigure 2: My map\n\n\n\nFor instance there are areas where my map has a lot more variation such as:\nthe northern SA3 areas of Queensland; and\nthe SA3 areas in the south west of Western Australia.\nAnd there are also some areas where the colours are fairly different (notwithstanding the fact that I didn’t match theirs exactly), such as:\nwest of Melbourne where my values are a lot higher.\nTriage\nI checked that I was using the dataset that I had meant to use. While I was checking this dataset (which is the one that the Grattan Institute makes available) I noticed that the numbers in the Grattan Institute’s dataset were not always the ones that were being mapped. This was easy to see because they included a static version of the map next to the dataset, so I was confident it was meant to be the same.\nI couldn’t work out how to download the actual dataset underlying the interactive Carto map. But I was able to check some on an area-by-area basis because the value was displayed on mouse-hover. I found that the figure that was displayed did line up with the colour of the area, but not the dataset that they offered as underpinning the map.\nThe following table summarises some of the SA3 areas in north Queensland.\nSA3 Area\nGrattan Carto value\nGrattan data value\nCairns - South\n1.73\n2.0\nCharters Towers - Ayr - Ingham\n2.35\n1.6\nFar North\n2.66\n3.0\nOutback - North\n2.33\n1.7\nPort Douglas - Daintree\n1.73\n1.4\nTablelands (East) - Kuranda\n2.23\n1.6\nThe key issue was whether it was the Carto map or the dataset that was wrong.\nDown the rabbit hole\nBy this stage it was after dinner, and I’d had a glass or three of wine. But the only way to work out whether it was the dataset or the map that was wrong was to recreate the dataset myself. To do this I needed:\nincomes data for financial years 2003-04 and 2014-15;\nan inflation rate over this time to make the 2003-04 data real; and\na correspondence from postcodes to the 2011 SA3 areas.\nWithout their code it would be hard to be certain, but the Grattan Institute provided enough information in the report that I was confident I could get reasonably close to what they’d done.\nIncomes\nThe incomes data is from Table 8 of the ATO’s 2015 tax stats which is available here: https://data.gov.au/dataset/taxation-statistics-2014-15/resource/02e58971-ddee-4f77-af15-5c45de569ed6\n\n\n# Import the tax data\ntax_data <- read_csv(\"data/taxstats2015individual08medianaveragetaxableincomestateterritorypostcode.csv\", skip = 2, col_names = FALSE)\n# Remove the debris rows\ntax_data <- tax_data[2:2254,]\n# Grattan says we only need postcode, the 2003/04 data and the 2014/15 data, so drop the rest of the variables\ntax_data <- tax_data %>%\n  select(X2:X5, X11:X13)\n# Fix the column names\ntax_data <- rename(tax_data, \"postcode\" = X2, \"population_0304\" = X3, \"median_inc_0304\" = X4, \"ave_inc_0304\" = X5, \"population_1415\" = X11, \"median_inc_1415\" = X12, \"ave_inc_1415\" = X13)\n# Finally, change the classes to numeric (need to remove the comma first)\ntax_data$population_0304 <- sub(\",\", \"\", tax_data$population_0304)\ntax_data$median_inc_0304 <- sub(\",\", \"\", tax_data$median_inc_0304)\ntax_data$ave_inc_0304 <- sub(\",\", \"\", tax_data$ave_inc_0304)\ntax_data$population_1415 <- sub(\",\", \"\", tax_data$population_1415)\ntax_data$median_inc_1415 <- sub(\",\", \"\", tax_data$median_inc_1415)\ntax_data$ave_inc_1415 <- sub(\",\", \"\", tax_data$ave_inc_1415)\ntax_data$postcode <- as.numeric(tax_data$postcode)\ntax_data$population_0304 <- as.numeric(tax_data$population_0304)\ntax_data$median_inc_0304 <- as.numeric(tax_data$median_inc_0304)\ntax_data$ave_inc_0304 <- as.numeric(tax_data$ave_inc_0304)\ntax_data$population_1415 <- as.numeric(tax_data$population_1415)\ntax_data$median_inc_1415 <- as.numeric(tax_data$median_inc_1415)\ntax_data$ave_inc_1415 <- as.numeric(tax_data$ave_inc_1415)\n\nInflation\nI needed to change the financial year 2003-04 incomes into 2014-15 dollars. In the Grattan Institute’s report (p. 34) they say:\n\nNominal income for the 2003-04 financial year was adjusted to real 2014-15 dollars, using a yearly average of ABS quarterly data on the Consumer Price Index, starting from the 2003 September quarter.\n\nI downloaded the inflation data that they specified from: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/6401.0Mar%202017?OpenDocument (it is Series A2325846C which is in Tables 1 and 2 of the release). I wasn’t exactly sure how the Grattan Institute constructed its measure, but I decided to just go with the RBA formula (http://www.rba.gov.au/calculator/) which is: (Average of the four quarters in the final year / average of the four quarters in the first year - 1) *100, although I didn’t need to worry about removing one or the multiplying.\n\n\n# Import the inflation data\ninflation_data <- read_csv(\"data/640101.csv\", skip = 10, col_names = FALSE)\n# Grab the series they use: A2325846C\ninflation_data <- inflation_data %>%\n  select(X1, X10)\n# Fix the column names\ninflation_data <- rename(inflation_data, \n                   \"quarter\" = X1,\n                   \"index_value\" = X10\n)\n# To use the RBA formula we need the average index number over the four quarters\ninflation_data <- inflation_data %>%\n  mutate(\n    average_index_over_year = rollmean(index_value, 4, align = 'right', fill = NA)\n  )\n# Now we just use (final_year_ave / first_year_ave) (don't need to bother with removing the 1 or multiplying by 100 because we are just going to multiply the first year data up to get in terms of final year)\ninflation_rate <- \n  ((inflation_data$average_index_over_year[inflation_data$quarter == \"Jun-2015\"] / inflation_data$average_index_over_year[inflation_data$quarter == \"Jun-2004\"]))\n\nThe inflation adjustment turned out to be about 1.34. I also worked out the annual inflation rates and then chained them, and also tried just using the start and end quarter index numbers (not averaged). They came to similar values, so even though I wasn’t exactly sure what the Grattan Institute had done, I was confident that small differences wouldn’t matter too much.\nGrowth\nI then needed to create the annual growth rate that the Grattan Institute used (p. 34):\n\nGrowth in taxable income is calculated as a compound annual growth rate from the 2003-04 financial year to the 2014-15 financial year.\n\n\n\n## Data manipulation - create the new interest variable that is of interest - 11 years?\ntax_data <- tax_data %>%\n  mutate(\n    real_ave_inc_0304 = ave_inc_0304 * inflation_rate,\n    real_median_inc_0304 = median_inc_0304 * inflation_rate,\n    annual_ave_growth = (ave_inc_1415/real_ave_inc_0304)^(1/11),\n    annual_median_growth = (median_inc_1415/real_median_inc_0304)^(1/11)\n  )\n\nCorrespondence\nThe incomes data that the ATO makes available is on a postcode basis. I needed to convert this into 2011 SA3 levels and the ABS makes a correspondence for this purpose. This is available at: http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.006July%202011?OpenDocument where the correct table is the zipped file ‘Postcode 2011 to Statistical Area Level 3 2011’.\n\n\n# Import the correspondence\ncorrespondence <- read_csv(\"data/1270055006_CG_POSTCODE_2011_SA3_2011.csv\", skip = 7, col_names = FALSE)\ncorrespondence <- correspondence %>%\n  select(X2:X6)\n# Fix the column names\ncorrespondence <- rename(correspondence, \n                   \"postcode\" = X2,\n                   \"SA3_CODE_2011\" = X3,\n                   \"SA3_NAME_2011\" = X4,\n                   \"ratio\" = X5,\n                   \"percentage\" = X6\n)\n\ncorrespondence <- merge(correspondence, tax_data, by.x = \"postcode\", by.y = \"postcode\")\nmatch\n\ncorrespondence <- correspondence %>%\n  mutate(\n    contribution_ave = ratio * annual_ave_growth,\n    contribution_median = ratio * annual_median_growth\n  )\n\nsa3_data <- correspondence %>%\n  group_by(SA3_NAME_2011) %>%\n  summarise(\n    ave_growth = weighted.mean(annual_ave_growth, ratio, na.rm = T),\n    median_growth = weighted.mean(annual_median_growth, ratio, na.rm = T)\n  )\n\nPutting it all together\nThis result of all this is that I created a dataset that seems pretty much the same as the one that the Grattan Institute mapped, but not the one that they released.\n\n\n#load(\"sa3_data.Rda\")\n#head(sa3_data, n = 20)\n\nMy dataset can be downloaded as a csv here: https://github.com/RohanAlexander/blogdown_website/blob/master/content/post/sa3_data.csv\nThe values underlying the Grattan Carto map and the values that I generated are the same for the five 2011 SA3 areas in the earlier table:\nSA3 Area\nGrattan Carto value\nGrattan data value\nMy value\nCairns - South\n1.73\n2.0\n1.73\nCharters Towers - Ayr - Ingham\n2.35\n1.6\n2.35\nFar North\n2.66\n3.0\n2.66\nOutback - North\n2.33\n1.7\n2.33\nPort Douglas - Daintree\n1.73\n1.4\n1.73\nTablelands (East) - Kuranda\n2.23\n1.6\n2.23\nAfter having compared my values with theirs I think that in their Excel dataset the Grattan Institute ordered the columns for the SA3 areas and the incomes data separately instead of together.\nReconciliation\nI’ve reached out to the Grattan Institute and will update this post based on what they say.\nConclusion\nThe Grattan Institute is probably Australia’s most important think tank in terms of not being overly associated with one side of politics but still making a contribution to thinking on policy. It is good that they are being more open about their datasets, but it would be much better if they made their code available. Using tools like Leaflet and ggmap instead of Carto would help with this.\nIt was fun to spend the day in the data-analyst’s version of a treasure chase. But hopefully the next time I decide to reproduce a Grattan Institute map common sense prevails before I go too far down the rabbit hole.\nI regret nothing :)Disclosures: In the interest of transparency I’ll point out that I applied unsuccessfully for a job at the Grattan Institute about five years ago. One friend works as a research assistant for them on a casual basis; and there are a few friends-of-friends who work there full time, but I haven’t talked about this post with any of them.\n\n\n",
    "preview": "posts/2017-08-11-reproducing-a-grattan-map/images/grattan.png",
    "last_modified": "2020-04-26T17:15:17-04:00",
    "input_file": {},
    "preview_width": 1888,
    "preview_height": 1336
  },
  {
    "path": "posts/2017-07-21-getting-started-with-blogdown/",
    "title": "Getting started with Blogdown",
    "description": "Blogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-07-21",
    "categories": [],
    "contents": "\nThank you to Minhee Chae and Peter Gibbard for helpful comments.\nIntroduction\nBlogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites.\nBlogdown lets you freely and quickly get a website up-and-running. It is easy to add content from time-to-time. It integrates with R Markdown which lets you easily share your work. And the separation of content and styling allows you to relatively quickly change your website’s design.\nThat said, using Blogdown is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if you want to customise many aspects of your website, or need everything to be ‘just so’ then Blogdown may not be for you.\nBlogdown is still under active development and various aspects may break in future releases. That said, the investment of time required to set up a Blogdown website is unlikely to be wasted. Even if Blogdown were shuttered tomorrow most of the content could be repurposed for a regular Hugo website.\nA Blogdown user-guide is being written by Yihui Xie, Amber Thomas, and Alison Presmanes Hill. The current draft can be viewed here: https://bookdown.org/yihui/blogdown/. Alison Presmanes Hill also has a very helpful post on getting started: https://apreshill.rbind.io/post/up-and-running-with-blogdown/.\nThis post is a simplified version of those two resources. It sticks to the basics and doesn’t require much decision-making. The purpose is to allow someone without much experience to use Blogdown to get a website up-and-running. Head to those two resources once you’ve got a website working and want to dive a bit deeper.\nSoftware installation\nSoftware\nTo use Blogdown you need R and R Studio.\nTo download R go to https://cran.csiro.au/ and download the version of R for your operating system. Follow the instructions to install R.\nTo download R Studio go to https://www.rstudio.com/products/rstudio/download/, scroll down to ‘Installers for Supported Platform’ and download the version of R Studio for your operating system (likely one of the first two links: Windows or Mac). Follow the instructions to install R Studio.\nPackages\nYou need to install the following packages: devtools, blogdown. To do this open R Studio and type the following into the console, hitting enter at the end of each line to run the command:\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"rstudio/blogdown\")\n\nFolder management\nThis section follows Alison Presmanes Hill’s post closely. Go there for more information.\nGitHub\nIt will be easier to put your website on the internet if you have a GitHub account. To create a GitHub account, go to https://github.com/ and sign-up for a free account. This restricts you to making everything public, but as we are using GitHub for a public website that’s fine. Once you have an account, create a new repository by clicking on the plus and call it ‘my_website’.\n\n\n\nDon’t worry about including a readme or gitignore. Once you get to the ‘Quick setup’ page, copy the website address.\n\n\n\nTerminal\nOpen Terminal (either cmd + space then search for ‘Terminal’ or find it in your Applications). Use a combination of typing ls followed by ‘return’ and typing cd followed by ‘return’, to navigate to your ‘Documents’ folder. This is where your website will live for now.\nType git clone and paste the address you copied earlier and follow by ‘return’. This links that folder to your GitHub account.\nWebsite building\nCreation\nOpen R Studio and install Hugo via the blogdown package with the following code:\n\n\nblogdown::install_hugo()\n\nIn R Studio create a new project in the folder that you just created ‘my_website’. To do this click on: File -> New Project -> Existing Directory. Then navigate to the folder ‘my_website’. This will open a new R Studio session. Creating a project just adds a .proj file in the folder that makes it easier to come back to your website later.\nUsing that new R Studio session create your website with the following code:\n\n\nblogdown::new_site(theme = \"gcushen/hugo-academic\", theme_example = TRUE)\n\nThis will:\ndownload files into your ‘my_website’ folder;\nopen a R Markdown file that you can close for now; and\nbegin serving the site in your R Studio viewer.\nThe console and viewer of your R Studio session should look like this:\n\n\n\nInitial editing\nAt this point, the default website is being ‘served’ locally. This means that changes you make will be reflected in the website that you see in your R Studio Viewer. To see the website in a web browser click the ‘show in new window’ button on the top left of the Viewer. This is circled in the above image. That will open the website using the address that the R Studio also tells you.\nHeadshot\nThe first change to make is to update the headshot. In your folder, go to my_website -> static -> img. Replace ‘portrait.jpg’ with your own square headshot jpg. If you do this correctly then when you go back to your website the image will have updated.\nPersonal details, contacts, and main menu\nTo update the biography and other details in that first pane, go to File -> Open File in the R Studio menu and open config.toml which is in my_website -> config.toml. This file will either open in a text editor or in R Studio – it doesn’t matter which. When you save the file the changes will be reflected in the website.\nSearch for ‘title’ or go to line 2. It should say:\n\n\n'title = \"Academic\"'\n\nChange that to:\n\n\n'title = \"Your Name\"'\n\nSearch for ‘[params]’ or go to line 21. There you can update parameters such as name, role, and contact details. If you don’t want a particular parameter to show up on your website then set it equal to \"\". (An example of this is on line 33.)\nOnce you’ve updated these parameters, search for ‘[[params.social]]’ or go to line 126. There you can update your contact details, such as email, twitter, etc. Just delete or comment out the full four lines if you don’t want a particular contact type displayed on your website. You can always add more later.\nFinally, search for ‘[[menu.main]]’ or go to line 152. There you can change the menu items that are displayed on the top right of your website. For instance if you don’t want a blog then delete or comment out the four lines:\n\n\n[[menu.main]]\n  name = \"Posts\"\n  url = \"#posts\"\n  weight = 3\n\nIf you want to change the order of the items then change the ‘weight’. Ascending values from left to right.\nBiography\nIn your folder, go to my_website -> content -> home -> about.md. That should open in R Studio or your text editor. Any changes that you save should immediately show up in your website.\nSearch for ‘# List your academic interests.’ or go to line 12. There you can change your academic interests. If you don’t want this to show up on your website then you can just delete or comment out lines 12-18.\nSearch for ‘# List your qualifications (such as academic degrees).’ or go to line 20. There you can change your academic qualification. If you don’t want this to show up on your website then you can just delete or comment out these lines.\nThe ‘year’ is a numeric field. If you’d prefer to include duration (e.g. 2013 – 2017), then replace the ‘2012’ with ‘“2013 – 2017”’ (the \"\" are important). Or similarly, if you are expecting a degree then you could replace the ‘year’ with ‘“Expected month year”’.\nSearch for ‘# Biography’ or go to line 43. There you can add a brief biography.\nTeaching\nMost of the other files in my_website -> content -> home just display content from elsewhere. This is because of the setup of the website. The exception is teaching.md. Open that and edit everything after line 15.\nPublications\nIn your folder, go to my_website -> content -> publication. There are two default publications added there. You can edit those and then copy them to add extra publications.\nPosts\nIf you want a blog in your website then the content is saved in: my_website -> content -> post. If you don’t want a blog then just delete this folder and comment out the posts menu item from my_website -> config.toml file so it doesn’t show up in the menu.\nOnce your website is working, if you want a new blog post, then you can simply use the R Studio menu bar: Tools -> Addins -> New Post.\nEtc\nGo through the different parts and change it as you need.\nSubsequent editing\nTo come back to editing your website once you’ve closed R Studio, go to the ‘my_website’ folder and then double-click on the Rproj file, ‘blogdown_test.Rproj’. That will open a new instance of R Studio.\nFrom there you can type ‘blogdown:::serve_site()’ into the console to serve your site and then continue editing, or you could use the R Studio menu bar: Tools -> Addins -> Serve Site.\nMaking your website public\nCommit\nSo far everything has happened on your own computer. The first step to making your website public is to commit these changes to GitHub. To do this open Terminal again and as before use cd and ls to navigate to ‘my_website’.\nOnce there, type each of the following lines (adding your own description) and follow each by ‘return’\n\n\ngit add -A\ngit commit -m \"DESCRIBE THE CHANGE YOU ARE ADDING\"\ngit push\n\n(You may be asked for your GitHub password. Terminal is a bit tricky to type passwords into because you don’t know how many characters you’ve typed, but have a go and follow it by ‘return’.)\nyour_domain.netlify.com\nThere are many ways to make your website public, but the best at the moment is to use Netlify. I don’t have anything to change from the instructions of Alison Presmanes Hill and you can follow those here: https://apreshill.rbind.io/post/up-and-running-with-blogdown/#deploy-in-netlify.\n\n\n",
    "preview": "posts/2017-07-21-getting-started-with-blogdown/images/blogdown_github_1.png",
    "last_modified": "2020-04-26T17:15:02-04:00",
    "input_file": {},
    "preview_width": 2152,
    "preview_height": 1142
  },
  {
    "path": "posts/2017-07-18-mapping-the-2016-australian-election-polling-place-results/",
    "title": "Mapping the 2016 Australian Election Polling Place Results",
    "description": "The note that follows introduces Australia's political system, and then details the process of downloading and merging first-preference votes by polling place, and then plotting it on an interactive map.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2017-07-18",
    "categories": [],
    "contents": "\n\n\n\n\nThe note that follows introduces Australia’s political system, and then details the process of downloading and merging first-preference votes by polling place, and then plotting it on an interactive map.\nAustralia’s political system\nIn 2016 Australia’s federal government was determined by the outcomes of elections in 150 divisions which each elected one member to the lower house. The Liberal/National Coalition won 76 seats which allowed it to form a majority government; while the Labor party won 69 seats to form the Opposition; the Greens and the Nick Xenophon Team each won one seat; and there were two Independent members (Andrew Wilkie and Cathy McGowan).\nVotes are cast at polling places in each division. In general voters can go to any polling place within their registered division, but some polling places that are close to a boundary will allow voting from there and some major polling places (such as the city hall of a state capital) will allow voting in any division.\nAlthough there are some exceptions divisions are generally constructed so that they each have roughly the same number of people. However this is not the case for polling places – some are much larger than others. Nonetheless it is interesting to see the geographic distribution of which party received the most first-preference votes in each polling place, especially in the context of which party won the division.\nPolling place data\nThe main packages for the data manipulation are the tidyverse and magrittr. leaflet allows the creation of interactive maps, ggmap creates static maps, and rgdal assists with dealing with geographic data. rmapshaper is used to reduce the size of the shapefile of division boundaries so that it is faster to load.\n\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(rgdal)\nlibrary(rmapshaper)\n\nThe polling place results can be downloaded by state from the AEC website at http://results.aec.gov.au/20499/Website/HouseDownloadsMenu-20499-Csv.htm. There the AEC also makes available a dataset that contains geocodes for each of the polling places. The separate datasets for each state need to be merged, and then each polling place needs to be geocoded. Finally some minor changes are needed to make the party names easier to follow.\n\n\n#### Read in the polling place datasets (are state specific), and the geocodes for each polling place. Then put it all together to have one geocoded polling place dataset for all of Australia: Australia_booths. Finally, create a dataset that is filtered so that it just shows the winner of each booth: Australia_booths_winner. ####\n# Data importing\ngeocodes <- read_csv(\"data/GeneralPollingPlacesDownload-20499.csv\", skip = 1)\nNSW_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-NSW.csv\", skip = 1)\nQLD_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-QLD.csv\", skip = 1)\nVIC_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-VIC.csv\", skip = 1)\nACT_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-ACT.csv\", skip = 1)\nTAS_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-TAS.csv\", skip = 1)\nSA_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-SA.csv\", skip = 1)\nWA_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-WA.csv\", skip = 1)\nNT_booths <- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-NT.csv\", skip = 1)\n# Merge\nAustralia_booths <- rbind(NSW_booths, QLD_booths, VIC_booths, ACT_booths, TAS_booths, SA_booths, WA_booths, NT_booths)\n# Add the geocodes\nAustralia_booths <- Australia_booths %>% \n  left_join(geocodes)\n# Clean up\nrm(NSW_booths, QLD_booths, VIC_booths, ACT_booths, TAS_booths, SA_booths, WA_booths, NT_booths)\n# If you need it use this to get a list of the parties, ordered by the number of first-preference votes\n# first_votes <- Australia_booths %>%\n#   group_by(PartyNm) %>%\n#   summarise(votes = sum(OrdinaryVotes, na.rm = TRUE)) %>%\n#   arrange(desc(votes))\n# Combine some parties that are separate, but equivalent: Australian Labor Party & Australian Labor Party (Northern Territory) Branch & Labor, Country Liberals (NT) & Liberal, The Greens & The Greens (WA).\nAustralia_booths$PartyNm <- recode(Australia_booths$PartyNm, \"Australian Labor Party (Northern Territory) Branch\" = \"Australian Labor Party\", \"Labor\" = \"Australian Labor Party\")\nAustralia_booths$PartyNm <- recode(Australia_booths$PartyNm, \"Country Liberals (NT)\" = \"Liberal/LNP\", \"Liberal National Party of Queensland\" = \"Liberal/LNP\", \"Liberal\" = \"Liberal/LNP\")\nAustralia_booths$PartyNm <- recode(Australia_booths$PartyNm, \"The Greens (WA)\" = \"The Greens\")\n# Create an indicator for who won the polling place then filter on that\nAustralia_booths_winner <- Australia_booths %>% \n  group_by(PollingPlaceID) %>% \n  mutate(polling_place_winner = ifelse(max(OrdinaryVotes) == OrdinaryVotes, max(OrdinaryVotes), 0)) %>%\n  filter(polling_place_winner >= 1)\n#table(Australia_booths_winner$PartyNm)\n# There are three parties that only win one booth, so combine all those into 'Other'\nAustralia_booths_winner$PartyNm <- recode(Australia_booths_winner$PartyNm, \"Australian Recreational Fishers Party\" = \"Other\", \"Christian Democratic Party (Fred Nile Group)\" = \"Other\", \"Derryn Hinch's Justice Party\" = \"Other\")\n\nDivision data\nThe divisions can be coloured based on which party won overall. The map of the boundaries for each division can be downloaded from the AEC website here: http://www.aec.gov.au/Electorates/gis/gis_datadownload.htm. The shapefile doesn’t have winner of each division so this needs to be merged into it. It is important to put the shapefile dataset first when merging. Finally, the shapefile is quite a large file and this can be reduced for faster loading.\n\n\n#### Read in the shapefiles (maps) that show each of the boundaries of the divisions (electorates) then add the data to say who won that division. Result is a spatial dataframe called boundaries. ####\n# Overall winner for each division, which will be used to color the division\nDivision_winner <- read_csv(\"data/HouseMembersElectedDownload-20499.csv\", skip = 1)\n# The boundaries of the divisions (downloaded from: http://www.aec.gov.au/Electorates/gis/gis_datadownload.htm)\nboundaries <- readOGR(dsn = \"data/national-midmif-09052016/COM_ELB.TAB\", layer = \"COM_ELB\")\n# Fix a couple - Mcmillan and Mcpherson - that have capitalisation issues\nboundaries$Elect_div <- recode(boundaries$Elect_div, \"Mcmillan\" = \"McMillan\", \"Mcpherson\" = \"McPherson\")\n# Add the overall division winner dataset into the boundaries dataset (thanks to http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS2_MergingSpatialData_part1_Joins.html)\nboundaries <- merge(boundaries, Division_winner, by.x = \"Elect_div\", by.y = \"DivisionNm\")\n# Simplify and reduce the size of the shapefile so that it loads better\nobject.size(boundaries)\nboundaries <- rmapshaper::ms_simplify(boundaries)\nobject.size(boundaries)\n# Clean up\nrm(Division_winner)\n\nThen colours need to be associated with each party.\n\n\n#### Specify the colour schemes that will be used. ####\n# Set the color scheme for the booth coloring\n# pal <- colorFactor(\n#   palette = \"Dark2\", \n#   domain = unique(Australia_booths$PartyNm)\npal <- colorFactor(palette = c(\"#c04745\", \"#616161\", \"black\", \"purple4\", \"#4776be\", \"#ff5800\", \"cyan1\", \"yellow\", \"#a8c832\", \"brown4\"), \n                          domain = c(\"Australian Labor Party\", \"Independent\", \"Informal\", \"Katter's Australian Party\", \"Liberal/LNP\", \"Nick Xenophon Team\", \"Other\", \"Pauline Hanson's One Nation\", \"The Greens\", \"The Nationals\"))\n# Set the color scheme for the division coloring\npall <- colorFactor(palette = c(\"#c04745\", \"#616161\", \"purple4\", \"#4776be\", \"#4776be\", \"#ff5800\", \"#a8c832\", \"brown4\"), \n                   domain = c(\"Australian Labor Party\", \"Independent\", \"Katter's Australian Party\", \"Liberal\", \"Liberal National Party\", \"Nick Xenophon Team\", \"The Greens\", \"The Nationals\"))\n\nInteractive map\nFinally, the map can be produced:\n\n\n#### Pull it all together to make the map ####\n# Make the map\nAustralia_map <- \n  leaflet() %>%\n  # Base groups\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %>% # Add a black and white alternative\n  setView(lng = 133.7751, lat = -25.2744, zoom = 4) %>% # Specify where the map is initially focused\n  addPolygons(data = boundaries, \n              color = \"#444444\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillColor = pall(boundaries$PartyNm),\n              highlightOptions = highlightOptions(color = \"#666\", weight = 2, bringToFront = FALSE)) %>% # Add the plot of the divisions, coloured by which party won it\n  addCircles(\n    data = Australia_booths_winner,\n    lng = Australia_booths_winner$Longitude, \n    lat = Australia_booths_winner$Latitude, \n    popup = paste(\"<b>Division:<\/b>\", as.character(Australia_booths_winner$DivisionNm), \"<br>\",\n                  \"<b>Polling place:<\/b>\", as.character(Australia_booths_winner$PollingPlaceNm), \"<br>\",\n                  \"<b>Address:<\/b>\", as.character(Australia_booths_winner$PremisesAddress1), \"<br>\",\n                  \"<b>Party with most first-pref votes:<\/b>\", as.character(Australia_booths_winner$PartyNm), \"<br>\",\n                  \"<b>First-pref votes:<\/b>\", as.character(Australia_booths_winner$OrdinaryVotes), \"<br>\"),\n    label = ~as.character(Australia_booths_winner$DivisionNm),\n    #clusterOptions = markerClusterOptions(),\n    color = pal(Australia_booths_winner$PartyNm),\n    fillOpacity = 0.5) %>% # Plot the booths, coloured by which party got the most first-preferences.\n  # Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"Toner Lite\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %>%\n  addLegend(\"bottomright\", pal = pal, values = Australia_booths_winner$PartyNm,\n            title = \"Which party won\",\n            #labFormat = labelFormat(prefix = \"$\"),\n            opacity = 1\n  )\n# Call the map\nAustralia_map\n\n\n\n",
    "preview": {},
    "last_modified": "2020-04-26T17:14:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-15-greitens-reports-for-duty/",
    "title": "Greitens Reports For Duty",
    "description": "Eric Greitens may be the Republican Übermensch. Rhodes Scholar, Navy SEAL officer, husband and father. He's now the Republican candidate in the Missouri gubernatorial election. And one suspects that being a governor could just be a step for Greitens. While 2016 will always be the year that US politics descended to the gutter, it could also be the year that the next Republican president begins his political career.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-10-15",
    "categories": [],
    "contents": "\n(Comment 27 March 2019. This has aged horribly! Not only was it written with a presumption that Clinton was going to win, but even though Greitens won he flamed out within 18 months having been accused of some very odd behaviour. Nonetheless, the visit to Missouri was fun, and I enjoyed putting this piece together.)\nEric Greitens may be the Republican Übermensch. Rhodes Scholar, Navy SEAL officer, husband and father. He’s now the Republican candidate in the Missouri gubernatorial election. And one suspects that being a governor could just be a step for Greitens. While 2016 will always be the year that US politics descended to the gutter, it could also be the year that the next Republican president begins his political career.\n\n\n\nIt is easy to imagine Eric Greitens as a senior at Duke University as he entered the boxing ring for his Golden Gloves bout. He may liked to have chewed on his mouth guard, and thrown a few jabs. But instead of touching gloves with an opponent to start a fight, Greitens entered the ring and was declared the winner. Greitens became the Golden Gloves Novice Champion without throwing a punch in anger – an opponent didn’t even show up.\nIn some ways, it’s unfair to mention this story. Greitens doesn’t hide the fact that he won unopposed – it was a key chapter in the memoir that acted as his political job application. But it is important to recognize how straightforward Greitens makes high achievement seem. It is easy to wonder if Greitens has ever felt close to failing.\n\n\n\nAt a recent rally of about 50 people in Des Peres on the outskirts of St Louis, Missouri, Greitens entered stage right. Greitens has lashed his political persona to his time as a Navy SEAL and styles himself as a ‘conservative outsider’. He started off g-droppin’ but then forgot himself and hints of his education could be heard. Greitens spoke like a head boy eager to impress – no fillers such as ‘um’ or ‘uh’, with plenty of appropriate gestures, and useful timing and cadence changes.\nHe’s still a little too polished to pull off George W. Bush’s everyman. He’s not yet comfortable enough in the weeds to give speeches about complicated ideas like Bill Clinton. And Greitens is not yet Obama in terms of ability to inspire. But Greitens is much better than many politicians at this stage of their career. It is important to recognize that events like this are the extent of many aspiring politician’s careers. And if he loses this race, Greitens surely has a plan for a political career that will be anything but average. Nonetheless, from his stump speech it is clear that if elected Greitens would like to be evaluated on jobs and education.\n\n\n\nIf you had to balance the US population like a plate based on where people live then the center would lie in Missouri. It is also in the middle of many measures of jobs and education, such as the unemployment rate and the percentage of the population with at least a high school education. Between 1904 and 2004 whichever candidate won Missouri also won the overall election with the exception of 1956. But, in 2008 and 2012 the state voted Republican even though Obama won overall.\nEconomically, Missouri did not have many of the financial sector jobs that were directly affected by the financial crisis of 2007-08. Nonetheless the state was hit hard. According to data from the St Louis Federal Reserve in May 2007 Missouri’s unemployment rate was 5.0 per cent, while the US unemployment rate was 4.4 per cent. A little over two years later, in December 2009, Missouri’s unemployment rate had reached 9.8 per cent while that of the broader US economy was 9.9 per cent. Since that time, Missouri’s unemployment rate reduced to 4.2 per cent in February 2016. But in contrast to the broader US economy during the past six months it has crept up. The September 2016 measure puts Missouri’s unemployment rate at 5.2 per cent, while that of the broader US economy was 4.9 per cent.\nGreitens didn’t mention many specific economic plans, but as it is only a few hours south of Chicago the location of St Louis, Missouri’s second-largest city, brings many economic opportunities. For instance St Louis could piggyback on the large number of tech firms that are starting in Chicago. If he wins, Greitens could convert some of the old St Louis warehouses into tech-friendly offices and offer any business coming out of a Chicago incubator subsidized rent if they, say, take on at least one intern from a Missouri college. Sure, most of the businesses will not survive, but those that will may stay in St Louis if, as Governor, Greitens ensures they plant roots.\nOn education Greitens gave the Republican rank-and-file what they wanted. The only point of agreement among warring Republican factions this presidential cycle seems to be an agreement that Common Core is terrible. It is worth remembering that Common Core is essentially just a set of education standards established by the federal government, but with considerable implementation leeway at a state level. Nonetheless Republicans from Jeb Bush to Donald Trump oppose it, and so does Greitens. Greitens also brings unions into the education debate, insisting that teachers should not be forced to join a union as a condition of employment.\nWhile neither of these issues are likely to really help Missouri improve its educational outcomes, they do suggest that Greitens has at least some understanding of the importance of give-and-take in politics. And there are measures, such as increased support for vocational training and improved teacher training and support that he could easily tweak if he wins.\nGiven Missouri’s circumstances there is plenty of opportunity for improvement and Greitens isn’t short on ambition. Reading his first book, ‘The Heart and the Fist’, published in 2011, makes it clear that running for office has always been his plan. And the website ‘ericgreitensforpresident.com’ was registered at least as early as 21 July 2009, so it’s clear what his goal is. Greitens makes life-defining achievements seem easy, but the swiftboating of John Kerry in 2004 showed how easy it can be to muddy a political opponent.\nGreitens has already picked up a couple of bad habits. At the rally he referenced his humanitarian work in Kosovo. But in his memoir, the humanitarian work is described as happening in Croatia. While there are undoubtedly nuances due to historical and cultural factors that make the issue complicated, it is also the case that the largest population of Kosovars outside of the country lives in St Louis. This difference has already been noted by at least one local newspaper and while it’s not damning, Greitens should be more careful.\n\n\n\nGreitens also seems to have an unexpected aversion to openness, such as not releasing his tax returns. This is odd given that for so much of his life his primary income would likely have come from the Navy and then a non-profit that he founded. He’d have some income from public speaking fees, but it’s hard to see why these would be embarrassing.\nThere’s also a lot that needs to be improved in his campaign. For instance, there was no staffer at the door to the rally gathering the contact details of attendees. And there were no merchandise sales either at the rally or on Greitens’ campaign website.\n\n\n\nWhether he wins or loses the Missouri gubernatorial race on November 8, this won’t be last we hear of Eric Greitens. In four years, he could run again for Governor or even against the Democratic Senator from Missouri, Claire McCaskill. The Republican 2020 cohort is firming with Senator Tom Cotton, Speaker Paul Ryan, and Governors John Kasich and Mike Pence already positioning themselves.\nGreitens is not perfect. But those who serve in the military are among the best of us. And Rhodes Scholars are among the cleverest. His success as a politician will be worth watching.\n\n\n",
    "preview": "posts/2016-10-15-greitens-reports-for-duty/images/StLouis_Greitens_1.jpg",
    "last_modified": "2020-04-26T17:14:07-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-08-professional-amateurs/",
    "title": "Professional Amateurs",
    "description": "Despite many unforced errors Hillary Clinton has won the Democratic nomination and polls suggest she will beat Donald Trump. But her campaign continues to make unforced errors. There was plenty of evidence of an amateur nature to what should be a professional campaign at a recent rally for Clinton’s running mate, Tim Kaine, in Grand Rapids, Michigan.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-09-08",
    "categories": [],
    "contents": "\nDespite many unforced errors Hillary Clinton has won the Democratic nomination and polls suggest she will beat Donald Trump. But her campaign continues to make unforced errors. There was plenty of evidence of an amateur nature to what should be a professional campaign at a recent rally for Clinton’s running mate, Tim Kaine, in Grand Rapids, Michigan.\n\n\n\nThe wait for Tim Kaine was around three hours. That’s not unusual. But it meant the campaign had three hours in which no one was able to walk away or claim they had somewhere else to be. They had three hours to turn supporters into advocates. They blew it.\nWe wrote our contact details on a piece of paper while we waited in line. Presumably a volunteer’s time was later spent transferring the scrawls into a computer. Why not have us enter our contact details ourselves into a tablet? It’s the work of just a few hours to make an app to do that.\nLocal campaign volunteers spent a lot of time leading chants while we waited in line. Americans seem to like this, and enthusiasm is important - but it should be captured and used in ways that get more votes. Everyone that I talked to was going to vote for Clinton, but what about their friends, families and neighbors? Many in the line complained that there was no easy way to volunteer. They were so fired up against Trump that they wanted to do something right away. Why did the Clinton campaign not capture this enthusiasm by using the time spent in the line to organize? It would have been easy to get those supporters to commit to doorknock their neighborhood.\n\n\n\nTo actually get into the event you had to hand-over the piece of paper with your contact details. Why did the campaign not send an SMS or email that could be shown to gain entry, thereby checking that the information was accurate? Even better, the local campaign could have made an organizing app, had people download it and then sent an entrance ticket using that. Sure, some people don’t carry phones or aren’t comfortable with technology, but it’s easy enough to use pens and paper for them.\nFinally, there no merchandise sales at the door. T-shirts are an easy source of $10 - $20 per person, and buttons an easy source of $5. With about 500 people at the event, local campaigns could have easily raised at least a few thousand dollars.\n\n\n\nClinton admits that she is not a good retail politician. And that’s fine. It may make it more difficult to get elected, but in many ways it could be an advantage when it comes to actually being president. However, the Clinton campaign didn’t seem able to even do the basic task that wins elections - organizing supporters to talk to their friends, family and others in their community.\nTrump is an extraordinary factor pushing supporters toward Clinton. But her campaign needs to establish a network of committed supporters that it can lean on in 2020 when she will ask for a fourth Democratic term. Clinton has made many unforced errors and gotten away with it. But there is no good reason that her campaign should not be better at organizing. Although it looks as though she will beat Trump, she’s risking 2020 if her campaign doesn’t stop making these sorts of basic errors.\n\n\n\n\n\n",
    "preview": "posts/2016-09-08-professional-amateurs/images/ProfessionalAmateurs-WelcomeSign.jpg",
    "last_modified": "2020-04-26T17:13:36-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-17-trump-revisited/",
    "title": "Trump, Revisited",
    "description": "Donald Trump is an improved politician, but it's unlikely to be enough. He has harnessed fervent anti-Clinton sentiment amongst Republicans. But he does not have time to build the coalitions usually needed to win a US presidential election.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-07-17",
    "categories": [],
    "contents": "\n(Comment 4 July 2017: Like much of my writing about Trump, the opinions here haven’t aged too well. Nonetheless, I learnt a lot from writing this and later from thinking about why I went wrong.)\nA few notes and photos from a Trump rally in Indiana earlier this week. The focus is on whether Trump ‘could’ win the election in November. In the interest of transparency, it’s worth acknowledging that I didn’t think Trump could win the Republican nomination. Thanks to Monica for helpful edits.\n\n\n\nDonald Trump is an improved politician, but it’s unlikely to be enough. He has harnessed fervent anti-Clinton sentiment amongst Republicans. But he does not have time to build the coalitions usually needed to win a US presidential election.\nAt a recent rally in Westfield, Indiana, Trump was comparatively structured and measured. There was less name-calling and little of the ludicrous hypotheticals that characterized a January rally in Iowa.1 2 Some things haven’t changed: the crowd is still overwhelmingly white; ‘Storm Trumpers’ in ill-fitting suits are still on patrol; and Trump still lies.3 But he is no longer politically inexperienced, and neither were the Trump supporters that I talked to.\n\n\n\nThe Republican Governor of Indiana, Mike Pence, introduced Trump at the rally. Pence was later announced as Trump’s running mate. If Trump is to win the election then he needs to easily win states such as Indiana, where Republicans have only lost once in the past fifty years.4 Even though the outcome in Indiana should not be in doubt, Hoosier Republicans are important. Trump needs fired-up volunteers to travel to neighboring Ohio, a crucial swing state, and Trump needs money. But mostly, Trump needs friends.\nDespite Trump’s improvement as a politician, winning a presidential election usually requires constructing coalitions. Often this is the work of a lifetime. For instance, Richard Ben Cramer describes how, beginning in his 20s, George H. W. Bush built a Christmas card list. By the time he was Vice President 30,000 ‘friends’ received an annual Christmas card from him.5 The Clintons have been building coalitions since their 20s too. Part of Trump’s appeal is that he has only been a politician for a year, but his campaign is inefficient without coalitions.\n\n\n\nThe most precious resource in any election is a candidate’s time. The US presidential election magnifies this because of the scrutiny, the electoral college, and the size of the country. Yes, a candidate needs to raise money, motivate supporters, and convince undecided voters. But to stand a chance of winning, a candidate usually also needs coalitions that can do all this for them. Without these, there is more pressure on Trump.\nTrump has also only recently put modern campaign essentials in place. For instance, just a few months ago Trump described the use of data in politics as ‘overrated’.6 But he seems to have changed his mind: Trump collected and verified the phone numbers of those who attended the rally in Indiana. Texts and phone calls will be critical to the effort of getting his supporters to turn out to vote in November. Trump is also now sending emails but it takes time to build a high-quality list.\n\n\n\nThe Trump supporters that I spoke to were unfailingly polite. They conscientiously thanked the many law enforcement personnel, and there were many military veterans in the audience. No one supported every aspect of Trump’s platform, but this is not unusual in political campaigns. There was some anti-Muslim sentiment, and a few conspiracy theories. Political correctness was a recurrent issue, as was declining US influence in the world. Although Trump uses ‘The Wall’ as a call-and-response device (Trump: ‘Who’s going to pay for the wall?’ Crowd: ‘Mexico’), anti-Mexican sentiment seemed to be driven more by a perceived willingness of Hispanics to work for low wages than racism.\nThe Republicans I talked to were united only in being against Clinton; but similarly many Democrats seem united only in wanting to stop Trump. How such a campaign translates into votes is unclear. While there don’t seem to be many undecided voters, there are many dejected ones. It will be interesting to see turnout estimates in swing states. Would you stand in line to vote against, rather than for, a candidate?\n\n\n\nIn just one year Trump has changed US politics. He is quickly improving as a politician, but remains divisive. Is Trump the moment, or just of the moment? Although he probably does not have enough time to do the work that would allow him to win in November,7 the impact of his campaign will be felt for many years.\nGo to https://www.rohanalexander.com/2016/01/14/notes-and-photos-from-iowa/ for that write-up.↩\nThe speech can be viewed here: https://youtu.be/ewMhP-V1ed8, as at 15 July 2016.↩\nSee, for instance, Politifact (http://www.politifact.com/personalities/donald-trump/) which awarded Trump PolitiFact’s 2015 Lie of the Year and rules 58 per cent of his statements as either ‘False’ or ‘Pants on Fire’.↩\nSee the entry for Indiana here: https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state, as at 15 uly 2016.↩\nSee Richard Ben Cramer’s ‘What It Takes’, page 153 of the Vintage; Reprint edition (June 1, 1993).↩\nSee: http://bigstory.ap.org/article/6d588a38061c4657a557d1dde86782ec/trumps-questioning-value-data-worries-republicans, accessed 15 July 2016.↩\nIt is estimated that Trump currently has a 20-40 per cent chance of winning the election. For instance, see the Five Thirty Eight election forecast (http://projects.fivethirtyeight.com/2016-election-forecast/), as at 15 July 2016 or the New York Times summary of election polls (http://www.nytimes.com/interactive/2016/us/elections/polls.html), again as at 15 July 2016.↩\n",
    "preview": "posts/2016-07-17-trump-revisited/images/2016_07_17_Children.jpg",
    "last_modified": "2020-04-26T17:13:28-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-03-broader-thinking-needed-on-the-australian-budget/",
    "title": "Broader Thinking Needed on the Australian Budget",
    "description": "The Treasury Secretary, and many others, bemoan the wasted years of the mining boom. Most agree that Australia should have more to show for what was the most significant boom since Federation. But the boom is over. And a fixation on budget surpluses means that we are missing an opportunity to make up for it. Australia’s credit rating is a strength that we should take advantage of. The Commonwealth should be borrowing to fund infrastructure investment.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-02-03",
    "categories": [],
    "contents": "\nThe Treasury Secretary, and many others, bemoan the wasted years of the mining boom. Most agree that Australia should have more to show for what was the most significant boom since Federation. But the boom is over. And a fixation on budget surpluses means that we are missing an opportunity to make up for it. Australia’s credit rating is a strength that we should take advantage of. The Commonwealth should be borrowing to fund infrastructure investment.\n\n\n\nDespite what many think, ‘deficit’ is not a dirty word, and like many other aspects of life, context matters. For instance, students are rarely chastised for taking a HELP loan. This is because that debt is being used to buy an education, which helps in the long term. It’s the same for the economy.\nIn a recent speech to the Sydney Institute the Treasury Secretary said that the Federal Government must exercise ‘expenditure restraint (that) will allow resources that would otherwise go to interest payments to be allocated to other priorities…’.1 But why should we lash expenditure restraint to interest payments? The Secretary explains that his rationale for this comes from the lessons of the late ’80s and early ’90s. But interest rates are no longer at such high levels. If the costs of borrowing have changed, then should we still dismiss its benefits?\nAdditionally, it is not clear why it should be the case, as the Secretary claims, that if the Commonwealth achieves surpluses then the states would ‘…run small(er) overall deficits that they can use to finance productive infrastructure investment’. Even if state governments were to identify and fund the best investments for their state, shouldn’t the Commonwealth be concerned about what would be best for Australia overall?\nThe Secretary implies that interest payments are wasted money and so we should do whatever it takes to reduce this. But to focus only on the cost of borrowing means to miss out on the benefits. For instance, many Australians choose to incur the cost of making interest payments so that they can enjoy the benefits of owning a house. Why should the Commonwealth be any different? If the Commonwealth were to issue bonds to fund projects in the public interest, it would be following in the footsteps of many governments since Federation.2\nThe current low interest rate environment is handy because it allows us to cheaply borrow money. This is especially true of the Commonwealth, which could issue bonds for a low cost at the moment. Even if there were no obvious efficient infrastructure projects available right now, locking in the money at these levels would be no bad thing even if it takes a year or two to identify appropriate projects.\nTaking on debt is actually what many large corporations are doing at the moment. For instance, Apple recently issued bonds and has now raised more than $55 billion since 2013.3 And Visa raised $16 billion via bond sales before Christmas.4 For what it is worth, the Commonwealth’s credit rating is better than either of those businesses.\nInterest payments are only inherently a waste of money if the principal is wasted. But if it is being used for national infrastructure projects then the question is more nuanced. The circumstances that the Australian economy faces are different to those of the early ’90s, and our policy responses and public debate should remain up-to-date.\nhttps://treasury.gov.au/speech/the-australian-budget-some-context/↩\nhttp://www.rba.gov.au/publications/rdp/2012/pdf/rdp2012-09.pdf.↩\nhttp://www.bloomberg.com/news/articles/2015-09-10/apple-said-to-market-euro-bonds-adding-to-53-billion-debt-binge↩\nhttp://www.bloomberg.com/news/articles/2015-12-09/visa-said-to-start-marketing-bonds-backing-visa-europe-takeover↩\n",
    "preview": "posts/2016-02-03-broader-thinking-needed-on-the-australian-budget/images/DSC_0599.jpg",
    "last_modified": "2020-04-26T17:13:21-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-14-notes-and-photos-from-iowa/",
    "title": "Notes and Photos From Iowa",
    "description": "Bernie Sanders seems quite reasonable for a revolutionary. An energetic man of 74, he spoke for an hour in Perry, Iowa, to a room of 300 from only a few lines of handwritten notes, and then fielded half an hour of questions. He does not have the same aura that surrounded, then, Senator Obama in his own Iowa battle with, then, Senator Clinton in 2008 say those who saw both. Instead, Sanders has preternatural calm.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2016-01-14",
    "categories": [],
    "contents": "\nSincere thanks to Bec, Callam, Monica, and Owen for reading and improving these notes.\nBernie Sanders seems quite reasonable for a revolutionary. An energetic man of 74, he spoke for an hour in Perry, Iowa, to a room of 300 from only a few lines of handwritten notes, and then fielded half an hour of questions. He does not have the same aura that surrounded, then, Senator Obama in his own Iowa battle with, then, Senator Clinton in 2008 say those who saw both. Instead, Sanders has preternatural calm.\n\n\n\nFigure 1: Photo of Bernie Sanders and audience at Perry, Iowa, by Monica Alexander.\n\n\n\nSo much calm, in fact, that some audience members who walked into the auditorium undecided, walked out excitedly supporting Sanders’ ‘socialist’ revolution. His revolution includes, amongst other features, universal medical coverage, paid maternity leave and a $15 hourly minimum wage; hardly revolutionary notions in many developed countries. For instance, few politicians that are against these policies get elected in Australia. However, Sanders will test whether a politician who supports them can get elected in the US.\nMuch has been made of Sanders’ recent polling. But the Iowa Caucasus, especially for Democrats, are a test not just of support, but enthusiasm and strength of will. Participants do not vote, they caucus. This means they gather with their neighbours and publicly indicate their support of a candidate - sometimes by raising a hand, sometimes by moving to a side of a room. Supporters must resist peer pressure, and continue supporting their candidate even as others try to sway them.\nIn addition to popularity, winning in Iowa requires organisation and attention to detail. This is why the operatives of Secretary Clinton, who is 68, remain content despite the polls. Caucusing is onerous, especially for those with children or without transport. A campaign is only as good as the number of supporters that it can get to turn up on the night. Clinton operatives are quick to mention their advantage in terms of this ‘ground game’.\nClinton’s ground game advantage is partly due to experience, but it is also due to money. Sanders unexpectedly raised $33 million in the final quarter of 2015, which compares favourably with Clinton’s $37 million, but Clinton has other sources of financial support. Sanders’ supporters should hope that the evident lack of preparation at the event in Perry, Iowa, itself (for instance, Sanders had to repeatedly ask for water as none had been left on the podium, nearly losing his voice on occasion) are not indicative of broader organisational oversights.\n\n\n\nFigure 2: Photo of Trump protesters.\n\n\n\nDonald Trump is different. More than a thousand people watched Trump, who is 69, speak in Cedar Falls, Iowa. Almost all were white. What he lacks in substance or structure, Trump makes up for with self-confidence. For 40 minutes he verbally picked at this and that, discussing polls, as well as goading opponents via ‘hypotheticals’ and name-calling. The audience was allowed no questions.\n\n\n\nFigure 3: Photo of Donald Trump at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\nTrump does not campaign in poetry (although he did literally recite song lyrics) and he seems unlikely to govern in prose. His operatives were clad in ill-fitting suits and shiny leather shoes that looked newly purchased. These ‘Storm Trumpers’ were unfailingly polite, but nonetheless menacing. The loudspeaker request, moments before Trump spoke, to not physically harm protesters was chilling rather than reassuring, not least since it was followed by laughter from the crowd. The comfort of knowing that the Secret Service would have confiscated any knives or guns at the door was relative rather than absolute. [Edit 26/1/16: Trump has since asserted that he could shoot somebody and not lose any voters. You get the uneasy sense that it would be better for his theory to remain untested, for fear that he may onto something.]\n\n\n\nFigure 4: Photo of Donald Trump audience at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\nTrump’s political inexperience seems matched by that of his supporters. His warm-up acts (one of whose qualification, as she explained, was being a runner-up on The Apprentice) spent considerable time explaining the importance of turning out to caucus on 1 February. Unlike the Democrats, Republican caucus-goers do not have to be as resistant to peer pressure - secret ballots are possible. But much of the crowd seemed new to the political process, and getting each of them to turn out, and in some cases register as Republican, may be too much to expect. If so, then it is likely that Senator Ted Cruz, a 45-year-old conservative Republican from Texas, will prevail.\n\n\n\nFigure 5: Photo of Trump at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\nFor all their differences, it is the same anger that propels Sanders and Trump toward the top of the polls. Neither candidate is an establishment member of their respective parties – Sanders only joined earlier this year despite having generally voted with the Democrats as a senator, and Trump appears to swear allegiance only to himself. It is the feeling of being let down by the status quo, of the system needing a catalyst for something more, that drives their popularity.\nWhile Sanders’ policies may not make much difference for today’s caucus-goers, he speaks to their concern that their children’s lives may not be better than their own. His is an appeal for hope. Trump’s appeal is to those who feel they are worse off now than they were in the past. He gives them someone to blame, and provides solutions such as tariffs and walls, that some see as plausible. Perhaps for Sanders the glass is half-full; for Trump, half-empty?\nThe economic reality is that feeling worse off is reasonable for many Americans. After accounting for inflation the 2014 measure (which is the most recent one) of American household median income is lower than it was in 1997. And, as both Trump and Sanders accurately explained to their audiences, the 5 per cent unemployment rate that President Obama appeals to as a measure of his success is artificially low because some have given up looking for work.\nThose suffering most from the success of Sanders and Trump are candidates such as John Ellis Bush. Jeb, 62, seems to have carefully studied the art of coming across as a nice guy, albeit one who is a little annoyed about having to speak to about 200 people in Coralville, Iowa. Being a Bush comes with baggage and expectations, but it does have its advantages, such as immaculate event advance work and plenty of press.\n\n\n\nFigure 6: Photo of Jeb! at Coralville, Iowa, by Monica Alexander.\n\n\n\nJeb emphasised his commander-in-chief credentials. His older brother’s war in Iraq may have ensured that the age of aggressive American imperialism is over for now, but Jeb was still introduced by a retired Admiral who spoke of Jeb’s leadership fighting hurricanes in Florida and neighbouring states.\n\n\n\nFigure 7: Photo of Jeb! at Coralville, Iowa, by Monica Alexander.\n\n\n\nThe crowd was lively and Jeb was frequently interrupted by cheers. He spoke without notes for about 30 minutes to a crowd surrounding him on all four sides. Maybe this is a plan to seem more approachable, or to encourage audience participation? An hour of questions, courtesy of roaming microphones, followed. Jeb worked hard to be nice, but it was apparent that he was, indeed, working at it, because he occasionally lapsed and gently made fun of a questioner.\nFor all the well-acknowledged flaws of presidential systems in general, and the US one specifically, it has much to be proud of. Every candidate for their party’s nomination will visit Iowa at some stage this week, many visiting multiple towns in a day. The scrutiny is intense. Iowa is a state of 3 million people, divided into 99 counties, and by 1 February, many candidates will have visited every county.\n\n\n\nFigure 8: Photo in Iowa, by Monica Alexander.\n\n\n\nIf one were starting from scratch, the Iowa Caucuses would probably not be the way to go. Iowa is not representative of the rest of the United States, and its outsized electoral importance skews national policy. But in their own way, the Iowa Caucuses are valuable. In general, the nature of the candidates seems to come through in the events because they are so intimate. Given that so much of leadership consists of reacting to unexpected events, and the difficulty that politicians tend to have implementing their desired policies, perhaps this is the most important consideration. It may be that, to rework Churchill’s aphorism, the Iowa Caucuses are the worst way to select presidential candidates, apart from all the others.\n\n\n\nFigure 9: Photo in Iowa, by Monica Alexander.\n\n\n\nThere are many aspects of the United States that should not be considered, let alone encouraged, in Australia. But the rigorous examination of candidates facilitated by the Iowa Caucuses is something Australia should emulate.\n\n\n",
    "preview": "posts/2016-01-14-notes-and-photos-from-iowa/images/2016_01_11_Bernie.jpg",
    "last_modified": "2020-04-26T17:13:12-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-31-prepare-for-future-economic-crises-now/",
    "title": "Prepare For Future Economic Crises Now",
    "description": "Few policymakers were prepared for the financial crisis of 2007-08. Until it hit, their focus was on more obvious threats to the economy, instead of such an unexpected event. Could this be because planning for unexpected economic events is not the explicit responsibility of any particular policy-maker? If so, this has to change.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2015-08-31",
    "categories": [],
    "contents": "\nOriginally published in the Canberra Times.\nFew policymakers were prepared for the financial crisis of 2007-08. Until it hit, their focus was on more obvious threats to the economy, instead of such an unexpected event. Could this be because planning for unexpected economic events is not the explicit responsibility of any particular policy-maker? If so, this has to change.\n\n\n\nFigure 1: Always better to buy a helmet before you fall off the bike.\n\n\n\nAfter the collapse of the US housing market, Lehman Brothers’ bankruptcy became an economic Rorschach test. Policymakers at the Reserve Bank of Australia and the Australian Treasury saw it as evidence that some banks cannot be allowed to fail. In response, the RBA reduced the cash rate and the Treasury implemented a stimulus package.\nWith the benefit of hindsight, some (such as Nobel Laureate Paul Krugman) argue that we should have anticipated the collapse. This is more than just wishful thinking. We now know there were those (for instance Raghuram Rajan, now the Governor of the Reserve Bank of India) whose warnings were more or less spot-on. Unfortunately, they were mostly ignored. It just seemed too unlikely that trouble in the US housing market could cause widespread recessions.\nIn an effort to prevent a repeat of the financial crisis, policymakers at the RBA, Treasury and elsewhere have taken steps to mitigate the weaknesses of the financial sector. And they have studied how those weaknesses could affect the broader economy. But there is a danger that they will overlook economic threats that do not originate in the financial sector. Like a general who trains troops to fight past wars, we may again be caught unprepared.\nThe financial crisis taught us that even infallible elements of an economy can fail. But because such failure is difficult to imagine ex ante, there appears to have been little attempt to act on this lesson. One way would be to practise dealing with unfamiliar – and less obvious – economic threats.\nThe process should begin by systematically examining the economy for weaknesses beyond the financial sector. Policymakers need to identify which elements of the economy pose systemic risks. We now know about the financial sector and there are many initiatives to deal with its risks, but we need to search for others.\nPolicymakers need to stress-test elements of the economy that appear robust and unfaltering, not just the financial sector. And they should conduct scenario analysis to practise responding if those elements were to fail. Just as the possibility that the US sub-prime housing market could cause recessions was almost unfathomable in 2006, the cause of the next economic crisis could seem unlikely to us now. Policymakers need to search broadly for possibilities.\nOf course most of the scenarios that are considered will never occur, and policymakers should not be given carte blanche to make legislative changes. What is needed is less heavy-handed but more useful: policymakers need to publicly consider and practise reacting to, unexpected events. This makes it more likely that they will react well when an unexpected crisis hits.\nThe financial crisis made it clear that unexpected elements of an economy can fail. Policymakers should be better prepared for this. They must fix the weaknesses identified by the financial crisis and deal with current threats. But importantly, policymakers must also give consideration to more obscure risks. It is only through this effort that we can hope to build an economy that is more resilient to future crises.\n\n\n",
    "preview": "posts/2015-08-31-prepare-for-future-economic-crises-now/images/DSCF1945.jpg",
    "last_modified": "2020-04-26T17:13:01-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-24-in-appreciation-of-ronald-coase/",
    "title": "In Appreciation of Ronald Coase",
    "description": "Ronald Coase, one of the most influential economists of the twentieth century, passed away in 2013 aged 102. Reading his papers today, I wonder whether he'd have become an economist if he were making that decision now.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2014-12-24",
    "categories": [],
    "contents": "\nRonald Coase, one of the most influential economists of the twentieth century, passed away in 2013 aged 102. Reading his papers today, I wonder whether he’d have become an economist if he were making that decision now.\n\n\n\nFigure 1: Coase’s article about lighthouses is brilliant. Which economist could write a paper like that today? Photo by Monica Alexander.\n\n\n\nCoase began his career by publishing ‘The Nature of the Firm’ in 1937. That article reported the results of Coase’s year-long study of American firms while he was an undergraduate at the London School of Economics (LSE). Until that article was published it was not clear why firms should do better when they are free to interact in a market, yet most individuals give up that freedom by tying themselves to a firm. Coase explained that it was due to transaction costs.\nAt the time the article was published Coase was teaching at the LSE, but this was soon interrupted by World War II. As he describes, during the war Coase ‘entered government service doing statistical work, first at the Forestry Commission and then at the Central Statistical Office, Offices of the War Cabinet’. He returned to the LSE after the war.\nIn 1951 he moved to the US taking a position first at the University of Buffalo then at the University of Virginia. He studied issues such as the allocation of radio frequency spectrum, which lead to the publication of ‘The Problem of Social Cost’ in 1960.\nThat article was concerned with externalities, that is, when an individual’s decision affects others. Think of factory that pollutes a river thereby affecting the livelihood of downstream fishermen. Although our inclination may be to shut down the factory, Coase points out that perhaps it would be better for the factory to continue to pollute if it also compensates the fishermen. For Coase, it was important to compare the costs with the benefits.\nThis article became one of the most cited articles in economics, and has been particularly influential in law. For instance, Michael Kirby describes a case where a decision would traditionally have been made by reference to cases ‘…decided long ago and in another country’. In contrast, the opinion of the NSW Court of Appeal, written by Kirby, referenced the costs and benefits of various options.\nIn 1964 Coase moved to the University of Chicago, alongside economists such as Gary Becker, Milton Friedman, and George Stigler. Coase’s influence was recognised in 1991 when he was awarded the Nobel Prize in Economics (now in 2014, formally the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel). He remained active at an advanced age, publishing a book about the rise of China when he was 101.\nCoase was an economist in the style of Adam Smith and John Maynard Keynes. He used mathematics sparingly in publications, and he ensured that his analysis was grounded in reality. Unfortunately these traditions are now largely lost to the profession. How many undergraduates take a year to study how businesses actually work, as Coase did?\nIn 2012 he published an article bemoaning how isolated economics was from ‘the ordinary business of life’. He believed that the changed nature of economics had undermined the usefulness of the subject and argued the need for it to be ‘firmly grounded in systematic empirical investigation of the working of the economy’.\nCoase’s work reminds us that economics does not have to be about manipulating abstractions on a whiteboard. His passing was a loss, not only to his friends and family, but also to the entire economics profession.\n\n\n",
    "preview": "posts/2014-12-24-in-appreciation-of-ronald-coase/images/IMG_0145.jpg",
    "last_modified": "2020-04-26T17:12:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-24-i-ll-have-what-they-re-having/",
    "title": "I'll Have What They're Having",
    "description": "Some accuse the Rich White Males from San Francisco's Bay Area of only making products for other Rich White Males. But that neglects the fact that what they want is sometimes also what the rest of us want.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2014-12-17",
    "categories": [],
    "contents": "\nSome accuse the Rich White Males from San Francisco’s Bay Area of only making products for other Rich White Males. But that neglects the fact that what they want is sometimes also what the rest of us want.\n\n\n\nFigure 1: If you’re rich even the San Francisco Bay can be your playground.\n\n\n\nFor instance, in the case of Facebook, Rich White Males wanted an easy way to communicate with their friends and family. Facebook is a success because others want this too. In the case of Uber, a taxi alternative, Rich White Males wanted frictionless transport. It turns out that everyone else (other than taxi medallion owners) wants this too.\nBloomThat, a Bay Area flower delivery business, is on the face of it, one of the newest solutions to Rich White Male problems.\nAlthough I’m neither rich nor white, being a twenty-eight year old male, I’ve been using flower delivery services for about a decade, and it’s often an awful experience. Their websites are too complicated, and have too many garish options. Their prices are too high. And delivery is ‘next day’, when I usually want flowers delivered right away.\nBloomThat claims to solve these problems. Each day they offer a small number of seasonal options that look as though Apple’s Jony Ive designed them. The small selection not only keeps it simple, but also reduces wastage. And they promise delivery within 90 minutes.\nCritics would say this is perhaps the definitive Rich White Male product. And although, like most Bay Area tech businesses, BloomThat’s founding team and initial investors (which includes Ashton Kutcher – lucky Mila Kunis!) are all male, I’m not sure that should be damning in this case.\nFor one thing, according to the founders, BloomThat’s numbers indicate the majority of their customers are female.\nBut more importantly, as a customer, the thrilling aspect of BloomThat isn’t the flowers, it’s how easy the business makes it for you to delight another person.\nTo my mind, BloomThat has attracted millions of dollars of funding for one reason. Investors think that if they can do this with flowers, then they can do it with other products. BloomThat hasn’t yet expanded beyond the Bay Area (although they’ll soon be in LA) but they’re already trying partnerships with complimentary products, such as donuts and SoulCycle.\nThese days many products arrive within a couple of days of being ordered. That used to be delightful, but the Rich White Males are tired of it. They want their products immediately. And they’d better enjoy the ordering process as well. By investing in businesses like BloomThat, they’re betting that applies to you too.\n\n\n",
    "preview": "posts/2014-12-24-i-ll-have-what-they-re-having/images/DSCF3336.jpg",
    "last_modified": "2020-04-26T17:12:23-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-19-the-human-equation-an-interview-with-george-zachary/",
    "title": "The Human Equation: An Interview With George Zachary",
    "description": "In an age when you can buy data-driven refrigerators and Moneyball is nominated for Academy Awards you may be surprised to hear there are investors who describe themselves as gut-driven. Frankly I thought such people would be too embarrassed to be out in public. Then I heard George Zachary talk.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2014-03-19",
    "categories": [],
    "contents": "\n\n“Two bicyclists start twenty miles apart and head toward each other, each going at 10 mph. At the same time a fly that travels at 15 mph starts from the front wheel of the southbound bicycle and flies to the front wheel of the northbound one, then turns around and flies to the front wheel of the southbound one again, and continues in this manner till he is crushed between the two front wheels. What total distance did the fly cover?” One way to find the answer is to calculate the distance the fly covers on the first leg of the trip, then on the second, then on the third, etc., and, finally, to sum the infinite series. The quick way is to observe that the bicycles meet exactly one hour after their start, so that the fly had just an hour for his travels; the answer must therefore be 15 miles. When the question was put to John von Neumann, he solved it in an instant, and thereby disappointed the questioner: “Oh, you must have heard the trick before!” “What trick?” asked von Neumann, “All I did was sum the infinite series.”\n\n\nPaul Halmos on John von Neumann\n\nIn an age when you can buy data-driven refrigerators and Moneyball is nominated for Academy Awards you may be surprised to hear there are investors who describe themselves as gut-driven. Frankly I thought such people would be too embarrassed to be out in public. Then I heard George Zachary talk.\n\n\n\nFigure 1: Some people can just see the answer, the rest of us have to work at it.\n\n\n\nZachary is a self-described ‘gut-driven investor’. But I didn’t storm out of the talk muttering about negative-alpha because Zachary is a venture capitalist at Charles River Ventures (CRV). And he has an impressive record, mostly notably being the first institutional investor at Twitter and Yammer. Of course, this could be down to chance, but what are the chances of it happening twice? So I was curious to understand what made Zachary tick.\nZachary is a first-generation American, whose parents were born in Greece. He made his initial money in the early ‘90s as a founder of Shutterfly, which is now a public company. For the past 18 years he’s been an investor, and has invested over $160 million. He’s funded 27 businesses at a ’venture level’ (multiple millions of dollars) and focuses on early stage businesses. He’s also funded 100-150 businesses at a ‘seed level’ (hundreds of thousands to a few million dollars). This is from a pool of 35,000 businesses that he’s looked at.\nCRV is a collection of partners. A business that wants funding from CRV needs one of the partners to champion them. All businesses are put to a vote, and if the champion cannot get the other partners excited then it’s unlikely the business will be funded. This means that usually a successful product pitch will not only be clear and concise, but also work just as well when it’s not delivered by the founders. Zachary’s one exception was Twitter, ‘who had the worst presentation ever’.\nSuccessful businesses, Zachary believes, have ‘founders that are maniacal, bipolar and slightly crazy’. Their need to win permeates every aspect of their life and although this drives them, it also means they are difficult to work with. Zachary’s one exception was Twitter, ‘who had three co-founders, none of whom wanted to confront each other’.\nZachary believes that founders must have a clear vision of what they are trying to achieve. This means that the best pitches cause a binary reaction: love or hate. This is because if the audience shares the founder’s vision then they will love the product, but otherwise they will hate it. A product that causes reactions such as ‘oh cool’ or ‘nice’ is unlikely to get VC funding at CRV – they prefer a low probability of an enormous success over a high probability of moderate success. As you probably expect by now, Zachary’s exception was Twitter, where he said the founders basically didn’t understand why anyone would tweet for the first couple of years.\nVision is even more important than a clear path to monetization to Zachary. When he evaluates businesses he asks himself whether people love the product and whether the founders are able to develop the product to take advantage of this. It’s only then that he asks whether it seems that they can monetize. And a lack of an obvious plan to monetize isn’t a deal-breaker.\nThe implication of all this, which Zachary was very clear about, is that successful businesses don’t need VC funding. Equally, a failure to get VC funding doesn’t mean the business is necessarily a failure. Here capital is an accelerator, and a successful business will not be dependent on VC funding.\nThroughout the talk Zachary continually referred to the fact that he’s a gut-instinct investor. But given his talk I’m not so sure it’s true. For instance, during Q&A he was pitched to by a person whose business is Uber for motorcycles. So the guy pitches that, just that, no other information, with an accent so thick you can barely make out what he’s saying, and then asks Zachary whether he thinks it’ll be a billion dollar business! Straight away Zachary says ‘no’. I’m expecting it to be over, but then Zachary continues for five minutes explaining why he thinks that way, applying some of the simple rules that he’d discussed earlier and references to data that he knew off-hand, and that I later verified to be correct.\nZachary’s gut just seemed to be able to do the calculations that data-driven investors require computers to do. So is he really gut-instinct investor? By now it was getting late and Zachary needed to get home to put his kids to sleep. But he left us all with his email address, and a plea to get in contact, ‘but not all tonight, otherwise I won’t be able to respond to you all’. So he mustn’t be a computer after all.\nSan Francisco\n\n\n",
    "preview": "posts/2014-03-19-the-human-equation-an-interview-with-george-zachary/images/IMG_0277.jpg",
    "last_modified": "2020-04-26T17:12:08-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-28-final-shot-at-ending-industrial-disputes/",
    "title": "Final Shot At Ending Industrial Disputes",
    "description": "The industrial relations disputes that culminated in the shutdown of Qantas last year took more than nine months to make it through arbitration. Such lengthy delays have left many wondering if changes should be made to the processes that underpin this form of dispute resolution. The use of ‘final offer’ arbitration could be the small change that has a big effect, saving conflicted parties time, money and reputation.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      },
      {
        "name": "Ben O'Neill",
        "url": {}
      }
    ],
    "date": "2012-11-28",
    "categories": [],
    "contents": "\nWritten with Ben O’Neill and originally published in the Canberra Times.\nThe industrial relations disputes that culminated in the shutdown of Qantas last year took more than nine months to make it through arbitration. Such lengthy delays have left many wondering if changes should be made to the processes that underpin this form of dispute resolution. The use of ‘final offer’ arbitration could be the small change that has a big effect, saving conflicted parties time, money and reputation.\n\n\n\nFigure 1: Qantas negotiates with a lot of other parties. Final offer arbitration may have helped reduce the length of the shutdown. Photo by Monica Alexander.\n\n\n\nArbitration is an alternative to a formal litigation. Unless the parties agree otherwise, arbitration is conducted as an adversarial process, but it has looser rules of evidence than standard litigation. An arbitrator, often an experienced lawyer or a highly regarded expert in the field of the dispute, guides the process and makes a binding determination if the parties fail to agree on a solution.\nDespite its benefits, the traditional arbitration process encourages par- ties to begin with an extreme position. This is because the parties know that during the process, they are likely to compromise from that position, and unlike in a formal litigation, it is unlikely they will suffer cost penalties for making spurious claims or arguments.\nUltimately the arbitrator is required to make a determination according to law, but finding the proper legal solution is made more difficult when the parties have an incentive to exaggerate their true position. As an analogy, imagine a real-estate agent who inflates the asking price of a house, knowing that people will try to talk the price down, and a buyer who feigns low interest in the house in order to bargain more effectively — both have an incentive to exaggerate their true position.\nFinal offer arbitration removes this perverse incentive problem. In FOA, the two parties submit their position on the proper legal outcome to the arbitrator and these positions are revealed simultaneously to both parties. A negotiation period ensues, and if the parties cannot agree to an outcome, the arbitrator makes a determination.\nHowever, the arbitrator must choose one of the submitted positions in its entirety — there is no alternative outcome allowed.\nAlthough this may seem restrictive, or even unfair, it creates strong in- centives for the parties to be reasonable in their submission to the arbitrator. The more extreme their submitted position (relative to the arbitrator’s view of the proper legal outcome), the less likely it is to be chosen. Moreover, if the parties are being reasonable in their submission, it is more likely that they will be able to find common ground in negotiation. Final offer ar- bitration encourages the parties to make a bona fide attempt to strive for impartiality in their own claim.\nNonetheless, FOA is not without its weaknesses. For instance, it assumes sufficient mastery of the law and the facts to prepare a ‘winning’ final offer. As such, it is best used in situations where the parties, their lawyers and their experts are experienced and understand the possible outcomes. This would usually be the case in commercial disputes and some labour disputes.\nThe ‘final offer’ restriction on the arbitrator’s decision means that, if a settlement is not reached, there will be a total acceptance of one party’s claim. This might lead some to worry about the arbitrator’s impartiality, though this is a consideration in any form of arbitration. In any case, this matter is already well provided for in law. For instance, arbitrators are under a legal obligation to be impartial and fair, and to resolve the dispute according to law. Additionally, parties can challenge the nomination of an arbitrator and apply to court to set aside an award on the basis of arbitrator misconduct.\nArbitration mechanisms in Australia are provided for under contract law and in various commercial arbitration acts. These could easily facilitate the use of FOA. Ultimately, the form of arbitration is best selected by the parties prior to a dispute, but it is not unthinkable that final offer arbitration could be useful during dispute resolution. It could save time and money, and could also foster more fruitful negotiation. If Qantas and their workers can achieve the miracle of human flight, efficient dispute resolution might also be possible.\n\n\n",
    "preview": "posts/2012-11-28-final-shot-at-ending-industrial-disputes/images/DSC3132.jpg",
    "last_modified": "2020-04-26T17:11:59-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-05-17-tournaments-could-drive-r-d-effort/",
    "title": "Tournaments Could Drive R&D Effort",
    "description": "The government should use tournaments to stimulate research and development in Australia. They have been largely overlooked since the Cutler Review of Innovation, but when structured properly, they encourage out-standing achievement and promote creative destruction.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      },
      {
        "name": "Andrew Barnes",
        "url": {}
      }
    ],
    "date": "2011-05-17",
    "categories": [],
    "contents": "\nWritten with Andrew Barnes and originally published in the Australian Financial Review.\nThe government should use tournaments to stimulate research and development in Australia. They have been largely overlooked since the Cutler Review of Innovation, but when structured properly, they encourage out-standing achievement and promote creative destruction.\n\n\n\nFigure 1: Tournaments bring out the best of us in sport, why not also R&D? Photo by Monica Alexander.\n\n\n\nCreative destruction is a phrase made famous by the 20th-century economist Joseph Schumpeter. It describes the process whereby companies, seeking profit, innovate so successfully that their new product destroys the demand for existing products.\nIt is not hard to find examples, just consider how CDs replaced cassettes, and faxes have been made almost extinct by email. Although harsh, creative destruction is necessary for economic progress, modern society would be unimaginable without it.\nA lesson that economists are now learning is that environments in which innovative ideas happen can be deliberately created. Authors such as Cass Sunstein and Richard Thaler suggest governments are most likely to be successful when they point markets in a desired direction and let them find their way.\nOne way to do this, common overseas but underused in Australia, is the use of tournaments. All the government does is put up a prize for a specific achievement, and then wait to crown the winner. There is little scope for bias because the criteria must be both publicly available and clearly defined to allow firms to do the necessary research and development.\nTournaments are effective because companies tend to spend more on the required R&D than the prizemoney that is offered. They do this because the rewards of winning are not just the proffered prize money; companies also get prestige, attention, and press coverage, all of which means successful solutions flourish.\nWith US$10 million in prizemoney, and the thrill of competition to entice firms, the Ansari X Prize proved that it was possible for a private company to fly into space. Though the cost to develop a vehicle capable of winning was many times the prizemoney, the tournament was a triumph and a catalyst for rapid innovation.\nAustralia already has one well-known tournament, the World Solar Challenge, a solar car race from Darwin to Adelaide. There should be more. The level of innovation is great that in recent years a new section has been set-up with more onerous design restrictions.\nTo enjoy strong and sustainable economic growth Australia must be at the forefront of innovation. Our companies must be responsible for creative destruction. Tournaments would not only appeal to our competitive natures, but also be good for the economy.\n\n\n",
    "preview": "posts/2011-05-17-tournaments-could-drive-r-d-effort/images/DSCF4403.jpg",
    "last_modified": "2020-04-26T17:11:50-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2009-05-25-plastic-policies/",
    "title": "Plastic Policies",
    "description": "There is broad agreement that Australian plastic bag consumption should be reduced. To this end, recent South Australian legislation has banned certain types of plastic bags. But other states wishing to reduce their plastic bag consumption may find a tax rather than a ban the more appropriate policy instrument.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      },
      {
        "name": "Flavio Menezes",
        "url": {}
      }
    ],
    "date": "2009-05-25",
    "categories": [],
    "contents": "\nWritten with Professor Flavio Menezes and originally published at Australian Policy Online.\nThere is broad agreement that Australian plastic bag consumption should be reduced. To this end, recent South Australian legislation has banned certain types of plastic bags. But other states wishing to reduce their plastic bag consumption may find a tax rather than a ban the more appropriate policy instrument.\nIn South Australia, one of the reasons for a ban rather than a tax was the belief that a tax would impose an additional cost on households. The implied corollary is that a ban on plastic bags would not impose these costs, and thus would presumably be paid for entirely by retailers. This claim is incorrect: not only does a ban on plastic bags impose costs on households, but they are greater than those imposed by a tax.\nTo understand the impact of a tax on plastic bag consumption it is necessary to distinguish between the initial and final ‘incidence’ of the tax — in other words, between who is legally responsible for paying a tax, and whom the burden of paying the tax would be passed on to. It is the final incidence that matters from a policy perspective. Economic theory suggests that except under very special circumstances, final and initial incidences do not coincide. Indeed, evidence from Germany, Ireland and Switzerland suggest that plastic bag consumption decreases as a result of a tax; and that it is both retailers and households who bear the burden of paying the tax, regardless of the initial incidence.\nUnder a ban on plastic bags, retailers must switch to other, more ex- pensive, options. Again the distinction between initial and final incidence is important. Although retailers would be legally responsible for providing the more expensive bags required to replace plastic bags, at least some of this additional cost would be passed onto households through higher prices or an explicit charge. Thus, regardless of whether a tax or a ban is used, households bear an additional cost.\nSo is a tax or a ban the more appropriate instrument? A ban reduces the number of plastic bags to zero, whatever the cost. Bans on smoking in workplaces, for example, are considered desirable because the cost of even a small amount of smoking in the workplace is believed to be unacceptable. For this reason, society is prepared to pay any price to ensure a non-smoking work environment. Is the same true for plastic bags?\nIn contrast, under a tax the reduction in the number of plastic bags is uncertain, but the additional cost imposed on households is known: it is the size of the tax. A tax more explicitly considers the cost of reduction. It allows households a choice that takes into account the cost and the ben- efit and results in plastic bag usage in cases where the benefit of doing so outweighs the cost.\nWhen the benefit of each additional unit of plastic bag reduction is small compared to the cost a tax is the better option; for should this cost be higher than anticipated, a tax rather than a ban would cost society less. And given that a ban supposes the cost of abatement to be entirely outweighed by the benefit for every level of abatement it is, in effect, not possible that a government advocating a ban has underestimated it.\nAlthough a number of factors must be considered when deciding between a tax or a ban, a ban on plastic bags is unlikely to be the best option for minimising the financial impact on households.\n\n\n",
    "preview": {},
    "last_modified": "2020-04-26T17:11:38-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2008-05-08-home-buyers-will-be-hurt-by-aid-plan/",
    "title": "Home Buyers Will Be Hurt by Aid Plan",
    "description": "The Government's First Home Saver Accounts policy, announced during the last federal election, is a promise that should never have been made. It will place upward pressure upon inflation and increase the potential for domestic financial turbulence. It will be the individuals that the policy is designed to help (young Australians with a still-insecure financial foundation) that will be most hurt in the long-run.",
    "author": [
      {
        "name": "Rohan Alexander",
        "url": {}
      }
    ],
    "date": "2008-05-08",
    "categories": [],
    "contents": "\nOriginally published in the Australian Financial Review.\nThe Government’s First Home Saver Accounts policy, announced during the last federal election, is a promise that should never have been made. It will place upward pressure upon inflation and increase the potential for domestic financial turbulence. It will be the individuals that the policy is designed to help (young Australians with a still-insecure financial foundation) that will be most hurt in the long-run.\n\n\n\nFigure 1: A view of the Harbour apparently adds c.$100,000 to the price of an apartment in Sydney.\n\n\n\nThere are four broad areas of concern. Firstly, although the policy appears economically sound, closer inspection shows that it amounts to government encouragement of irresponsible mortgage practices (remember that irresponsible mortgage practices contributed to the sub-prime meltdown in the US). Specifically, it provides a tax concession to allow individuals to qualify for a loan but does not provide similar help to make repayments. The effect will be similar to that of the substantial decrease in the US interest rate during the early 2000s – families will be able to take out home loans which they could not ordinarily afford. These are the American families experiencing the greatest hardship from the sub-prime meltdown and similarly, it will be the Australian families taking advantage of this policy that will suffer when forced to service debt larger than they can reasonably repay.\nSecondly, even if the Australians helped by this policy are able to afford the repayments given the current domestic economic position it is irresponsible to expect these conditions to continue indefinitely. The inability of individuals to meet repayments under changed conditions would lead to significant foreclosure activity – an outcome not desired by any Australian.\nThirdly, the policy increases the exposure of young Australians to inter- est rates (and thereby international economic conditions). Increased inter- national money market links are not beneficial to most Australians in the short-term given current international money market conditions and especially not the group targeted by the policy.\nFinally, as the policy is available to all first home buyers it does not increase the relative ability of any targeted individual to purchase a house from the normal first home market i.e. when bidding against other first home buyers – a zero-sum game. However, the increase in house prices to be expected as a result of the game engenders domestic inflation pressure.\nGiven domestic economic conditions, responsible policy ought to encourage saving. In order to avoid the above concerns, such savings must not be tied to the residential property market and thus policy needs to encourage broader economy-wide saving. Its nature would be such that some of the increased savings would be invested in projects increasing productive capacity – important given current capacity constraints.\nThe First Home Saver Accounts policy amounts to encouragement of irresponsible mortgage practices and even if it does increase the ability of young Australians to purchase their first home (which it will not) it is likely that they would be placed under substantial financial pressure during a domestic slowdown. The economic conditions we currently enjoy represent a rare opportunity to increase the long term savings rate – a change which will fight against inflation and policy must reflect this.\n\n\n",
    "preview": "posts/2008-05-08-home-buyers-will-be-hurt-by-aid-plan/images/DSCF0213.jpg",
    "last_modified": "2020-04-26T15:49:08-04:00",
    "input_file": {}
  }
]
